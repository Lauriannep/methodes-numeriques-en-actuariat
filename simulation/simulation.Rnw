%%% Copyright (C) 2018 Vincent Goulet
%%%
%%% Ce fichier fait partie du projet
%%% ¬´M√©thodes num√©riques en actuariat avec R¬ª
%%% http://github.com/vigou3/methodes-numeriques-en-actuariat
%%%
%%% Cette cr√©ation est mise √† disposition selon le contrat
%%% Attribution-Partage dans les m√™mes conditions 4.0
%%% International de Creative Commons.
%%% http://creativecommons.org/licenses/by-sa/4.0/

\chapter{Simulation de nombres al√©atoires non uniformes}
\label{chap:simulation}

<<echo=FALSE>>=
options(width = 52)
@

\begin{objectifs}
\item D√©velopper un algorithme de simulation de
  nombres non uniformes √† partir de la m√©thode de l'inverse.
\item D√©velopper un algorithme de simulation de nombres non uniformes
  √† partir de la m√©thode d'acceptation-rejet.
\item Calculer des nombres pseudo-al√©atoires non uniformes en
  suivant un algorithme donn√©.
\item Utiliser les outils de Excel et R pour la simulation de
  nombres non uniformes.
\end{objectifs}

Habituellement, les applications de la simulation requi√®rent des
nombres al√©atoires provenant non pas d'une distribution $U(0, 1)$,
mais plut√¥t d'une distribution avec fonction de r√©partition $F_X(x)$.
Ceci n√©cessite donc de transformer les nombres uniformes en des nombres
provenant de distributions non-uniformes.

Il existe de tr√®s nombreux algorithmes pour g√©n√©rer des nombres
al√©atoires de diff√©rentes distributions; voir, par exemple,
\cite{Devroye:random:1986}. Nous n'en √©tudierons que deux en d√©tail,
soit la m√©thode de l'inverse et la m√©thode d'acceptation-rejet. Par
ailleurs, la \autoref{sec:simulation:modeles-actuariels} pr√©sente des
proc√©dures de simulation pour les mod√®les actuariels les plus
courants.

Plusieurs algorithmes de simulation de nombres non uniformes reposent
sur des r√©sultats de transformations de variables al√©atoires. Par
exemple:
\begin{itemize}
\item la somme de $\alpha$ exponentielles est une loi gamma avec
  param√®tre de forme $\alpha$ entier;
\item la loi g√©om√©trique est la partie enti√®re de l'exponentielle;
\item la loi $F$ est un ratio de deux khi-carr√©; etc.
\end{itemize}
Le lecteur qui ne serait pas √† l'aise avec les notions de
transformation de variables al√©atoires trouvera √†
l'\autoref{chap:rappels_transformations} des rappels sur les
principales techniques √©tudi√©es dans les cours d'analyse probabiliste
et d'analyse statistique.

\begin{prob-enonce}
  Au chapitre pr√©c√©dent, nous nous sommes pench√©s sur le probl√®me des
  quatre points de Sylvester lorsque la r√©gion $R$ est un carr√©.

  Cette fois, nous devons v√©rifier par simulation que lorsque la
  r√©gion $R$ est un disque, la probabilit√© que l'enveloppe convexe
  forme un triangle est $\frac{35}{12 \pi^2} \approx 0,29552$.
\end{prob-enonce}


\section{M√©thode de l'inverse}
\label{sec:simulation:inverse}

La m√©thode de l'inverse repose sur une id√©e toute simple, soit que
nous pouvons transformer des nombres uniformes sur $(0, 1)$ en des
nombres provenant de la distribution avec fonction de r√©partition
$F_X(x)$ en utilisant le th√©or√®me suivant.

\begin{thm}
  \label{thm:simulation:inverse}
  Soit $X$ une variable al√©atoire avec fonction de r√©partition
  $F_X(x)$. Alors
  \begin{displaymath}
    F_X(X) \sim U(0, 1).
  \end{displaymath}
\end{thm}
\begin{proof}
  Soit la transformation $U = F_X(X)$. Alors,
  \begin{align*}
    F_U(u)
    &= \Pr[U \leq u] \\
    &= \Pr[F_X(X) \leq u] \\
    &= \Pr[X \leq F_X^{-1}(u)] \\
    &= F_X(F_X^{-1}(u)) \\
    &= u,
  \end{align*}
  d'o√π $U \sim U(0, 1)$.
\end{proof}

Par cons√©quent, si $U \sim U(0, 1)$, alors
\begin{displaymath}
  F_X^{-1}(U) \sim X.
\end{displaymath}
La fonction de r√©partition inverse, $F_X^{-1}$, est aussi appel√©e
\emph{fonction de quantile}.

\begin{algorithme}[M√©thode de l'inverse]
  Soit $X$ une variable al√©atoire avec fonction de r√©partition
  $F_X(x)$. Les √©tapes ci-dessous permettent de g√©n√©rer une
  observation $x$ de cette variable al√©atoire.
  \begin{enumerate}
  \item Obtenir un nombre $u$ d'une loi $U(0, 1)$.
  \item Poser $x = F_X^{-1}(u)$.
  \end{enumerate}
\end{algorithme}

La m√©thode de l'inverse consiste √† choisir un nombre uniform√©ment sur
l'axe des ordonn√©es d'une fonction de r√©partition (donc un nombre
entre $0$ et $1$) et √† trouver la valeur correspondante sur l'axe des
abscisses telle que donn√©e par la fonction de quantile. Comme les
valeurs en $x$ seront plus concentr√©es l√† o√π la pente de la fonction
de r√©partition est la plus grande, et vice versa, la distribution en
abscisse ne sera pas uniforme. Voir la
\autoref{fig:simulation:inverse} pour une repr√©sentation
graphique de ce ph√©nom√®ne.

\begin{figure}
  \centering
<<echo=FALSE,fig=TRUE, width=8, height=4>>=
par(mfrow = c(1, 2), mar = c(5, 4, 2, 2))

## Graphique de fonction de r√©partition
plot(NA, xlim = c(0, 14), ylim = c(0, 1),
     xlab = expression(x), ylab = expression(F[X](x)),
     xaxs="i", yaxs="i")
u <- c(0.3, 0.4, 0.8, 0.9)
x <- qgamma(u, 5, 1)
polygon(c(0, x[1], x[1], x[2], x[2], 0),
        c(u[1], u[1], 0, 0, u[2], u[2]), col="lightblue")
polygon(c(0, x[3], x[3], x[4], x[4], 0),
        c(u[3], u[3], 0, 0, u[4], u[4]), col="lightblue")
curve(pgamma(x, 5, 1), add=TRUE)

## Graphique de la densit√©
plot(NA, xlim = c(0, 14), ylim = c(0, 0.2),
     xlab = expression(x), ylab = expression(f[X](x)),
     xaxs = "i", yaxs = "i")
xx <- seq(from = x[1], to = x[2], length = 100)
polygon(c(xx[1], xx, xx[100]), c(0, dgamma(xx, 5, 1), 0), col = "lightblue")
xx <- seq(from = x[3], to = x[4], length = 100)
polygon(c(xx[1], xx, xx[100]), c(0, dgamma(xx, 5, 1), 0), col = "lightblue")
curve(dgamma(x, 5, 1), xlim = c(0, 14), add=TRUE)
@
  \caption{Repr√©sentation graphique de la m√©thode de l'inverse. √Ä
    des intervalles √©gaux en ordonn√©e correspondent des intervalles
    diff√©rents en abscisse selon la forme de la distribution. La
    fonction de r√©partition √† gauche correspond √† la densit√© de droite.}
  \label{fig:simulation:inverse}
\end{figure}

La m√©thode de l'inverse en est une bonne si la fonction de quantile
est facile √† calculer. S'il n'existe pas de forme explicite pour
$F_X^{-1}(\cdot)$, r√©soudre num√©riquement
\begin{displaymath}
  F_X(x) - u = 0
\end{displaymath}
peut s'av√©rer aussi efficace que bien d'autres m√©thodes.

\tipbox{Dans Excel, il faut n√©cessairement utiliser la m√©thode de
  l'inverse. Plusieurs fonctions de quantiles sont disponibles; voir
  la \autoref{sec:simulation:excel_et_al}.}


\subsection{Distributions continues}
\label{sec:simulation:inverse:continues}

La m√©thode de l'inverse est, en principe du moins, simple √† utiliser
avec les distributions continues: il suffit de conna√Ætre la fonction
de quantile et de l'appliquer √† des nombres uniformes pour obtenir des
nombres de la distribution souhait√©e.

Dans les faits, il y a peu de lois de probabilit√© continues dont la
fonction de r√©partition est simple √† inverser (exponentielle, Pareto,
Weibull). Il faut parfois utiliser d'autres m√©thodes.

\begin{exemple}
  Nous voulons obtenir un √©chantillon al√©atoire d'une distribution
  exponentielle de param√®tre $\lambda$ avec fonction de densit√© de
  probabilit√©
  \begin{align*}
    f(x)
    &= \lambda e^{-\lambda x}, \quad x > 0 \\
    \intertext{et fonction de r√©partition}
    F(x)
    &= 1 - e^{-\lambda x}, \quad x > 0.
  \end{align*}
  Or,
  \begin{displaymath}
    F^{-1}(u) = - \frac{1}{\lambda} \ln (1 - u),
  \end{displaymath}
  donc
  \begin{displaymath}
    X = - \frac{1}{\lambda} \ln (1 - U) \sim \text{Exponentielle}(\lambda),
  \end{displaymath}
  o√π $U \sim U(0, 1)$. En fait, puisque $U \sim U(0, 1)
  \Leftrightarrow 1 - U \sim U(0, 1)$, nous pouvons nous contenter de la
  relation
  \begin{displaymath}
    X = - \frac{1}{\lambda} \ln U \sim \text{Exponentielle}(\lambda).
  \end{displaymath}
  Par cons√©quent, l'algorithme pour simuler des nombres provenant
  d'une exponentielle de param√®tre $\lambda$ est:
  \begin{enumerate}
  \item Obtenir un nombre $u$ d'une $U(0, 1)$;
  \item Poser $x = - \lambda^{-1} \ln u$.
  \end{enumerate}

  \gotorbox{Ex√©cuter le code informatique R de la
    \autoref{sec:simulation:code} correspondant √† cet exemple pour une
    illustration de l'algorithme ci-dessus.}%
  \qed
\end{exemple}


\subsection{Distributions discr√®tes}
\label{sec:simulation:inverse:discretes}

Nous pouvons aussi utiliser la m√©thode de l'inverse avec les distributions
discr√®tes. Cependant, puisque la fonction de r√©partition comporte des
sauts, son inverse n'existe pas formellement. Par cons√©quent, il
n'existe pas de solution de $u = F_X(x)$ pour certaines valeurs de
$u$, ou alors une infinit√© de solutions.

Supposons une distribution avec un saut en $x_0$ et
\begin{align*}
  F_X(x_0^-) &= a \\
  F_X(x_0) &= b > a.
\end{align*}
Si $a < u < b$, nous posons $x = x_0$. Ainsi, nous simulerons $x_0$ dans une
proportion $b - a$ du temps, ce qui correspond √† $\Pr[X = x_0]$. Voir
la \autoref{fig:simulation:discrete} pour une illustration.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
set.seed(1)
x <- rbinom(1000, 4, 0.6)
Fn <- ecdf(x)
plot(Fn, xlim = c(0, 5), ylab="F(x)", main = "")
polygon(c(par("usr")[1], 2, 2, par("usr")[1]),
        Fn(c(1, 1, 2, 2)), col = "lightblue", border = NA)
plot(Fn, add = TRUE)
u <- runif(1, Fn(1), Fn(2))
arrows(par("usr")[1], u, 2, u, length = 0.125)
segments(2, par("usr")[3], 2, Fn(2), lty = 2)
axis(2)
@
  \caption{Illustration de la m√©thode de l'inverse pour une distribution
   discr√®te. Tout nombre uniforme tir√© dans la zone color√©e est
   converti en la m√™me valeur, ce qui correspond √† la probabilit√©
   d'obtenir cette valeur (la hauteur du saut).}
  \label{fig:simulation:discrete}
\end{figure}

Que faire si $u = a$ ou $u = b$? Nous prenons la plus grande valeur de
l'intervalle o√π $F_X(x)$ est constante:
\begin{align*}
  u = a  &\Rightarrow x = x_0 \\
  u = b  &\Rightarrow x = x_1.
\end{align*}
Il faut proc√©der ainsi parce que plusieurs g√©n√©rateurs produisent des
nombres uniformes sur $[0, 1)$.

\begin{exemple}
  \label{exemple:simulation:mixte}
  Soit $X$ une variable al√©atoire avec fonction de densit√© de probabilit√©
  \begin{displaymath}
    f(x) =
    \begin{cases}
      0,5, & 0 \leq x < 1 \\
      0,   & 1 \leq x < 2 \\
      0,5, & 2 \leq x < 3.
    \end{cases}
  \end{displaymath}
  Il s'agit d'une variable al√©atoire \emph{mixte} (en partie continue
  et en partie continue) dont la fonction de r√©partition est
  \begin{displaymath}
    F(x) =
    \begin{cases}
      0,5x, & 0 \leq x < 1 \\
      0,5   & 1 \leq x < 2 \\
      0,5x - 0,5, & 2 \leq x < 3.
    \end{cases}
  \end{displaymath}
  Cette fonction est repr√©sent√©e √† la \autoref{fig:simulation:mixte}.

  \begin{figure}
    \centering
<<echo=FALSE,fig=TRUE, width=8, height=4>>=
par(mfrow = c(1, 2), mar = c(5, 4, 2, 2))
plot(NA, xlim = c(0, 3), ylim = c(0, 1),
     xlab = expression(x), ylab = expression(f[X](x)),
     xaxt = "n", yaxt = "n")
points(c(0, 1), rep(0.5, 2), type = "o", pch = c(19, NA))
points(c(1, 2), rep(0, 2), type = "o", pch = c(19, NA))
points(c(2, 3), rep(0.5, 2), type = "o", pch = c(19, NA))
axis(1, at = 0:3)
axis(2, at = 0:2/2)
plot(0:3, c(0, 0.5, 0.5, 1), type = "o", pch = 19,
     xlab = expression(x), ylab = expression(F[X](x)),
     xaxt = "n", yaxt = "n")
axis(1, at = 0:3)
axis(2, at = 0:2/2)
@
    \caption{Fonction de densit√© de probabilit√© (gauche) et fonction de
      r√©partition (droite) de l'\autoref{exemple:simulation:mixte}}
    \label{fig:simulation:mixte}
  \end{figure}

  Un algorithme pour simuler des nombres al√©atoires de cette
  distribution est donc:
  \begin{enumerate}
  \item Obtenir un nombre $u$ d'une loi $U(0, 1)$.
  \item Poser
    \begin{displaymath}
      x =
      \begin{cases}
        2u,    & \text{si } 0 \leq u < 1 \\
        2,     & \text{si } u = 0,5 \\
        2u + 1 & \text{si } 0,5 < u < 1.
      \end{cases}
    \end{displaymath}
  \end{enumerate}
  \gotorbox{Une mise en ≈ìuvre en R de l'algorithme ci-dessus est
    pr√©sent√©e dans le code informatique de la
    \autoref{sec:simulation:code}.}%
  \qed
\end{exemple}

\begin{exemple}
  \label{ex:simulation:binomiale}
  Nous voulons simuler des observations d'une distribution binomiale de
  param√®tres $n$ et $\theta$. Nous pourrions, pour chaque $x$ simul√©,
  faire $n$ exp√©riences de Bernoulli et compter le nombre de succ√®s.
  Par la m√©thode de l'inverse pour la loi de Bernoulli, on a un succ√®s
  si $u \leq \theta$. Par cons√©quent, un algorithme serait:
  \begin{enumerate}
  \item Simuler $n$ nombres uniformes ind√©pendants $u_1, \dots, u_n$
    d'une loi $U(0, 1)$.
  \item Poser
    \begin{displaymath}
      x = \sum_{i = 1}^n I\{u_i \leq \theta\}.
    \end{displaymath}
  \end{enumerate}
  Cette technique requiert toutefois de simuler $n$ nombres uniformes
  pour chaque valeur de $x$.

  Il est plus efficace d'utiliser la m√©thode de l'inverse directement
  avec la distribution binomiale. Par exemple, si $n = 4$ et $\theta =
  0,5$, on a que
  \begin{align*}
    \Pr[X = x]
    &=
    \begin{cases}
      0,0625, & x = 0 \\
      0,25,   & x = 1 \\
      0,375,  & x = 2 \\
      0,25,   & x = 3 \\
      0,0625, & x = 4
    \end{cases} \\
    \intertext{et}
    \Pr[X \leq x]
    &=
    \begin{cases}
      0,      & x < 0 \\
      0,0625, & 0 \leq x < 1 \\
      0,3125, & 1 \leq x < 2 \\
      0,6875, & 2 \leq x < 3 \\
      0,9375, & 3 \leq x < 4 \\
      1,      & x \geq 4,
    \end{cases}
  \end{align*}
  d'o√π l'algorithme suivant:
  \begin{enumerate}
  \item Simuler un nombre $u$ d'une distribution $U(0, 1)$.
  \item D√©terminer $x$ selon le tableau suivant:
    \begin{center}
      \begin{tabular}{lc}
        \toprule
        $u$ dans l'intervalle & Valeur de $x$ \\
        \midrule
        $[0, 0,0625)$         & $0$ \\
        $[0,0625, 0,3125)$    & $1$ \\
        $[0,3125, 0,6875)$    & $2$ \\
        $[0,6875, 0,9375)$    & $3$ \\
        $[0,9375, 1)$         & $4$ \\
        \bottomrule
      \end{tabular}
    \end{center}
  \end{enumerate}
  \qed
\end{exemple}

\tipbox{La m√©thode de l'inverse pour les distributions discr√®tes √† support
  fini est facile √† mettre en ≈ìuvre dans Excel √† l'aide de la
  fonction \code{RECHERCHEV()}.}

\begin{prob-astuce}
  Au c≈ìur du probl√®me que nous tentons de r√©soudre, il y a la
  simulation de points dans un cercle. L'interpr√©tation g√©om√©trique
  est claire.

  Il serait √©galement possible d'en faire une interpr√©tation
  probabiliste: nous voulons simuler des valeurs d'une distribution
  bidimensionnelle uniforme sur un disque --- sans perte de
  g√©n√©ralit√©, de rayon $1$ et centr√© √† l'origine --- dont la fonction
  de densit√© conjointe est
  \begin{equation*}
    f_{XY}(x, y) = \frac{1}{\pi}, \quad -1 < x < 1,\, x^2 + y^2 < 1.
  \end{equation*}
  Cependant, les m√©thodes de simulation pour les distributions
  multidimensionnelles deviennent rapidement complexes, surtout
  lorsque les variables ne sont pas ind√©pendantes, comme c'est le cas
  ici. Nous t√¢cherons donc de trouver une mani√®re plus intuitive d'un
  point de vue g√©om√©trique pour simuler des points sur un disque.
\end{prob-astuce}


\section{M√©thode acceptation-rejet}

Supposons qu'il est compliqu√© de simuler des r√©alisations de la
variable al√©atoire $X$ avec fonction de densit√© de probabilit√©
$f_X(x)$. Si nous pouvons trouver une variable al√©atoire $Y$ avec fonction
de densit√© de probabilit√© $g_Y(x)$ pour laquelle la simulation est
simple (uniforme, triangle, exponentielle, etc.) et qu'il est possible
d'¬´envelopper¬ª la densit√© $f$ par un multiple de $g$, c'est-√†-dire que
\begin{displaymath}
  c g_Y(x) \geq f_X(x)
\end{displaymath}
pour tout $x$, alors nous pouvons utiliser l'algorithme
d'acceptation-rejet ci-dessous pour g√©n√©rer des observations de la
variable al√©atoire $X$.

\begin{algorithme}[M√©thode d'acceptation-rejet]
  Soit $X$ une variable al√©atoire avec fonction de densit√© de
  probabilit√© $f_X(x)$ et $Y$ une variable al√©atoire avec fonction de
  densit√© de probabilit√© $g_Y(x)$. Les √©tapes ci-dessous permettent de
  g√©n√©rer un nombre $x$ issu de la distribution $f$.
  \begin{enumerate}
  \item Obtenir une r√©alisation $y$ de distribution avec densit√©
    $g_Y(\cdot)$.
  \item Obtenir un nombre $u$ d'une $U(0, 1)$.
  \item Si
    \begin{equation*}
      u \leq \frac{f_X(y)}{c g_Y(y)},
    \end{equation*}
    poser $x = y$. Sinon, retourner √† l'√©tape 1.
  \end{enumerate}
\end{algorithme}

\tipbox{Puisque, par d√©finition, l'aire sous les densit√©s $f$ et $g$
  vaut $1$ dans les deux cas, la relation $c g_Y(x) \geq f_X(x)$ ne
  peut √™tre vraie pour tout $x$ que si $c > 1$.}

L'id√©e de la %
\capsule{https://youtu.be/1ptmo6CPKuQ}{m√©thode d'acceptation-rejet} %
consiste √† accepter la ¬´bonne proportion¬ª des r√©alisations de $Y$
comme provenant de $X$. La principale difficult√© avec la m√©thode
d'acceptation-rejet consiste √† trouver la fonction enveloppante
$c g_Y(x)$.

Dans l'illustration de la \autoref{fig:simulation:acceptation-rejet},
la densit√© $f$ √† support fini est facilement envelopp√©e par un
rectangle. Un nombre $y$ simul√© de cette distribution (√† toutes fins
pratiques une uniforme, ici) est accept√© comme provenant de $f$ dans
une proportion correspondant au ratio entre la valeur de $f_X(y)$ (les
segments pointill√©s dans la figure) et la valeur de $c g_Y(y)$ (les
segments pleins). Vous constaterez ais√©ment que certaines valeurs $y$
seront accept√©es plus souvent que d'autres en conformit√© avec la forme
de la densit√©.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
plot(NA, xlim = c(0, 1), ylim = c(0, 2), xlab = "y", ylab = "f(y)")
m <- dbeta(2/3, 3, 2)
x <- c(0.22, 0.58, 0.83)
eps <- eps <- 30E-4
segments(x - eps, 0, x - eps, dbeta(x, 3, 2), col = "lightblue", lwd = 3, lty = 2)
segments(x + eps, 0, x + eps, m, col = "lightblue", lwd = 3)
curve(dbeta(x, 3, 2), xlim = c(0, 1), add = TRUE, lwd = 2, col = "black")
segments(0, m, 1, m, col = "orange", lwd = 2)
text(0.1, m + 0.08, "c g(y)")
@
  \caption{Illustration de la m√©thode acceptation-rejet}
  \label{fig:simulation:acceptation-rejet}
\end{figure}

Une autre interpr√©tation est possible. La m√©thode dit d'accepter la
valeur $y$ simul√©e de la densit√© $g$ comme provenant de la densit√© $f$
si
\begin{equation*}
  u c g_Y(y) \leq f_X(y),
\end{equation*}
o√π $u$ est un nombre issu d'une loi $U(0, 1)$. Graphiquement, cela
signifie que nous acceptons la valeur $y$ si le point $(y, u c g_Y(y))$
se trouve sous la courbe $f$ en $y$ et que nous le rejetons s'il se trouve
entre $f$ et l'enveloppe. La
\autoref{fig:simulation:acceptation-rejet2} illustre cette
interpr√©tation.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
## plot(NA, xlim = c(0, 1), ylim = c(0, 2), xlab = "y", ylab = "f(y)")
## m <- dbeta(2/3, 3, 2)
## x <- rep(c(0.22, 0.58, 0.83), each = 8)
## y <- runif(24, 0, m)
## points(x, y, pch = ifelse(y <= dbeta(x, 3, 2), 21, 1), bg = "darkgray")
## curve(dbeta(x, 3, 2), xlim = c(0, 1), add = TRUE, lwd = 2, col = "blue3")
## segments(0, m, 1, m, col = "red3", lwd = 2)
m <- dbeta(2/3, 3, 2)
x <- runif(100)
mu <- m * runif(100)
plot(x, mu, xlim = c(0, 1), ylim = c(0, 2), xlab = "y", ylab = "f(y)",
     pch = ifelse(mu <= dbeta(x, 3, 2), 21, 1), bg = "lightblue")
curve(dbeta(x, 3, 2), xlim = c(0, 1), add = TRUE, lwd = 2, col = "black")
segments(0, m, 1, m, col = "orange", lwd = 2)
text(0.1, m + 0.08, "c g(y)")
@
  \caption{Interpr√©tation alternative de la m√©thode acceptation-rejet.
    Chaque point repr√©sente un couple $(y, u c g_Y(y))$. On accepte les
    points pleins et on rejette les points vides.}
  \label{fig:simulation:acceptation-rejet2}
\end{figure}

\tipbox{√âvidemment, plus l'enveloppe est pr√®s de $f_X(x)$, plus
  l'algorithme est performant puisque nous rejetons alors moins de
  valeurs.}

\begin{exemple}
  \label{exemple:simulation:beta:1}
  Soit $X \sim \text{B√™ta}(3, 2)$. La densit√© de cette distribution est:
  \begin{align*}
    f_X(x)
    &= \frac{\Gamma(5)}{\Gamma(3) \Gamma(2)}\,
    x^{3 - 1} (1 - x)^{2 - 1} \\
    &= 12 x^2 (1 - x), \quad 0 < x < 1.
  \end{align*}
  Elle est repr√©sent√©e aux figures
  \ref{fig:simulation:acceptation-rejet} et
  \ref{fig:simulation:acceptation-rejet2}. Nous pouvons facilement
  inscrire celle-ci dans un rectangle. Le mode de la densit√© se trouvant en
  $x = 2/3$, la hauteur du rectangle est $f(2/3) = 48/27 = 16/9$. Par
  cons√©quent,
  \begin{displaymath}
    c g_Y(x) = \frac{16}{9}, \quad 0 < x < 1.
  \end{displaymath}
  Nous d√©duisons que $c = 16/9$ et que la densit√© $g$ est une uniforme sur
  $(0 , 1)$. Nous avons donc l'algorithme suivant:
  \begin{enumerate}
  \item Simuler deux nombres $u_1$ et $u_2$ d'une $U(0, 1)$.
  \item Poser $y = u_1$.
  \item Si
    \begin{displaymath}
      u_2 \leq \frac{f_X(y)}{16/9},
    \end{displaymath}
    alors poser $x = y$. Sinon, retourner √† l'√©tape 1.
  \end{enumerate}

  L'aire du rectangle √©tant de $16/9$, nous pouvons nous attendre
  √† rejeter une proportion de
  \begin{equation*}
    \frac{16/9 - 1}{16/9} = \frac{7}{16} \approx 44~\%
  \end{equation*}
  des nombres $u_1$ simul√©s √† l'√©tape~1 de
  l'algorithme ci-dessus. Si l'enveloppe
  √©tait plus ¬´serr√©e¬ª autour de la densit√©, l'efficacit√© de
  l'algorithme en serait augment√©e.
  \qed
\end{exemple}

\begin{exemple}
  \label{exemple:simulation:beta:2}
  Reprenons l'\autoref{exemple:simulation:beta:1} en tentant
  d'am√©liorer l'efficacit√© de l'algorithme. Il s'av√®re que nous pouvons
  inscrire la densit√© de la loi B√™ta$(3, 2)$ dans un triangle aux
  caract√©ristiques suivantes (voir la
  \autoref{fig:simulation:beta-triangle}):
  \begin{figure}
    \centering
<<echo=FALSE, fig=TRUE>>=
curve(dbeta(x, 3, 2), xlim = c(0, 1), ylim = c(0, 2.5), col = "black", lwd = 2)
lines(c(0, 0.8, 1), c(0, 2.4, 0), lwd = 2, col = "orange")
segments(0.5, 0, 0.5, 1.5, col = "lightblue", lwd = 2, lty = 3)
segments(0.8, 0, 0.8, 2.4, col = "lightblue", lwd = 2, lty = 3)
axis(side = 1, at = 0.5)
@
    \caption{Fonction de densit√© de probabilit√© d'une loi B√™ta$(3, 2)$
      envelopp√©e d'un triangle}
    \label{fig:simulation:beta-triangle}
  \end{figure}
  \begin{enumerate}
  \item l'ar√™te gauche passe par $(0, 0)$, donc est de la forme $y =
    mx$. Cette droite √©tant tangente √† $f(x)$, la pente $m$ est telle
    que l'√©quation $mx = 12 x^2 (1 - x)$ a une seule racine autre que
    $0$, d'o√π $y = 3 x$;
  \item l'ar√™te droite passe par $(1, 0)$, donc est de la forme $y =
    mx + b$ avec $m + b = 0$. Comme la pente de cette droite est √©gale
    √† la pente de $f(x)$ en $x = 1$, nous trouvons que $y = 12 - 12 x$.
  \end{enumerate}
  Ainsi,
  \begin{displaymath}
    c g_Y(x) =
    \begin{cases}
      3x,       & 0 < x < 0,8 \\
      12 - 12 x, & 0,8 < x < 1.
    \end{cases}
  \end{displaymath}
  Or, l'aire du triangle est
  \begin{displaymath}
    \frac{(1) c g_Y(0,8)}{2} = \frac{(1)(2,4)}{2} = 1,2,
  \end{displaymath}
  d'o√π $c = 1,2$. Par cons√©quent,
  \begin{align*}
    g_Y(x)
    &=
    \begin{cases}
      2,5 x,     & 0 < x < 0,8 \\
      10 - 10 x, & 0,8 < x < 1.
    \end{cases}
  \end{align*}
  Pour simuler des observations de cette densit√© par la m√©thode de
  l'inverse, nous calculons la fonction de r√©partition correspondante,
  \begin{align*}
    G_Y(x)
    &=
    \begin{cases}
      1,25 x^2,          & 0 < x < 0,8 \\
      -5 x^2 + 10 x - 4, & 0,8 < x < 1,
    \end{cases} \\
    \intertext{d'o√π}
    G_Y^{-1}(y)
    &=
    \begin{cases}
      \sqrt{0,8 y},           & 0 < y < 0,8 \\
      1 - \sqrt{0,2 - 0,2 y}, & 0,8 < y < 1.
    \end{cases}
  \end{align*}
  Au final, nous obtenons l'algorithme suivant:
  \begin{enumerate}
  \item Simuler deux nombres $u_1$ et $u_2$ d'une $U(0, 1)$.
  \item Poser $y = G_Y^{-1}(u_1)$.
  \item Si
    \begin{displaymath}
      u_2 \leq \frac{f_X(y)}{1,2 g_Y(y)} \Leftrightarrow
      1,2 g_Y(y) u_2 \leq f_X(y),
    \end{displaymath}
    alors poser $x = y$. Sinon, retourner √† l'√©tape 1.
  \end{enumerate}

  L'aire du triangle √©tant de $1,2$, nous pouvons maintenant
  s'attendre √† ne rejeter que $0,2/1,2 \approx 17~\%$ des nombres
  simul√©s, une am√©lioration importante par rapport √†
  l'\autoref{exemple:simulation:beta:1}. %
  \qed
\end{exemple}

\begin{prob-astuce}
  \label{astuce:simulation:2}
  L'interpr√©tation g√©om√©trique de la m√©thode d'acceptation-rejet
  illustr√©e √† la \autoref{fig:simulation:acceptation-rejet2} nous met
  sur la piste d'une mani√®re simple de g√©n√©rer des points distribu√©s
  uniform√©ment sur un disque.

  En effet, il suffit de simuler des points uniform√©ment sur un carr√©
  comme nous l'avons d√©j√† fait au \autoref{chap:generation}, puis de
  rejeter ceux qui ne se trouvent pas √† l'int√©rieur du disque. La
  \autoref{fig:simulation:disque} illustre cette id√©e: nous
  accepterions les points pleins et nous rejetterions les points
  vides.
\end{prob-astuce}

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
x <- runif(200, -1, 1)
y <- runif(200, -1, 1)
w <- x^2 + y^2 <¬†1
t <- seq(0, 2 * pi, length = 1000)

plot(x, y, xlab="x", ylab="y", xlim=c(-1, 1), ylim=c(-1, 1),
     pch = ifelse(w, 21, 1), bg = "lightblue")
#points(x[!w], y[!w], xlab="x", ylab="y", pch = 1)
lines(x = cos(t), y = sin(t), col = "black", lwd = 2)
rect(-1, -1, 1, 1, border = "orange", lwd = 2)
@
  \caption{Simulation de points sur un disque par une m√©thode d'acceptation-rejet}
  \label{fig:simulation:disque}
\end{figure}


\section{Fonctions de simulation dans Excel et R}
\label{sec:simulation:excel_et_al}

Il est important de savoir simuler des valeurs d'une variable
al√©atoire quelconque √† partir de la m√©thode de l'inverse ou d'un autre
algorithme, surtout si la distribution de la variable al√©atoire est
peu usit√©e. N√©anmoins, les diff√©rents outils statistiques fournissent
en g√©n√©ral la majorit√© des fonctions de simulation de variables
al√©atoires dont nous pouvons avoir besoin pour un usage quotidien.

\subsection{Excel}

Tel que mentionn√© √† la \autoref{sec:simulation:inverse}, il faut
g√©n√©ralement utiliser la m√©thode de l'inverse pour simuler des
observations de lois de probabilit√© dans Excel. Cette proc√©dure est
facilit√©e par le fait qu'il existe des fonctions Excel pour calculer
la fonction de r√©partition inverse (ou fonction de quantile) des lois
les plus courantes.

Ainsi, Excel fournit la fonction de densit√© de probabilit√© (lois
continues) ou la fonction de masse de probabilit√© (lois discr√®tes) ou
la fonction de r√©partition et, dans certains cas seulement, la
fonction de quantile des lois de probabilit√© pr√©sent√©es au
\autoref{tab:excel}. Les noms anglais des fonctions ont √©t√© modifi√©s
pour √™tre standardis√©s dans Excel~2010. Vous remarquerez que les
traducteurs fran√ßais ont omis de faire de m√™me.

\begin{table}[t]
  \centering
  \begingroup
    \renewcommand{\code}[1]{\texttt{\small #1}}
  \begin{tabularx}{\linewidth}{lp{17ex}X}
    \toprule
    Loi &
    Fonctions Excel \newline (nom anglais) &
    Fonctions Excel \newline (nom fran√ßais) \\
    \midrule
    B√™ta &
    \code{BETA.DIST} \newline \code{BETA.INV} &
    \code{LOI.BETA.N} \newline \code{BETA.INVERSE.N} \\
    Binomiale &
    \code{BINOM.DIST} \newline \code{BINOM.INV} &
    \code{LOI.BINOMALE.N} \newline \code{LOI.BINOMIALE.INVERSE} \\
    Binomiale n√©g. &
    \code{NEGBINOM.DIST} &
    \code{LOI.BINOMIALE.NEG.N} \\
    Exponentielle &
    \code{EXPON.DIST} &
    \code{LOI.EXPONENTIELLE.N} \\
    \emph{F} (Fisher) &
    \code{F.DIST} \newline \code{F.INV} &
    \code{LOI.F.N} \newline \code{INVERSE.LOI.F.N} \\
    Gamma &
    \code{GAMMA.DIST} \newline \code{GAMMA.INV} &
    \code{LOI.GAMMA.N} \newline \code{LOI.GAMMA.INVERSE.N} \\
    Hyperg√©om. &
    \code{HYPGEOM.DIST} &
    \code{LOI.HYPERGEOMETRIQUE.N} \\
    Khi carr√© &
    \code{CHISQ.DIST} \newline \code{CHISQ.INV} &
    \code{LOI.KHIDEUX} \newline \code{LOI.KHIDEUX.INVERSE} \\
    Log-normale &
    \code{LOGNORM.DIST} \newline \code{LOGNORM.INV} &
    \code{LOI.LOGNORMALE.N} \newline \code{LOI.LOGNORMALE.INVERSE.N} \\
    Normale &
    \code{NORM.DIST} \newline
    \code{NORM.INV} \newline
    \code{NORM.S.DIST} \newline
    \code{NORM.S.INV} &
    \code{LOI.NORMALE.N} \newline
    \code{LOI.NORMALE.INVERSE.N} \newline
    \code{LOI.NORMALE.STANDARD.N} \newline
    \code{LOI.NORMALE.STANDARD.INVERSE.N} \\
    Poisson &
    \code{POISSON.DIST} &
    \code{LOI.POISSON.N} \\
    \emph{t} (Student) &
    \code{T.DIST} \newline \code{T.INV} &
    \code{LOI.STUDENT.N} \newline \code{LOI.STUDENT.INVERSE.N} \\
    Weibull &
    \code{WEIBULL.DIST} &
    \code{LOI.WEIBULL.N} \\
    \bottomrule
  \end{tabularx}
  \endgroup
  \caption{Liste des fonctions relatives √† des lois de probabilit√©
    depuis Excel 2010.}
  \label{tab:excel}
\end{table}

\subsection{R}

Un large √©ventail de fonctions donne directement acc√®s aux
caract√©ristiques de plusieurs lois de probabilit√© dans R. Pour chaque
racine \code{\meta{loi}}, il existe quatre fonctions diff√©rentes :
\begin{enumerate}
\item \code{d\meta{loi}} calcule la fonction de densit√© de
  probabilit√© (loi continue) ou la fonction de masse de probabilit√©
  (loi discr√®te);
\item \code{p\meta{loi}} calcule la fonction de r√©partition;
\item \code{q\meta{loi}} calcule la fonction de quantile;
\item \code{r\meta{loi}} simule des observations de cette loi.
\end{enumerate}

Les diff√©rentes lois de probabilit√© disponibles dans le syst√®me R de
base, leur racine et le nom de leurs param√®tres sont rassembl√©s au
\autoref{tab:rng:lois}. Des paquetages fournissent des fonctions
pour d'autres lois dont, entre autres, \pkg{actuar} \citep{actuar} et
\pkg{SuppDists} \citep{Rpackage:SuppDists}.

\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    Loi de probabilit√© & Racine dans R & Noms des param√®tres \\
    \midrule
    B√™ta & \code{beta} & \code{shape1}, \code{shape2} \\
    Binomiale & \code{binom} & \code{size}, \code{prob} \\
    Binomiale n√©gative & \code{nbinom} & \code{size},
    \code{prob} ou \code{mu} \\
    Cauchy & \code{cauchy} & \code{location}, \code{scale} \\
    Exponentielle & \code{exp} & \code{rate} \\
    \emph{F} (Fisher) & \code{f} & \code{df1}, \code{df2} \\
    Gamma & \code{gamma} & \code{shape}, \code{rate} ou
    \code{scale} \\
    G√©om√©trique & \code{geom} & \code{prob} \\
    Hyperg√©om√©trique & \code{hyper} & \code{m}, \code{n},
    \code{k} \\
    Khi carr√© & \code{chisq} & \code{df} \\
    Logistique & \code{logis} & \code{location}, \code{scale} \\
    Log-normale & \code{lnorm} & \code{meanlog}, \code{sdlog} \\
    Normale & \code{norm} & \code{mean}, \code{sd} \\
    Poisson & \code{pois} & \code{lambda} \\
    \emph{t} (Student) & \code{t} & \code{df} \\
    Uniforme & \code{unif} & \code{min}, \code{max} \\
    Weibull & \code{weibull} & \code{shape}, \code{scale} \\
    Wilcoxon & \code{wilcox} & \code{m}, \code{n} \\
    \bottomrule
  \end{tabular}
  \caption{Lois de probabilit√© pour lesquelles il existe des fonctions
    dans le syst√®me R de base}
  \label{tab:rng:lois}
\end{table}

Toutes les fonctions du \autoref{tab:rng:lois} sont vectorielles,
c'est-√†-dire qu'elles acceptent en argument un vecteur de points o√π la
fonction (de densit√©, de r√©partition ou de quantile) doit √™tre √©valu√©e
et m√™me un vecteur de param√®tres. Par exemple, l'expression ci-dessous
retourne la probabilit√© que des lois de Poisson de param√®tre $1$, $4$
et $10$ prennent les valeurs $3$, $0$ et $8$, dans l'ordre.
<<echo=TRUE>>=
dpois(c(3, 0, 8), lambda = c(1, 4, 10))
@

Le premier argument de toutes les fonctions de simulation est la
quantit√© de nombres al√©atoires d√©sir√©e. Ainsi, l'expression suivante
retourne trois nombres al√©atoires issus de distributions de Poisson de
param√®tre $1$, $4$ et $10$, respectivement.
<<echo=TRUE>>=
rpois(3, lambda = c(1, 4, 10))
@

√âvidemment, sp√©cifier un vecteur comme premier argument d'une fonction
de simulation n'a pas tellement de sens, mais, si c'est fait, R
retournera une quantit√© de nombres al√©atoires √©gale √† la
\emph{longueur} du vecteur (sans √©gard aux valeurs contenues dans le
vecteur).

Autre joueur important dans l'√©quipe de simulation de R, la fonction
\code{sample} permet de g√©n√©rer des nombres d'une distribution
discr√®te quelconque. Sa syntaxe est
\begin{Schunk}
\begin{Verbatim}
sample(x, size, replace = FALSE, prob = NULL)},
\end{Verbatim}
\end{Schunk}
\begin{itemize}
\item \code{x} est un vecteur des valeurs possibles de
  l'√©chantillon √† simuler (le support de la distribution);
\item \code{size} est la quantit√© de nombres √† simuler;
\item \code{prob} est un vecteur de probabilit√©s associ√©es √† chaque
  valeur de \code{x} (par d√©faut \code{1/length(x)});
\item \code{replace} d√©termine si l'√©chantillonnage s'effectue avec ou
  sans remise.
\end{itemize}

\gotorbox{Le code informatique R de la \autoref{sec:simulation:code}
  contient des exemples plus d√©taill√©s d'utilisation des fonctions
  mentionn√©es ci-dessus.}


\section{Mod√®les actuariels}
\label{sec:simulation:modeles-actuariels}

Cette section traite de techniques de simulation pour des
distributions couramment utilisÈes dans la mod√©lisation des risques en
sciences actuarielles.

\subsection{M√©langes discrets}
\label{sec:simulation:modeles-actuariels:melanges}

Les \emph{m√©langes discrets} (ou finis; \emph{mixture} en anglais) sont
utiles pour d√©crire des ph√©nom√®nes consid√©r√©s comme provenant de la
combinaison de plusieurs ph√©nom√®nes qui surviennent chacun avec une
probabilit√© connue d'avance. Par exemple, si nous estimons qu'il y a
90~\% de chances que la distribution du nombre de personnes dans une
file d'attente soit une Poisson de param√®tre $2$ et 10~\% de chances
qu'il s'agisse plut√¥t d'une Poisson de param√®tre $10$, alors la
fonction de masse de probabilit√© de la distribution du nombre de
personnes dans la file d'attente en tout temps est:
\begin{equation*}
  \Pr[X = x] =
  0,9 \times \frac{2^x e^{-2}}{x!} + 0,1 \times \frac{10^x e^{-10}}{x!}.
\end{equation*}
Cette fonction de masse de probabilit√© est un m√©lange discret de deux
lois de Poisson.

\begin{definition}
  \label{def:simulation:melange-discret}
  La variable al√©atoire $Y$ est un m√©lange discret √† $k$ points des
  variables al√©atoires $X_1, X_2, \dots X_k$ si sa fonction de
  r√©partition est
  \begin{equation*}
    F_Y(x) = p_1 F_{X_1}(x) + p_2 F_{X_2}(x) + \dots + p_k F_{X_k}(x),
  \end{equation*}
  o√π $p_j \in [0, 1]$ pour tout $j = 1, \dots, k$ et
  $\sum_{i = 1}^k p_j = 1$.
\end{definition}

Dans la suite, nous ne consid√©rerons que les m√©langes de $k = 2$
distributions. Lorsque celles-ci sont continues avec fonctions de
densit√© de probabilit√© $f_1$ et $f_2$, la densit√© du m√©lange peut
s'√©crire
\begin{equation*}
  f(x) = p f_1(x) + (1 - p) f_2(x).
\end{equation*}

\begin{exemple}
  Les m√©langes discrets sont tr√®s souvent utilis√©s pour cr√©er de
  nouvelles distributions aux caract√©ristiques particuli√®res que l'on
  ne retrouve pas chez les distributions d'usage courant.

  La \autoref{fig:simulation:ln.ln} montre un m√©lange de deux
  distributions log-normales qui r√©sulte en une fonction de densit√© de
  probabilit√© bimodale ayant une queue similaire √† celle d'une
  log-normale. %
  \qed

  \begin{figure}
    \centering
<<echo=FALSE, fig=TRUE, width=4.5, height=7>>=
par(mfrow = c(3, 1), lwd = 1)
par1 <- c(3.6, 0.6)
par2 <- c(4.5, 0.3)
p <- 0.55
titre1 <- bquote(paste("Log-normale(", mu, " = ", .(par1[1]), ", ", sigma, " = ", .(par1[2]), ")"))
titre2 <- bquote(paste("Log-normale(", mu, " = ", .(par2[1]), ", ", sigma, " = ", .(par2[2]), ")"))
titre3 <- bquote(paste("M√©lange (p = ", .(p), ")"))
curve(dlnorm(x, par1[1], par1[2]), xlim = c(0, 250),
      main = titre1, ylab = "f(x)", lwd = 2)
curve(dlnorm(x, par2[1], par2[2]), xlim = c(0, 250),
      main = titre2, ylab = "f(x)", lwd = 2)
curve(p * dlnorm(x, par1[1], par1[2]) + (1 - p) * dlnorm(x, par2[1], par2[2]),
      xlim = c(0, 250), main = titre3, ylab = "f(x)", lwd = 2)
@
    \caption{Fonctions de densit√© de probabilit√© de deux log-normales et
      de leur m√©lange discret}
    \label{fig:simulation:ln.ln}
  \end{figure}
\end{exemple}

La conception d'un algorithme de simulation d'un m√©lange discret √†
deux distributions n√©cessite de bien comprendre que chaque observation
provient de l'une \emph{ou} de l'autre des distributions. Reste, √†
chaque fois, √† d√©terminer de quelle distribution simuler.

\begin{algorithme}[M√©lange discret de deux distributions]
  \label{algo:simulation:melange-discret}
  Soit $F(x) = p F_1(x) + (1 - p) F_2(x)$ la fonction de r√©partition
  d'un m√©lange discret des distributions avec fonctions de r√©partition
  $F_1$ et $F_2$. Les √©tapes ci-dessous permettent de g√©n√©rer un
  nombre al√©atoire $x$ de la distribution $F$.
  \begin{enumerate}
  \item Obtenir un nombre $u$ d'une loi $U(0, 1)$.
  \item Si $u \leq p$, obtenir $x$ de la distribution $F_1$, sinon
    obtenir $x$ de $F_2$.
  \end{enumerate}
\end{algorithme}

Vous aurez reconnu dans le test $u \leq p$, o√π $u$ provient d'une loi
$U(0, 1)$, le c≈ìur de la proc√©dure de simulation d'une loi de
Bernoulli mentionn√©e √† l'\autoref{ex:simulation:binomiale}.
L'\autoref*{algo:simulation:melange-discret} revient donc √† simuler
un nombre de la distribution $F_1$ si $u \leq p$ et aucun nombre de la
distribution $F_2$ ou, au contraire, aucun nombre de $F_1$ si $u > p$ et
un nombre de $F_2$. Cette observation joue un r√¥le important pour
concevoir une proc√©dure de simulation de $n > 1$ valeurs d'un m√©lange.

La fonction \code{rmixture} du paquetage \pkg{actuar} offre une
interface conviviale pour obtenir des nombres al√©atoires de m√©langes
discrets.

\gotorbox{Le code informatique de la \autoref{sec:simulation:code}
  propose une proc√©dure de simulation d'un m√©lange discret ainsi que
  des exemples d'utilisation de la fonction \code{rmixture}.}

\subsection{M√©langes continus et mod√®les hi√©rarchiques}
\label{sec:simulation:modeles-actuariels:hierarchiques}

Nous pouvons √©tendre le concept de m√©lange discret √† une quantit√© non
d√©nombrable de distributions. Il en r√©sulte ce que l'on nomme un
\emph{m√©lange continu}. La fonction de densit√© de probabilit√© $u(\theta)$
dans la d√©finition ci-dessous joue le r√¥le des probabilit√©s discr√®tes
$p_j$ de la \autoref{def:simulation:melange-discret}.

\begin{definition}
  Soit $X$ une variable al√©atoire dont la distribution d√©pend d'un
  param√®tre $\theta$. On consid√®re ce param√®tre comme une r√©alisation
  d'une variable al√©atoire $\Theta$ avec fonction de densit√©
  $u(\theta)$. La fonction de densit√© de la variable al√©atoire
  $X|\Theta = \theta$ est $f(x|\theta)$. Par la loi des probabilit√©s
  totales, la fonction de densit√© marginale (ou non conditionnelle) de
  la variable al√©atoire $X$ est
  \begin{equation*}
    f_X(x) = \int_\Omega f(x|\theta) u(\theta)\, d\theta.
  \end{equation*}
  Cette distribution distribution est un m√©lange continu des
  distributions de $X|\Theta$ et $\Theta$.
\end{definition}

Les m√©langes continus sont beaucoup utilis√©s en analyse bayesienne et
dans certaines disciplines de l'actuariat. Plusieurs lois de
probabilit√© sont uniquement d√©finies en tant que m√©langes.

L'algorithme ci-dessous, tr√®s simple, permet d'obtenir des
observations al√©atoires d'un m√©lange de distributions.

\begin{algorithme}[M√©lange continu de deux distributions]
  \label{algo:simulation:melange-continu}
  Soit $f(x)$ la fonction de densit√© de probabilit√© du m√©lange continu
  des distributions avec densit√©s $f(x|\theta)$ et $u(\theta)$. Les
  √©tapes ci-dessous permettent de g√©n√©rer un nombre al√©atoire $x$ de
  la densit√© $f_X$.
  \begin{enumerate}
  \item Obtenir un nombre $\theta$ issu de la distribution avec
    densit√© $u(\theta)$.
  \item Obtenir $x$ de la distribution avec densit√© $f(x|\theta)$.
  \end{enumerate}
\end{algorithme}

Pour simuler plusieurs observations d'un m√©lange, il faut r√©p√©ter les
deux √©tapes de l'\autoref*{algo:simulation:melange-continu} pour
\emph{chaque} observation. Le param√®tre de m√©lange ($\theta$) change
donc √† chaque fois. C'est essentiel, autrement il n'y aurait pas de
m√©lange et nous obtiendrions simplement un √©chantillon de la
distribution $f(x|theta)$ pour une valeur de $\theta$ quelconque.

La simulation de m√©langes continus s'av√®re tr√®s simple √† r√©aliser en R
gr√¢ce √† la nature vectorielle des fonctions de simulation.

\begin{exemple}
  Soit $X$ une variable al√©atoire issue du m√©lange
  $X|\Theta = \theta \sim \text{Poisson}(\theta)$ et
  $\Theta \sim \text{Gamma}(5, 4)$. Les deux expressions R ci-dessous
  permettent de g√©n√©rer \nombre{1000} observations de $X$.
<<echo=TRUE, eval=FALSE>>=
theta <- rgamma(1000, 5, 4)
x <- rpois(1000, theta)
@

  Il est tout-√†-fait raisonnable, ici, d'√©crire le tout en une seule
  expression. Cela fait d'ailleurs bien ressortir le fait que le
  param√®tre de la loi de Poisson est lui-m√™me issu d'une simulation.
<<echo=TRUE, eval=FALSE>>=
x <- rpois(1000, rgamma(1000, 5, 4))
@
  \qed
\end{exemple}

Il est possible de r√©p√©ter la proc√©dure de m√©lange plus d'une fois, ce
qui donne lieu √† ce que nous nommerons des \emph{mod√®les
  hi√©rarchiques}\footnote{%
  En sciences actuarielles, cette terminologie est surtout employ√©e
  dans les contextes de structures de classification des risques sous
  forme d'arbre. Dans de tels cas, le nombre de valeurs √† simuler
  pourrait diff√©rer √† chaque niveau de l'arbre.}. %
La g√©n√©ralisation de l'\autoref{algo:simulation:melange-continu} pour
les mod√®les hi√©rarchique devrait aller de soi.

\begin{exemple}
  Soit le mod√®le hi√©rarchique suivant o√π les variables al√©atoires
  $\Theta$ et $\Lambda$ repr√©sentent les param√®tres de m√©lange:
  \begin{align*}
    X|\Lambda, \Theta &\sim \text{Poisson}(\Lambda) \\
    \Lambda|\Theta    &\sim \text{Gamma}(3, \Theta) \\
    \Theta            &\sim \text{Gamma}(2, 2).
  \end{align*}
  L'expression R suivante permet de g√©n√©rer $n$ observations de la
  variable al√©atoire $X$.
<<echo=TRUE, eval=FALSE>>=
rpois(n, rgamma(n, 3, rgamma(n, 2, 2)))
@
  \qed
\end{exemple}

\subsection{Distributions compos√©es}
\label{sec:simulation:modeles-actuariels:composees}

Les distributions compos√©es jouent un r√¥le central dans la
mod√©lisation des risques en actuariat puisqu'elles permettent de
repr√©senter s√©par√©ment l'effet de la fr√©quence et de la s√©v√©rit√© des
sinistres dans un portefeuille d'assurance.

\begin{definition}
  Soit $N$ une variable al√©atoire discr√®te avec fonction de
  r√©partition $P_N(x)$ et $X_1, X_2, \dots$ des variables al√©atoires
  mutuellement ind√©pendantes, identiquement distribu√©es avec fonction
  de r√©partition $F_X(x)$ et ind√©pendantes de la variable al√©atoire
  $N$. La variable al√©atoire $S = X_1 + \dots + X_N$ --- avec, par
  convention, $S = 0$ lorsque $N = 0$ --- a une distribution compos√©e.
  Sa fonction de r√©partition est
  \begin{equation*}
    F_S(x) = \sum_{n = 0}^\infty F_X^{* n}(x) p_N(n),
  \end{equation*}
  o√π $p_N$ est la fonction de masse de probabilit√© de $N$ et
  $F_X^{* n}$ est la fonction de r√©partition de la $n\ieme$
  convolution de $X$.
\end{definition}

Dans l'appellation de la distribution de la variable al√©atoire $S$,
l'adjectif ¬´compos√©e¬ª est accol√© au nom de la distribution de la
variable al√©atoire $N$. Par exemple, si la distribution de $N$ est une
loi de Poisson, alors la distribution de $S$ est une Poisson compos√©e.
C'est d'ailleurs l√† le cas le plus fr√©quent --- et de loin --- en
mod√©lisation des risques d'assurance.

L'algorithme suivant d√©crit la proc√©dure de simulation d'observations
d'une distribution compos√©e.

\begin{algorithme}[Distribution compos√©e]
  \label{algo:simulation:compound}
  Soit $S = X_1 + \dots + X_N$ une distribution compos√©e. Les √©tapes
  ci-dessous permettent de g√©n√©rer une observation $s$ de la variable
  al√©atoire $S$.
  \begin{enumerate}
  \item Obtenir un nombre $n$ de la distribution avec fonction de
    r√©partition $P_N$.
  \item Obtenir des nombres $x_1, \dots, x_n$ de la distribution avec
    fonction de r√©partition $F_X$.
  \item Poser $s = x_1 + \dots + x_n$.
  \end{enumerate}
\end{algorithme}

Contrairement √† la simulation des m√©langes continus, il n'est pas
possible, en R, de vectoriser la proc√©dure de simulation de
l'\autoref*{algo:simulation:compound}. Il faut avoir recours √† une
boucle ou, mieux, √† une fonction d'application.

La fonction \code{rcompound} du paquetage \pkg{actuar} permet de
g√©n√©rer facilement et intuitivement des nombres al√©atoires d'une
distribution compos√©e quelconque. La fonction \code{rcomppois} offre
une interface simplifi√©e pour le cas le plus courant de la Poisson
compos√©e. Enfin, bien que le sujet se trouve hors de la port√©e du
pr√©sent document, mentionnons que la tr√®s g√©n√©rale fonction
\code{rcomphierarc} permet de g√©n√©rer des observations de mod√®les
hi√©rarchiques compos√©s.

\gotorbox{Le code informatique de la \autoref{sec:simulation:code}
  fournit deux mises en ≈ìuvre de l'\autoref{algo:simulation:compound}
  ainsi que des exemples d'utilisation des fonctions
  \code{rcomppois} et \code{rcompound}.}

\tipbox{L'\autoref{chap:planification} propose quelques trucs pour
  bien planifier une √©tude de simulation en R.}

\begin{figure}
  \centering
  \begin{framed}
\begin{lstlisting}
sim.disque <- function(n)
{
    ## La fonction retourne les coordonn√©es des points
    ## dans une matrice de deux colonnes. Nous cr√©ons
    ## d'abord un contenant.
    X <- matrix(NA, n, 2)

    ## Remplissage de la matrice.
    i <- 1
    repeat
    {
        ## Simulation de coordonn√©es dans un carr√©
        ## 2 x 2 centr√© √† l'origine.
        x <- runif(2, -1, 1)

        ## Si les coordonn√©es sont dans le disque...
        if (sum(x^2) < 1)
        {
            ## ... la paire est ajout√©e √† la matrice 'X'.
            X[i, ] <- x

            ## Il faut cesser apr√®s avoir accept√© 'n' paires.
            if (n < (i <- i + 1))
                break
        }
    }
    X
}
\end{lstlisting}
  \end{framed}
  \caption{Code d'une fonction pour simuler des nombres uniform√©ment
    sur un disque de rayon $1$ centr√© √† l'origine par la m√©thode
    d'acceptation-rejet}
  \label{fig:simulation:sim.disque}
\end{figure}

\begin{prob-solution}
  Nous adoptons la strat√©gie expliqu√©e √† l'astuce de la
  \autopageref{astuce:simulation:2}. La fonction R de la
  \autoref{fig:simulation:sim.disque} gen√®re des points distribu√©s
  uniform√©ment sur un disque par la m√©thode d'acceptation-rejet en
  simulant d'abord des points sur un carr√©.

  Avec en mains cette fonction, il devient simple de v√©rifier par
  simulation la probabilit√© $\frac{35}{12 \pi^2} \approx 0,29552$ en
  proc√©dant comme au \autoref{chap:generation}:
<<echo=FALSE>>=
sim.disque <- function(n)
{
    X <- matrix(NA, n, 2)

    i <- 1
    repeat
    {
        x <- runif(2, -1, 1)

        if (sum(x^2) < 1)
        {
            X[i, ] <- x
            if (n < (i <- i + 1))
                break
        }
    }
    X
}
@
<<<echo=TRUE>>=
mean(replicate(1E5, 3 == length(chull(sim.disque(4)))))
@

  Une autre strat√©gie de simulation se r√©v√®le toutefois plus efficace. Elle
  repose sur la simulation des coordonn√©es polaires (rayon et angle)
  des points sur le disque, puis de leur transformation en coordonn√©es
  cart√©siennes. Si un point du disque unit√© centr√© √† l'origine se trouve √†
  un rayon $r < 1$ du centre et √† un angle $\theta$ de l'horizontale,
  alors ses coordonn√©es cart√©siennes sont
  \begin{align*}
    x &= \sqrt{r} \cos \theta \\
    y &= \sqrt{r} \sin \theta.
  \end{align*}
  La \autoref{fig:simulation:sim.disque2} propose une
  mise en ≈ìuvre de cette m√©thode en R. La d√©monstration de la
  validit√© de cette m√©thode fait quant √† elle l'objet de
  l'\autoref{ex:simulation:disque}.

  Parce qu'elle ne requiert aucune boucle, cette seconde m√©thode est
  \emph{beaucoup} plus rapide que la m√©thode d'acceptation-rejet:
<<echo=FALSE>>=
sim.disque2 <- function(n)
{
    r <- runif(n)
    angle <- runif(n, 0, 2 * pi)
    sqrt(r) * cbind(cos(angle), sin(angle))
}
@
<<echo=TRUE>>=
system.time(sim.disque(1E5))
system.time(sim.disque2(1E5))
@
\end{prob-solution}

\begin{figure}
  \centering
  \begin{framed}
\begin{lstlisting}
sim.disque2 <- function(n)
{
    ## Simulation des coordonn√©es polaires
    r <- runif(n)                       # rayon
    angle <- runif(n, 0, 2 * pi)        # angle

    ## Transformation en coordonn√©es cart√©siennes
    sqrt(r) * cbind(cos(angle), sin(angle))
}
\end{lstlisting}
  \end{framed}
  \caption{Code d'une fonction pour simuler des nombres uniform√©ment
    sur un disque de rayon $1$ centr√© √† l'origine en passant par les
    coordonn√©es polaires}
  \label{fig:simulation:sim.disque2}
\end{figure}


\section{Code informatique}
\label{sec:simulation:code}

\def\scriptfilename{simulation.R}

\scriptfile{\scriptfilename}
\lstinputlisting[firstline=14]{\scriptfilename}


\section{Exercices}
\label{sec:simulation:exercices}

\Opensolutionfile{reponses}[reponses-simulation]
\Opensolutionfile{solutions}[solutions-simulation]

\begin{Filesave}{reponses}
\bigskip
\section*{R√©ponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref*{chap:simulation}}
\addcontentsline{toc}{section}{Chapitre \ref*{chap:simulation}}

\end{Filesave}

\begin{exercice}
  La transformation de Box--Muller est populaire pour simuler des
  nombres normaux √† partir de nombres uniformes. Soit $U_1 \sim U(0,
  1)$ et $U_2 \sim U(0, 1)$ deux variables al√©atoires ind√©pendantes et
  \begin{align*}
    X_1 &= (-2 \log U_1)^{1/2} \cos (2\pi U_2) \\
    X_2 &= (-2 \log U_1)^{1/2} \sin (2\pi U_2).
  \end{align*}
  \begin{enumerate}
  \item V√©rifier de mani√®re heuristique que la transformation
    ci-dessus est bijective de $\{(u_1, u_2); 0 < u_1 < 1, 0 < u_2 <
    1\}$ √† $\{(x_1, x_2); -\infty < x_1 < \infty, -\infty < x_2 <
    \infty\}$, c'est-√†-dire qu'elle associe √† un point $(u_1, u_2)$ un
    et un seul point $(x_1, x_2)$.
  \item D√©montrer que la transformation inverse est
    \begin{align*}
      U_1 &= e^{-(X_1^2 + X_2^2)/2} \\
      U_2 &= \frac{1}{2 \pi} \arctan \frac{X_2}{X_1}.
    \end{align*}
  \item D√©montrer que $X_1$ et $X_2$ sont deux variables al√©atoires
    ind√©pendantes chacune distribu√©e selon une $N(0, 1)$.
  \item V√©rifier empiriquement la validit√© de ces formules √† l'aide de
    Excel ou de R. Dans R, il est possible de transformer les nombres
    uniformes obtenus avec la fonction \code{runif} en nombres normaux
    sans m√™me utiliser de boucles gr√¢ce √† la fonction \code{outer} et
    deux fonctions anonymes.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Tout d'abord, on voit que
      \begin{align*}
        \cos (2\pi U_{2}) &\in (-1, 1) \\
        \sin (2\pi U_{2}) &\in (-1, 1) \\
        \intertext{et}
        (-2 \log U_1)^{1/2} &\in (0, \infty).
      \end{align*}
      Par cons√©quent, $X_1 \in (-\infty, \infty)$ et $X_2 \in
      (-\infty, \infty)$. On v√©rifie la bijectivit√© de fa√ßon
      heuristique avec quelques valeurs de $u_1$ et $u_2$.
    \item On a
      \begin{align*}
        X_1^2 &= (-2 \log U_1) \cos^2 (2\pi U_2) \\
        X_2^2 &= (-2 \log U_1) \sin^2 (2\pi U_2).
      \end{align*}
      Or, puisque $\sin^{2}(x) + \cos^{2}(x) = 1$, $X_1^2 + x_2^2 = -2
      \log U_1$, d'o√π $U_1 = e^{-(X_1^2 + X_2^2)/2}$. D'autre part,
      $\sin(x)/\cos(x) = \tan(x)$, donc $\tan (2\pi U_2) = X_2/X_1$
      ou, de mani√®re √©quivalente, $U_2 = (2 \pi)^{-1} \arctan
      X_2/X_1$.
    \item Soit les fonctions
      \begin{align*}
        x_1(u_1, u_2)
        &= (-2\log u_1)^{1/2} \cos (2\pi u_2)  &
        u_1(x_1,x_2)
        &= e^{-(x_1^2 + x_2^2)/2}  \\
        x_2(u_1, u_2)
        &= (-2\log u_1)^{1/2} \sin (2\pi u_2) &
        u_2(x_1, x_2)
        &= \frac{1}{2\pi} \arctan \frac{x_2}{x_1}.
      \end{align*}
      Les variables al√©atoires $U_1$ et $U_2$ sont ind√©pendantes, donc
      leur fonction de densit√© de probabilit√© conjointe est le produit
      des densit√©s marginales:
      \begin{displaymath}
        f_{U_1, U_2}(u_1, u_2) = 1, \quad 0 < u_1 < 1, 0 < u_2 < 1.
      \end{displaymath}
      La densit√© conjointe de $X_1$ et $X_2$ est, par d√©finition d'une
      transformation,
      \begin{displaymath}
        f_{X_1,X_2}(x_1, x_2) =
        f_{U_1, U_2}(x_1(u_1, u_2), x_2(u_1,u_2)) |\det(J)|,
      \end{displaymath}%
      o√π
      \begin{equation*}
        J =
        \begin{bmatrix}
          \dfrac{\partial u_1}{\partial x_1} &
          \dfrac{\partial u_1}{\partial x_2} \\[6pt]
          \dfrac{\partial u_2}{\partial x_1} &
          \dfrac{\partial u_2}{\partial x_2}
        \end{bmatrix} =
        \begin{bmatrix}
          -x_1 e^{- (x_1^2 + x_2^2)/2} &
          -x_2 e^{- (x_1^2 + x_2^2)/2} \\[6pt]
          -\dfrac{1}{2\pi} \dfrac{x_2}{x_1^2 + x_2^2} &
          \dfrac{1}{2\pi} \dfrac{x_1}{x_1^2 + x_2^2}
        \end{bmatrix}.
      \end{equation*}
      Or,
      \begin{align*}
        |\det(J)|
        &= \frac{1}{2\pi}\, e^{-(x_1^2 + x_2^2)/2} \\
        &= \frac{1}{\sqrt{2\pi}}\, e^{-x_1^2/2} \cdot
        \frac{1}{\sqrt{2\pi}} e^{-x_2^2/2},
      \end{align*}
      d'o√π
      \begin{displaymath}
        f_{X_1,X_2}(x_1, x_2) = \frac{1}{\sqrt{2\pi}} e^{-x_1^2/2} \cdot
        \frac{1}{\sqrt{2\pi}} e^{-x_2^2/2}.
      \end{displaymath}
      Par cons√©quent, $X_1$ et $X_2$ sont deux variables al√©atoires $N(0, 1)$
      ind√©pendantes.
    \item
<<echo=TRUE, eval=FALSE>>=
u1 <- runif(500)
u2 <- runif(500)
x1 <- outer(u1, u2, function(x, y)
            sqrt((-2 * log(x))) * cos(2 * pi * y))
x2 <- outer(u1, u2, function(x, y)
            sqrt((-2 * log(x))) * sin(2 * pi * y))
hist(x1, prob = TRUE)
curve(dnorm(x), add = TRUE)
@
    \end{enumerate}
    La \autoref{fig:simulation:boxmuller} illustre d'une autre fa√ßon que la
    transformation de Box--Muller fonctionne bel et bien. Le
    graphique de gauche contient plusieurs couples de points $(u_1, u_2)$,
    o√π chaque composante provient d'une distribution uniforme sur
    l'intervalle $(0, 1)$.

    Chacun de ces points a √©t√© transform√© en un point $(x_1, x_2)$
    selon la transformation de Box--Muller, puis plac√© dans le
    graphique de droite. Nous avons superpos√© le nuage de points ainsi
    obtenu aux lignes de niveau d'une distribution normale bivari√©e
    (avec param√®tre $\rho = 0$). Vous observerez que la r√©partition et la
    densit√© du nuage de points correspond effectivement aux lignes de
    niveau.
    \begin{figure}
      \centering
<<echo=FALSE, fig=TRUE, width=10, height=5>>=
u1 <- runif(1000)
u2 <- runif(1000)
x1 <- sqrt(-2 * log(u1)) * cos(2 * pi * u2)
x2 <- sqrt(-2 * log(u1)) * sin(2 * pi * u2)

col <- rgb(0.67, 0.85, 0.90, alpha = 0.4) # "lightblue" semi-transparent
par(mfrow = c(1, 2))

plot(u1, u2, pch = 19, col = col)

f <- function(x, y) dnorm(x) * dnorm(y)
x <- seq(-3, 3, length = 100)
z <- outer(x, x, f)
contour(x, x, z, xlim = c(-3, 3), ylim = c(-3, 3),
        nlevels = 15, method = "edge", xlab = "x1", ylab = "x2")
points(x1, x2, pch = 19, col = col)
@
      \caption{D√©monstration graphique du fonctionnement de la
        transformation de Box--Muller}
      \label{fig:simulation:boxmuller}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  La distribution de Laplace, ou double exponentielle, est d√©finie
  comme la diff√©rence entre deux distributions exponentielles
  identiques et ind√©pendantes. Sa fonction de densit√© de probabilit√©
  est
  \begin{displaymath}
    f(x) = \frac{\lambda}{2}\, e^{-\lambda |x|}, \quad
    -\infty < x < \infty.
  \end{displaymath}
  Proposer une ou plusieurs fa√ßons de simuler des nombres issus de
  cette distribution.
  \begin{sol}
    Deux suggestions:
    \begin{enumerate}[1.]
    \item Simuler deux nombres ind√©pendants $x_1$ et $x_2$ d'une loi
      exponentielle de param√®tre $\lambda$ et poser $y = x_1 - x_2$.
    \item La fonction de r√©partition de la distribution de Laplace est
      \begin{align*}
        F_{Y}(y)
        &=
        \begin{cases}
          \frac{1}{2}e^{\lambda x}, & x < 0 \\
          1-\frac{1}{2}e^{-\lambda x}, & x \geq 0,
        \end{cases} \\
        \intertext{d'o√π}
        F_{Y}^{-1}(u)
        &=
        \begin{cases}
          \frac{1}{\lambda} \ln (2u) & u < 0,5 \\
          \frac{-1}{\lambda} \ln (2(1-u) ) & u \geq 0,5.
        \end{cases}
      \end{align*}
      Il est donc aussi possible d'utiliser la m√©thode de l'inverse.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  En ouverture de chapitre, nous mentionnons qu'une fa√ßon de simuler des
  nombres issus d'une loi g√©om√©trique consiste √† extraire la partie
  enti√®re de nombres al√©atoires provenant d'une loi exponentielle.

  D√©montrer que si la loi de la variable al√©atoire $X$ est une
  exponentielle, alors la loi de $\lfloor X \rfloor$ est une
  g√©om√©trique. D√©terminer le param√®tre de la loi g√©om√©trique en
  fonction du param√®tre de la loi exponentielle.
  \begin{rep}
    $p = 1 - e^{-\lambda}$
  \end{rep}
  \begin{sol}
    Soit $X \sim \text{Exponentielle}(\lambda)$ et
    $K = \lfloor X \rfloor$. Par cons√©quent:
    \begin{align*}
      \Pr[K = 0]
      &= \Pr[\lfloor X \rfloor = 0] \\
      &= \Pr[0 \leq X < 1] \\
      &= F_X(1) \\
      &= 1 - e^{-\lambda}, \displaybreak[0] \\
      \Pr[K = 1]
      &= \Pr[1 \leq X < 2]\\
      &= F_X(2) - F_X(1)\\
      &= (1 - e^{-2 \lambda}) - (1 - e^{-\lambda}) \\
      &= (e^{-\lambda} - e^{-2\lambda}) \\
      &= (1 - e^{-\lambda}) e^{-\lambda}, \displaybreak[0] \\
      \intertext{soit, de mani√®re g√©n√©rale,}
      \Pr[K = k]
      &= \Pr[k \leq X < k+1]\\
      &= (1 - e^{-\lambda})(e^{-\lambda})^k,
        \quad k = 0, 1, 2, \dots
    \end{align*}
    La forme de cette fonction de masse de probabilit√© est celle
    d'une loi g√©om√©trique de param√®tre $p = 1 - e^{-\lambda}$.
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:gamma}
  R√©aliser une mise en ≈ìuvre informatique de l'algorithme de
  simulation suivant. Il s'agit d'un algorithme pour simuler des
  observations d'une loi Gamma$(\alpha, 1)$, o√π $\alpha > 1$.
  \begin{enumerate}[1.]
  \item G√©n√©rer $u_1$ et $u_2$ ind√©pendemment d'une loi $U(0, 1)$ et
    poser
    \begin{displaymath}
      v = \frac{(\alpha - \frac{1}{6\alpha}) u_1}{(\alpha - 1) u_2}.
    \end{displaymath}
  \item Si
    \begin{displaymath}
      \frac{2 (u_2 - 1)}{\alpha - 1} + v + \frac{1}{v} \leq 2,
    \end{displaymath}
    alors retourner le nombre $x = (\alpha - 1) v$. Sinon, si
    \begin{displaymath}
      \frac{2 \log u_2}{\alpha - 1} - \log v + v \leq 1,
    \end{displaymath}
    alors retourner le nombre $x = (\alpha - 1) v$.
  \item R√©p√©ter au besoin la proc√©dure depuis l'√©tape 1.
  \end{enumerate}
  Faire les v√©rifications empiriques usuelles de la validit√© de
  l'algorithme.
  \begin{sol}
    Voir la fonction R de la \autoref{fig:simulation:gamma}. Les
    expressions suivantes permettent de tracer les graphiques pour v√©rifier
    la validit√© de l'algorithme.
<<echo=TRUE, eval=FALSE>>=
hist(rgamma2(1000, 5), prob = TRUE)
curve(dgamma(x, 5, 1), add = TRUE)
@
    \begin{figure}
      \centering
      \begin{framed}
\begin{lstlisting}
rgamma2 <- function(nsim, alpha)
{
    x <- numeric(nsim)
    i <- 0
    while (i < nsim)
    {
        u <- runif(2)
        v <- (alpha - 1/(6 * alpha)) * u[1] /
               ((alpha - 1) * u[2])

        if ((2 * (u[2] - 1)/(alpha - 1) +
               v + 1/v <= 2) |
            (2 * log(u[2])/(alpha - 1) -
               log(v) + v <= 1))
            x[i <- i + 1] <- (alpha - 1) * v
    }
    x
}
\end{lstlisting}
      \end{framed}
      \caption{Fonction de simulation d'une distribution
        Gamma$(\alpha, 1)$, $\alpha > 1$}
      \label{fig:simulation:gamma}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  En utilisant le r√©sultat de l'exercice pr√©c√©dent, quelle proc√©dure
  pourriez-vous suivre pour simuler des nombres d'une loi Gamma$(\alpha,
  \lambda)$ o√π $\alpha > 1$?
  \begin{sol}
    Deux suggestions.
    \begin{enumerate}[1.]
    \item Si $X \sim \text{Gamma}(\alpha, 1)$, alors $Y = X/\lambda
      \sim \text{Gamma}(\alpha, \lambda)$. Nous pouvons donc g√©n√©rer un
      nombre $x$ d'une loi Gamma$(\alpha, 1)$ avec l'algorithme de
      l'\autoref{ex:simulation:gamma}, puis poser $y =
      x/\lambda$.
    \item Si $\alpha$ est entier, nous pouvons √©galement g√©n√©rer
      $\alpha$ nombres $x_1, \dots, x_\alpha$ d'une distribution
      Exponentielle$(\lambda)$ et poser $y = \sum_{i=1}^\alpha x_i$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item D√©montrer que si $X|\Theta \sim \text{Exponentielle}(\Theta)$
    et $\Theta \sim \text{Gamma}(\alpha, \lambda)$, alors $X \sim
    \text{Pareto}(\alpha, \lambda)$. La fonction de densit√© de
    probabilit√© d'une loi de Pareto est
    \begin{displaymath}
      f(x) = \frac{\alpha \lambda^\alpha}{(x + \lambda)^{\alpha + 1}},
      \quad x > 0.
    \end{displaymath}
  \item Utiliser le r√©sultat ci-dessus pour proposer un algorithme de
    simulation de nombres issus d'une loi de Pareto. Faire la mise en
    ≈ìuvre informatique de cet algorithme et les v√©rifications d'usage
    de sa validit√©.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Vous aurez reconnu dans cette combinaison de variables
      al√©atoires un m√©lange d'une exponentielle et d'une gamma. Par la
      loi des probabilit√©s totales,
      \begin{align*}
        f_{X}(x)
        &= \int_0^\infty f(x|\theta) u(\theta)\, d\theta  \\
        &= \frac{\lambda^\alpha}{\Gamma(\alpha)} \int_0^\infty
        \theta^{\alpha + 1 - 1} e^{-(\lambda + x) \theta}\, d\theta
          \displaybreak[0]\\
        &= \frac{\lambda^\alpha}{\Gamma(\alpha)}
        \frac{\Gamma(\alpha + 1)}{(\lambda + x)^{\alpha +1}} \\
        &= \frac{\alpha \lambda^\alpha}{(\lambda + x)^{\alpha +1}}.
      \end{align*}
    \item Pour g√©n√©rer un nombre d'une distribution de Pareto de
      param√®tres $\alpha$ et $\lambda$ avec le r√©sultat en a), il faut
      d'abord obtenir un nombre $\theta$ d'une distribution gamma de
      m√™mes param√®tres, puis obtenir un nombre $x$ d'une distribution
      exponentielle de param√®tre $\theta$.
<<echo=TRUE, eval=FALSE>>=
rexp(1, rgamma(1, alpha, lambda))
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  La fonction de densit√© de probabilit√© de la loi de
  Pareto translat√©e est
  \begin{displaymath}
    f(x) = \frac{\alpha \lambda^\alpha}{x^{\alpha + 1}}, \quad x > \lambda.
  \end{displaymath}
  Simuler trois valeurs d'une telle distribution avec $\alpha = 2$ et
  $\lambda = \nombre{1000}$ √† l'aide de la m√©thode de l'inverse et du
  g√©n√©rateur congruentiel lin√©aire suivant:
  \begin{displaymath}
    x_n = (65 x_{n-1} + 1) \bmod \nombre{2048}.
  \end{displaymath}
  Utiliser une amorce de $12$.
  \begin{rep}
    $\nombre{\Sexpr{round(1000/sqrt(1 - 0.3813))}}$,
    $\nombre{\Sexpr{round(1000/sqrt(1 - 0.7881))}}$,
    $\nombre{\Sexpr{round(1000/sqrt(1 - 0.2261))}}$
  \end{rep}
  \begin{sol}
    La fonction de r√©partition de la Pareto translat√©e$(\alpha,
    \lambda)$ est
    \begin{align*}
      F(x)
      &= \int_\lambda^x
      \frac{\alpha \lambda^\alpha}{y^{\alpha + 1}}\, dy \\
      &=
      \begin{cases}
        0, & x \leq \lambda \\
        1 - \left( \frac{\lambda}{x} \right)^\alpha, & x > \lambda
      \end{cases} \\
      \intertext{et son inverse est}
      F^{-1}(y)
      &=
      \begin{cases}
        \lambda, & y = 0 \\
        \frac{\lambda}{(1 - y)^{1/\alpha}}, & 0 < y < 1.
      \end{cases}
    \end{align*}
    Par cons√©quent, si $U \sim U(0, 1)$, alors
    \begin{displaymath}
      X = \frac{\lambda}{(1 - U)^{1/\alpha}} \sim
      \text{Pareto translat√©e}(\alpha, \lambda).
    \end{displaymath}
    Les trois premi√®res valeurs retourn√©es par le g√©n√©rateur
    \begin{displaymath}
      x_n = (65 x_{n-1} + 1) \bmod \nombre{2048}
    \end{displaymath}
    avec une amorce de $12$ sont $781$, $\nombre{1614}$ et $463$. La division
    de ces nombres par $\nombre{2048}$ fournit des nombres dans
    l'intervalle $(0, 1)$:
    \begin{displaymath}
      0,3813 \quad 0,7881 \quad 0,2261.
    \end{displaymath}
    Finalement, les observations de la $\text{Pareto}(2,
    \nombre{1000})$ sont les suivantes.
<<echo=TRUE>>=
1000/sqrt(1 - c(0.3813, 0.7881, 0.2261))
@
    \emph{Remarque:} puisque $1 - U \sim U(0, 1)$ si $U \sim U(0, 1)$,
    les nombres issus de la transformation $\lambda (1 -
    U)^{-1/\alpha}$ seraient tout aussi distribu√©s selon une Pareto
    translat√©e. Les r√©ponses seraient toutefois diff√©rentes.
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:disque}
  Soit $U_1$ et $U_2$ deux variables al√©atoires ind√©pendantes
  uniform√©ment distribu√©es sur l'intervalle $(0, 1)$ et soit la
  transformation
  \begin{align*}
    X_1 &= \sqrt{U_1} \cos(2 \pi U_2) \\
    X_2 &= \sqrt{U_1} \sin(2 \pi U_2).
  \end{align*}
  D√©montrer que la distribution conjointe de $X_1$ et $X_2$ est
  uniforme sur le disque de rayon de $1$ centr√© en $(x_1, x_2) = (0,
  0)$. √Ä quoi ce r√©sultat peut-il servir?
  \begin{sol}
    Nous avons la transformation
    \begin{displaymath}
      \begin{split}
        X_1 &= \sqrt{U_1} \cos(2 \pi U_2) \\
        X_2 &= \sqrt{U_1} \sin(2 \pi U_2)
      \end{split}
      \quad \Leftrightarrow \quad
      \begin{split}
        U_1 &= X_1^2 + X_2^2 \\
        U_2 &= \frac{1}{2\pi} \arctan \frac{X_2}{X_1}.
      \end{split}
    \end{displaymath}
    Cette transformation associe les points de l'espace $\{(u_1, u_2):
    0 < u_1 < 1, 0 < u_2 < 1\}$ √† ceux de l'espace $\{(x_1, x_2):
    x_1^2 + x_2^2 < 1 \backslash (0, 0)\}$. Cela se v√©rifie ais√©ment
    en examinant les limites de l'espace de d√©part:
    \begin{align*}
      u_1 &> 0 &\Rightarrow&& x_1^2 + x_2^2 &> 0 \\
      u_1 &< 1 &\Rightarrow&& x_1^2 + x_2^2 &< 1 \\
      u_2 &> 0 &\Rightarrow&& \frac{x_2}{x_1} &> 0 \\
      u_2 &< 1 &\Rightarrow&& \frac{x_2}{x_1} &< 0.
    \end{align*}
    Les troisi√®me et quatri√®me in√©galit√©s d√©finissent les quadrants I
    et III, puis II et IV de $\R^2$, respectivement. Vous remarquerez
    que le point $(0, 0)$, qui a probabilit√© z√©ro, ne se trouve pas
    dans l'espace image.

    Le Jacobien de la transformation est
    \begin{align*}
      J
      &=
      \begin{vmatrix}
        \dfrac{\partial u_1}{\partial x_1} &
        \dfrac{\partial u_1}{\partial x_2} \\[8pt]
        \dfrac{\partial u_2}{\partial x_1} &
        \dfrac{\partial u_2}{\partial x_2}
      \end{vmatrix} \\
      &=
      \begin{vmatrix}
        2 x_1 &
        2 x_2 \\[6pt]
        -\dfrac{1}{2\pi} \dfrac{x_2}{x_1^2 + x_2^2} &
        \dfrac{1}{2\pi} \dfrac{x_1}{x_1^2 + x_2^2},
      \end{vmatrix} \\
      &=
      \frac{1}{\pi}.
    \end{align*}
    La fonction de densit√© de probabilit√© conjointe de $X_1$ et $X_2$
    est donc
    \begin{align*}
      f_{X_1,X_2}(x_1, x_2)
      &= f_{U_1, U_2}(u_1, u_2) |J| \\
      &= \frac{1}{\pi}, \quad -1 < x_1 < 1, -\sqrt{1 - x_1^2} < x_2 <
      \sqrt{1 - x_1^2},
    \end{align*}
    soit une distribution uniforme sur le disque unit√©.

    Le r√©sultat peut √©videmment servir √† simuler des points
    uniform√©ment r√©partis sur un disque de rayon $1$ centr√© en $(0,
    0)$. La \autoref{fig:simulation:disque2} illustre d'ailleurs cette
    transformation. Les points $(u_1, u_2)$ dans le graphique de gauche
    sont tir√©s al√©atoirement sur le carr√© $(0, 1) \times (0, 1)$. Le
    graphique de droite montre que suite √† la transformation ci-dessus,
    les points $(x_1, x_2)$ sont distribu√©s uniform√©ment sur un
    disque de rayon $1$ centr√© en $(0, 0)$.
    \begin{figure}
      \centering
<<echo=FALSE, fig=TRUE, width=10, height=5>>=
col <- rgb(0.67, 0.85, 0.90, alpha = 0.4) # "lightblue" semi-transparent
par(mfrow = c(1, 2))
u1 <- runif(1000)
u2 <- runif(1000)
x1 <- sqrt(u1) * cos(2 * pi * u2)
x2 <- sqrt(u1) * sin(2 * pi * u2)
plot(u1, u2, pch = 19, col = col)
plot(x1, x2, pch = 19, col = col)
@
      \caption{D√©monstration graphique du fonctionnement de la
        transformation de l'\autoref{ex:simulation:disque}}
      \label{fig:simulation:disque2}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:gamma-beta}
  \begin{enumerate}
  \item Soit $Y_1 \sim \text{Gamma}(\alpha, 1)$ et $Y_2 \sim
    \text{Gamma}(\beta, 1)$ deux variables al√©atoires
    ind√©pendantes. D√©montrer que
    \begin{displaymath}
      X = \frac{Y_1}{Y_1 + Y_2} \sim \text{B√™ta}(\alpha, \lambda).
    \end{displaymath}
  \item Utiliser le r√©sultat en a) pour proposer un algorithme de
    simulation d'observations d'une loi B√™ta$(\alpha, \lambda)$.
  \item Faire la mise en ≈ìuvre informatique de l'algorithme en b)
    ainsi que les v√©rifications d'usage.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Nous avons
      \begin{align*}
        f_{Y_1}(y_1)
        &= \frac{1}{\Gamma(\alpha)}\, y_1^{\alpha - 1} e^{-y_1}, \quad
        y_1 > 0, \\
        f_{Y_2}(y_2)
        &= \frac{1}{\Gamma(\beta)}\, y_2^{\beta - 1} e^{-y_2}, \quad
        y_2 > 0 \\
        \intertext{et}
        f_{Y_1, Y_2}(y_1, y_2)
        &= \frac{1}{\Gamma(\alpha) \Gamma(\beta)}\,
        y_1^{\alpha - 1} y_2^{\beta - 1} e^{-(y_1 + y_2)}, \quad
        y_1 > 0, y_2 > 0.
      \end{align*}
      Soit $X_1 = Y_1/(Y_1 + Y_2)$ et $X_2 = Y_1 + Y_2$ (le choix de
      $X_2$ √©tant justifi√© par l'exposant de la distribution conjointe
      de $Y_1$ et $Y_2$). Nous cherchons la distribution conjointe de
      $X_1$ et $X_2$, $f_{X_1, X_2}(x_1, x_2)$ √† partir de la transformation
      \begin{displaymath}
        \begin{split}
          x_1 &= \frac{y_1}{y_1 + y_2} \\
          x_2 &= y_1 + y_2
        \end{split}
        \quad \Leftrightarrow \quad
        \begin{split}
          y_1 &= x_1 x_2 \\
          y_2 &= x_2 - x_1 x_2.
        \end{split}
      \end{displaymath}
      Cette transformation associe de mani√®re √©vidente les points de
      l'espace $\{(y_1, y_2): y_1 > 0, y_2 > 0\}$ √† ceux de l'espace
      $\{(x_1, x_2): 0 < x_1 < 1, x_2 > 0\}$.

      Le Jacobien de la transformation est
      \begin{align*}
        J
        &=
        \begin{vmatrix}
          \dfrac{\partial y_1}{\partial x_1} &
          \dfrac{\partial y_1}{\partial x_2} \\[8pt]
          \dfrac{\partial y_2}{\partial x_1} &
          \dfrac{\partial y_2}{\partial x_2}
        \end{vmatrix} \\
        &=
        \begin{vmatrix}
          x_2 & x_1 \\
          -x_2 & 1 - x_1
        \end{vmatrix} \\
        &=
        x_2.
      \end{align*}
      La fonction de densit√© de probabilit√© conjointe de $X_1$ et $X_2$
      est donc
      \begin{align*}
        f_{X_1,X_2}&(x_1, x_2)
         = f_{Y_1, Y_2}(y_1, y_2) |J| \\
        &= \frac{1}{\Gamma(\alpha) \Gamma(\beta)}\,
        x_1^{\alpha - 1} (1 - x_1)^{\beta - 1}
        x_2^{\alpha + \beta - 1} e^{-x_2} \\
        &=
        \left[
          \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}\,
          x_1^{\alpha - 1} (1 - x_1)^{\beta - 1}
        \right]
        \left[
          \frac{1}{\Gamma(\alpha + \beta)}\,
          x_2^{\alpha + \beta - 1} e^{-x_2}
        \right],
      \end{align*}
      pour $0 < x_1 < 1$, $x_2 > 0$, d'o√π $X_1$ et $X_2$ sont
      ind√©pendantes, $X_1 \sim \text{B√™ta}(\alpha, \beta)$ et $X_2
      \sim \text{Gamma}(\alpha + \beta)$ (un r√©sultat connu).
    \item La conversion du r√©sultat en un algorithme est tr√®s simple:
      \begin{quote}
        \begin{enumerate}[1.]
        \item G√©n√©rer $y_1$ d'une distribution Gamma$(\alpha, 1)$.
        \item G√©n√©rer $y_2$ d'une distribution Gamma$(\beta, 1)$.
        \item Poser $x = y_1/(y_1 + y_2)$.
        \end{enumerate}
      \end{quote}
      Cet algorithme suppose √©videmment qu'une source de nombres
      provenant d'une loi gamma est disponible.

      La \autoref{fig:simulation:gamma-beta} illustre le fonctionnement de
      cette transformation. Dans le graphique de gauche, nous avons un nuage
      de points $(y_1, y_2)$ tir√©s de mani√®re ind√©pendante de deux
      distributions gamma de param√®tre d'√©chelle √©gal √† $1$. Nous avons
      superpos√© ce nuage de points aux courbes de niveaux de la
      distribution conjointe des deux lois gamma.

      Dans le graphique de droite, les points $x = y_1/(y_1 + y_2)$
      r√©sultant de la transformation sont plac√©s en abscisse. Vous
      constaterez que la r√©partition et la densit√© de ces points
      correspond √† la densit√© de la loi b√™ta √©galement repr√©sent√©e sur
      le graphique.
      \begin{figure}
        \centering
<<echo=FALSE, fig=TRUE, width=10, height=5>>=
col <- rgb(0.67, 0.85, 0.90, alpha = 0.4) # "lightblue" semi-transparent
par(mfrow = c(1, 2))
f <- function(x, y) dgamma(x, 2) * dgamma(y, 3)
y <- seq(0, 8, length = 100)
z <- outer(y, y, f)
contour(y, y, z, xlim = c(0, 8), ylim = c(0, 8),
        nlevels = 15, method = "edge",
        xlab = expression(y[1]), ylab = expression(y[2]))

y1 <- rgamma(100, 2)
y2 <- rgamma(100, 3)
points(y1, y2, pch = 19, col = col)

curve(dbeta(x, 2, 3), xlim = c(0, 1), lwd = 2)
points(y1/(y1 + y2), rep(0, length(y1)), pch = 19, col = col)
@
        \caption{D√©monstration graphique du fonctionnement de la
          transformation de l'\autoref{ex:simulation:gamma-beta}}
        \label{fig:simulation:gamma-beta}
      \end{figure}
    \item En R:
<<echo=TRUE,eval=FALSE>>=
(y <- rgamma(1, alpha, 1))/(y + rgamma(1, beta, 1))
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item Dans la m√©thode d'acceptation-rejet, un nombre $y$ tir√© d'une
    variable al√©atoire $Y$ avec fonction de densit√© de probabilit√©
    $g_Y(\cdot)$ est accept√© comme r√©alisation d'une variable
    al√©atoire $X$ avec fonction de densit√© de probabilit√© $f_X(\cdot)$ si
    \begin{displaymath}
      U \leq \frac{f_X(y)}{c g_Y(y)},
    \end{displaymath}
    o√π $U \sim U(0, 1)$. Calculer la probabilit√© d'accepter une valeur
    lors de toute it√©ration de la m√©thode d'acceptation-rejet,
    c'est-√†-dire
    \begin{displaymath}
      \Pr \left[ U \leq \frac{f_X(Y)}{c g_Y(Y)} \right].
    \end{displaymath}
    \emph{Astuce}: utiliser la loi des probabilit√©s totales en
    conditionnant sur $Y = y$.
  \item D√©terminer la distribution du nombre d'essais avant d'accepter
    un nombre $y$ dans la m√©thode d'acceptation-rejet.
  \item D√©terminer le nombre moyen d'essais avant d'accepter un nombre
    $y$ dans la m√©thode d'acceptation-rejet.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $1/c$
    \item G√©om√©trique$(1/c)$ commen√ßant √† $1$
    \item $c$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Nous avons
      \begin{align*}
        \Pr \left[ U \leq \frac{f_X(y)}{c g_Y(y)} \right]
        &= \int_{-\infty}^\infty
        \Pr \left[ U \leq \frac{f_X(y)}{c g_Y(y)}\biggr\rvert Y = y \right] %|
        g_Y(y)\, dy \\
        &= \int_{-\infty}^\infty
        \frac{f_X(y)}{c g_Y(y)}\, g_Y(y)\, dy \displaybreak[0] \\
        &= \frac{1}{c} \int_{-\infty}^\infty f_X(y)\, dy \\
        &= \frac{1}{c}.
      \end{align*}
    \item Les essais √©tant ind√©pendants, la distribution du nombre
      d'essais avant d'obtenir un succ√®s (accepter un nombre $y$) est
      g√©om√©trique de param√®tre $1/c$, c'est-√†-dire que
      \begin{displaymath}
        \Pr[Z = z] =
        \left( \frac{1}{c} \right)
        \left( 1 - \frac{1}{c} \right)^{z - 1}, \quad z = 1, 2, \dots,
      \end{displaymath}
      o√π $Z$ repr√©sente le nombre d'essais avant d'accepter un nombre.
    \item Nous savons que $\esp{Z} = 1/(1/c) = c$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:mode}
  Soit $X$ une variable al√©atoire continue d√©finie sur l'intervalle
  $(a, b)$, o√π $a$ et $b$ sont des nombres r√©els. Pour simuler des
  observations de cette variable al√©atoire par la m√©thode
  d'acceptation-rejet, il est toujours possible d'inscrire la fonction de
  densit√© de probabilit√© de $X$ dans un rectangle de hauteur $M$, ou
  $M$ est la valeur de la densit√© au mode de $X$.
  \begin{enumerate}
  \item √ânoncer l'algorithme d'acceptation-rejet d√©coulant d'une telle
    proc√©dure.
  \item Calculer l'\emph{efficacit√©} de l'algorithme en a), soit la
    probabilit√© d'accepter une valeur lors d'une it√©ration de
    l'algorithme.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
    \item $1/(M (b - a))$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Posons
      \begin{displaymath}
        c g_Y(x) = M, \quad a < x < b,
      \end{displaymath}
      soit $Y \sim U(a, b)$ et $c = M (b - a)$. L'algorithme
      d'acceptation-rejet est donc le suivant:
      \begin{enumerate}[1.]
      \item Simuler deux nombres ind√©pendants $u_1$ et $u_2$ d'une
        loi $U(0, 1)$.
      \item Poser $y = a + (b - a) u_1$.
      \item Si $u_2 \leq f_X(y)/M$, poser $x = y$. Sinon, retourner
        √† l'√©tape 1.
      \end{enumerate}
    \item L'efficacit√© est
      \begin{displaymath}
        \frac{1}{c} = \frac{1}{M (b - a)}.
      \end{displaymath}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:beta}
  Effectuer les calculs ci-dessous dans le contexte de la simulation
  d'observations d'une loi B√™ta$(3, 2)$ √† l'aide de la m√©thode
  d'acceptation-rejet.
  \begin{enumerate}
  \item Calculer l'efficacit√© de l'algorithme d√©velopp√© √†
    l'\autoref{ex:simulation:mode} et √†
    l'\autoref{exemple:simulation:beta:1} pour le cas pr√©sent.
  \item Calculer l'efficacit√© de l'algorithme d√©velopp√© dans
    l'\autoref{exemple:simulation:beta:2}, o√π nous avons d√©termin√© que
    \begin{displaymath}
      f_{X}(x) \leq
      \begin{cases}
        3x, & 0 < x < 0,8 \\
        12 - 12 x, & 0,8 \leq x < 1.
      \end{cases}
    \end{displaymath}
  \item Faire la mise en ≈ìuvre informatique de l'algorithme le plus
    efficace entre celui de la partie a) et celui de la partie b).
    V√©rifier la fonction en superposant l'histogramme d'un grand
    √©chantillon obtenu avec cette fonction et la vraie fonction de
    densit√© de la loi b√™ta.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $9/16$
    \item $4/5$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il est facile de d√©montrer que le mode $M$ d'une distribution
      b√™ta de param√®tres $\alpha$ et $\beta$ se trouve en
      \begin{displaymath}
        x = \frac{\alpha - 1}{\alpha + \beta - 2}.
      \end{displaymath}
      Par cons√©quent, l'efficacit√© de l'algorithme d'acceptation-rejet
      d√©crit √† l'\autoref{ex:simulation:mode} et consistant √†
      borner la densit√© par un rectangle de hauteur $M$ est
      \begin{align*}
        \frac{1}{M}
        &= \frac{1}{f((\alpha - 1)/(\alpha + \beta - 2))} \\
        &= \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}
        \left(
          \frac{\alpha - 1}{\alpha + \beta - 2}
        \right)^{1 - \alpha}
        \left(
          \frac{\beta - 1}{\alpha + \beta - 2}
        \right)^{1 - \beta}.
      \end{align*}
      Avec $\alpha = 3$ et $\beta = 2$, l'efficacit√© est $9/16$.
    \item Nous avons obtenu $c = 1,2$ dans
      l'\autoref{exemple:simulation:beta:2}, soit une efficacit√© de
      $1/c = 5/6$. Cet algorithme est √©videmment plus efficace puisque
      la surface de l'enveloppe de la densit√© b√™ta est nettement plus
      petite.
    \item Utilisons l'algorithme d√©velopp√© √†
      l'\autoref{exemple:simulation:beta:2}. Une premi√®re mise en
      ≈ìuvre de l'algorithme en R est fournie dans le code
      informatique de la \autoref{sec:simulation:code}. La
      \autoref{fig:simulation:rbeta.ar2} en propose une autre.
      \begin{figure}[t]
        \centering
        \begin{framed}
\begin{lstlisting}
rbeta.ar2 <- function(n)
{
    g <- function(x)
        ifelse(x < 0.8, 2.5 * x, 10 - 10 * x)
    Ginv <- function(y)
        ifelse(y < 0.8, sqrt(0.8 * y),
               1 - sqrt(0.2 - 0.2 * y))
    x <- numeric(n)
    i <- 0
    while(i < n)
    {
        y <- Ginv(runif(1))
        if(1.2 * g(y) * runif(1) <=
           dbeta(y, shape1 = 3, shape2 = 2))
            x[i <- i + 1] <- y
    }
    x
}
\end{lstlisting}
        \end{framed}
        \caption{Code R de la fonction \code{rbeta.ar2}}
        \label{fig:simulation:rbeta.ar2}
      \end{figure}
      Vous pouvez v√©rifier l'exactitude la fonction \code{rbeta.ar2}
      avec les expressions suivantes.
<<echo=TRUE, eval=FALSE>>=
x <- rbeta.ar2(10000)
hist(x, prob = TRUE)
curve(dbeta(x, 3, 2), add = TRUE)
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:rejet}
  La fonction R de la \autoref{fig:simulation:fonction} permet de
  simuler des observations de la distribution B√™ta$(\alpha, \beta)$.
  \begin{figure}[t]
    \begin{framed}
\begin{lstlisting}
simul <- function(n, alpha, beta)
{
    xmax <- (alpha - 1)/(alpha + beta - 2)
    M <- dbeta(xmax, alpha, beta)
    x <- numeric(n)
    i <- 0
    repeat
    {
        u <- runif(1)
        if (M * runif(1) <= dbeta(u, alpha, beta))
            x[i <- i + 1] <- u
        if (i == n)
            break
    }
    x
}
\end{lstlisting}
    \end{framed}
    \caption{Fonction de simulation d'une loi B√™ta$(\alpha, \beta)$
      pour l'\autoref{ex:simulation:rejet}}
    \label{fig:simulation:fonction}
  \end{figure}
  \begin{enumerate}
  \item Identifier le type d'algorithme utilis√© dans cette fonction.
  \item Vous disposez des valeurs suivantes, obtenues dans R.
<<echo=FALSE>>=
op <- options(width = 50, digits = 2)
simul <- function(n, alpha, beta)
{
    ymax <- dbeta((alpha-1)/(alpha + beta - 2), alpha, beta)
    x <- numeric(n)
    i <- 1
    repeat
    {
        u <- runif(1)
        if (ymax * runif(1) <= dbeta(u, alpha, beta))
        {
            x[i] <- u
            i <- i + 1
        }
        if (i > n)
            break
    }
    x
}
@
<<echo=TRUE>>=
set.seed(12345)
runif(10)
@
    √âvaluer le r√©sultat des expressions ci-dessous.
<<echo=TRUE, eval=FALSE>>=
set.seed(12345)
simul(2, alpha = 2, beta = 3)
@
  \end{enumerate}
  \begin{rep}
<<echo=FALSE>>=
set.seed(12345)
x <- round(simul(2, alpha = 2, beta = 3), 2)
x <- format(x, dec = ",")
options(op)
@
    \begin{enumerate}[a)]
      \stepcounter{enumii}
    \item $(\Sexpr{x[1]}$, $\Sexpr{x[2]})$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Vous reconnaitrez l'algorithme d'acceptation-rejet de
      l'\autoref{ex:simulation:mode}.
    \item Vous devez simuler deux observations d'une loi B√™ta$(2, 3)$
      dont la fonction de densit√© de probabilit√© est
      \begin{displaymath}
        f(x) = 12 x (1 - x)^2, \quad 0 < x < 1.
      \end{displaymath}
      Le mode de cette densit√© se trouve en $x = 1/3$ (voir la
      solution de l'\autoref{ex:simulation:beta}) et la valeur de
      ce mode est $M = f(1/3) = 16/9$. Pour obtenir le r√©sultat de
      l'appel de la fonction \code{simul}, il faut s'assurer
      d'utiliser les nombres uniformes dans le bon ordre. Quatre
      it√©rations de la boucle \code{repeat} seront n√©cessaires.
      Voici leurs r√©sultas.
      \begin{enumerate}[1.]
      \item On a $u = 0,72$, puis $(16/9) (0,88) > f(0,72)$, donc $u$
        est rejet√©.
      \item On a $u = 0,76$, puis $(16/9) (0,89) > f(0,76)$, donc $u$
        est rejet√©.
      \item On a $u = 0,46$, puis $(16/9) (0,17) < f(0,46)$, donc $u$
        est accept√©: $x_1 = 0,46$.
      \item On a $u = 0,33$, puis $(16/9) (0,51) < f(0,33)$, donc $u$
        est accept√©: $x_2 = 0,33$.
      \end{enumerate}
      Le r√©sultat est donc le vecteur $\mat{x} = (0,46, 0,33)$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item D√©montrer que, si $0 < \alpha < 1$,
    \begin{displaymath}
      x^{\alpha - 1} e^{-x} \leq
      \begin{cases}
        x^{\alpha - 1}, & 0 \leq x \leq 1 \\
        e^{-x},        & x > 1.
      \end{cases}
    \end{displaymath}
  \item D√©velopper un algorithme d'acceptation-rejet pour simuler des
    observations d'une loi Gamma$(\alpha, 1)$, $0 < \alpha < 1$ √†
    partir du r√©sultat en a).
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Si $0 \leq x \leq 1$, $e^{-1} < e^{-x} < 1$, d'o√π $x^{\alpha
        - 1} e^{-x} \leq x^{\alpha - 1}$. De m√™me, puisque $0 < \alpha
      < 1$, $x^{\alpha - 1} < 1$ pour $x > 1$, d'o√π  $x^{\alpha - 1}
      e^{-x} \leq e^{-x}$ pour $x > 1$.
    \item Il faut borner la densit√© $f_X(x) = x^{\alpha - 1}
      e^{-x}/\Gamma(\alpha)$, $x > 0$ et $0 < \alpha < 1$. Du r√©sultat
      en a):
      \begin{displaymath}
        f_X(x) \leq
        \begin{cases}
          x^{\alpha - 1}/\Gamma(\alpha), & 0 \leq x \leq 1 \\
          e^{-x}/\Gamma(\alpha),        & x > 1.
        \end{cases}
      \end{displaymath}
      Posons
      \begin{displaymath}
        c g_Y(x) =
        \begin{cases}
          x^{\alpha - 1}/\Gamma(\alpha), & 0 \leq x \leq 1 \\
          e^{-x}/\Gamma(\alpha),        & x > 1.
        \end{cases}
      \end{displaymath}
      L'aire totale sous la fonction $c g_Y(x)$ est
      \begin{displaymath}
        \int_0^1 \frac{x^{\alpha - 1}}{\Gamma(\alpha)}\, dx +
        \int_1^\infty \frac{e^{-x}}{\Gamma(\alpha)}\, dx =
        \frac{1}{\Gamma(\alpha)}
        \left(
          \frac{1}{\alpha} + \frac{1}{e}
        \right),
      \end{displaymath}
      d'o√π
      \begin{align*}
        g_Y(x)
        &=
        \begin{cases}
          \dfrac{x^{\alpha - 1}}{(1/\alpha) + (1/e)}, & 0 \leq x \leq 1 \\
          \dfrac{e^{-x}}{(1/\alpha) + (1/e)}, & x > 1,
        \end{cases} \\
        G_Y(x)
        &=
        \begin{cases}
          \dfrac{e}{\alpha + e}\, x^\alpha, & 0 \leq x \leq 1 \\
          1 - \dfrac{e^{-x}}{(1/\alpha) + (1/e)}, & x > 1,
        \end{cases} \\
        \intertext{et}
        G_Y^{-1}(x)
        &=
        \begin{cases}
          \left( \dfrac{\alpha + e}{e}\, x \right)^{1/\alpha},
          & 0 \leq x \leq e/(\alpha + e) \\
          - \ln [((1/\alpha) + (1/e))(1 - x)],
          & e/(\alpha + e) < x \leq 1.
        \end{cases}
      \end{align*}
      Or,
      \begin{displaymath}
        \frac{f_X(x)}{c g_Y(x)} =
        \begin{cases}
          e^{-x},        & 0 \leq x \leq 1 \\
          x^{\alpha - 1}, & x > 1.
        \end{cases}
      \end{displaymath}
      Il d√©coule de ce pr√©c√®de l'algorithme de simulation suivant.
      \begin{enumerate}[1.]
      \item Simuler deux nombres $u_1$ et $u_2$ d'une $U(0, 1)$.
      \item Poser $y = G_Y^{-1}(u_1)$.
      \item Si
        \begin{displaymath}
          u_2 \leq
          \begin{cases}
            e^{-y},        & 0 \leq y \leq 1 \\
            y^{\alpha - 1}, & y > 1,
          \end{cases}
        \end{displaymath}
        alors poser $x = y$. Sinon, retourner √† l'√©tape 1.
      \end{enumerate}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit l'in√©galit√© suivante, valide pour $\alpha \geq 1$:
  \begin{displaymath}
    x^{\alpha - 1} e^{-x}
    \leq \alpha^{\alpha - 1} e^{-x/\alpha + 1 -  \alpha}, \quad x > 0.
  \end{displaymath}
  Utiliser cette in√©galit√© pour justifier l'algorithme
  d'acceptation-rejet ci-dessous servant √† simuler des observations d'une loi
  Gamma$(\alpha, 1)$ avec $\alpha \geq 1$.
  \begin{enumerate}[1.]
  \item Simuler deux observations ind√©pendantes $v_1$ et $v_2$ d'une
    loi exponentielle de moyenne $1$.
  \item Si $v_2 < (\alpha - 1)(v_1 - \ln v_1 - 1)$, poser $x =
    \alpha v_1$. Sinon, retourner √† l'√©tape 1.
  \end{enumerate}
  \begin{sol}
    Vous devez simuler des observations de la fonction de densit√© de
    probabilit√© $f_X(x) = x^{\alpha - 1} e^{-x}/\Gamma(\alpha)$ avec
    $\alpha \geq 1$. Or, vous savez que
    \begin{displaymath}
      f_X(x) \leq \frac{\alpha^\alpha}{\Gamma(\alpha)}\,
      e^{1 - \alpha}\,
      \frac{1}{\alpha}\, e^{-x/\alpha}, \quad x > 0,
    \end{displaymath}
    d'o√π $f_X(x) \leq c g_Y(x)$ avec
    \begin{align*}
      c
      &= \frac{\alpha^\alpha}{\Gamma(\alpha)}\, e^{1 - \alpha} \\
      \intertext{et}
      g_Y(x)
      &= \frac{1}{\alpha}\, e^{-x/\alpha}.
    \end{align*}
    Ainsi, $Y \sim \text{Exponentielle}(1/\alpha)$. Soit $y$ une
    observation de la variable al√©atoire $Y$ et $u$ une observation
    d'une loi $U(0, 1)$. Selon l'algorithme d'acceptation-rejet, on
    accepte la valeur $y$ comme observation d'une loi Gamma$(\alpha,
    1)$ avec $\alpha \geq 1$ si
    \begin{gather*}
      u \leq \frac{f_X(y)}{c g_Y(y)} =
      y^{\alpha - 1}\, \frac{e^{-y (1 - 1/\alpha)}}{\alpha^{\alpha -
          1} e^{-(\alpha - 1)}} \\
      \Updownarrow \\
      u^{1/(\alpha - 1)} \leq \left( \frac{y}{\alpha} \right)
      \frac{e^{-y/\alpha}}{e^{-1}} \\
      \Updownarrow \\
      \ln u \leq (\alpha - 1)
      \left[
        \ln \left( \frac{y}{\alpha} \right) - \frac{y}{\alpha} + 1
      \right] \\
      \Updownarrow \\
      - \ln u > (\alpha - 1)
      \left[
        \frac{y}{\alpha} - \ln \left( \frac{y}{\alpha} \right) - 1
      \right].
    \end{gather*}
    Or, tant la distribution de $-\ln U$ que celle de $Y/\alpha$ est
    une exponentielle de moyenne $1$, d'o√π l'algorithme donn√© dans
    l'√©nonc√©.
  \end{sol}
\end{exercice}

\begin{exercice}
  Dans la mise en ≈ìuvre de
  l'\autoref{algo:simulation:melange-discret} de simulation d'un
  m√©lange discret pr√©sent√©e √† la \autoref{sec:simulation:code}, nous
  g√©n√©rons un nombre d'une distribution binomiale pour d√©terminer la
  quantit√© de nombres √† simuler de la premi√®re distribution du m√©lange
  (et, cons√©quemment, de celle de la seconde distribution).
  G√©n√©raliser le concept pour simuler des observations d'un m√©lange
  discret de $k > 2$ distributions. \emph{Astuce}: consulter le
  code source de la fonction \code{rmixture}.
  \begin{sol}
    Il faut d√©terminer le nombre d'observations provenant de chacune
    des $k$ distributions $F_1, \dots, F_k$ sachant que la probabilit√©
    qu'une observation provienne de la distribution $j$ est $p_j$. La
    distribution conjointe du nombre d'observations de chaque
    distribution est une multinomiale de param√®tres $p_1, \dots, p_k$
    dont la fonction de masse de probabilit√© est:
    \begin{equation*}
      \Pr[X_1 = x_1, ... , X_k = x_k] =
      \frac{n!}{x_1! \cdots x_k!}
      \prod_{j = 1 , ..., k} p_j^{x_j},
    \end{equation*}
    o√π $n = x_1 + \dots + x_k$. La fonction \code{rmultinom} de R
    permet de simuler des valeurs $x_1, \dots, x_k$ pour $n$ et $p_1,
    \dots, p_k$ donn√©es. C'est la strat√©gie employ√©e par la fonction
    \code{rmixture} de \pkg{actuar}.
  \end{sol}
\end{exercice}

\begin{exercice}
  √âcrire des expressions R pour simuler des observations des mod√®les
  ci-dessous d'abord sans, puis avec, les fonctions d√©di√©es du
  paquetage \pkg{actuar}, le cas √©ch√©ant.
  \begin{enumerate}
  \item Variable al√©atoire dont la fonction de densit√© est
    $f(x) = 0,3 f_1(x) + 0,7 f_2(x)$, o√π $f_1$ est la densit√© d'une
    distribution exponentielle de moyenne $1/2$ et $f_2$ est la
    densit√© d'une distribution gamma de param√®tre $\alpha = 3$ et de
    moyenne $6$.
  \item M√©lange $f(x) = 0,5647 I\{x = 0\} + 0,4353 (2^x e^{-2}/x!)$,
    o√π $I\{\mathcal{A}\}$ est une fonction indicatrice qui vaut $1$
    lorsque la condition $\mathcal{A}$ est vraie et $0$ sinon.
  \item Variable al√©atoire $X$ d√©finie ainsi:
    $X|\Theta = \theta \sim \text{Poisson}(\theta)$,
    $\Theta \sim \text{Inverse gaussienne}(2, 1)$. La fonction de
    densit√© de probabilit√© de la distribution inverse gaussienne de
    moyenne $\mu$ et de dispersion $\phi$ est
    \begin{equation*}
      f(x) = \left( \frac{1}{2 \pi \phi x^3} \right)^{1/2}
      \exp\left( -\frac{(x - \mu)^2}{2 \mu^2 \phi x} \right), \quad
      x \ge 0, \mu > 0, \phi > 0.
    \end{equation*}
    \emph{Astuce}: la fonction \code{rinvgauss} du paquetage
    \pkg{actuar} permet de simuler des observations de la distribution
    inverse gaussienne.
  \item Dsitribution compos√©e $S = X_1 + \dots + X_N$ avec
    $N \sim \text{Poisson}(2)$ et $X \sim \text{Log-normale}(7,1, 1)$.
  \item Binomiale n√©gative compos√©e o√π les param√®tres de la binomiale
    n√©gative sont $r = 2$ et $p = 0,3$, et la distribution de la
    variable al√©atoire $X$ est une log-normale de param√®tres
    $(7,1, 1)$.
  \end{enumerate}
  \begin{sol}
    Dans chacun des cas ci-dessous, nous consid√©rons que l'objet
    \code{n} contient le nombre de valeurs √† simuler.
    \begin{enumerate}
    \item Il s'agit de simuler des observations d'un m√©lange discret
      d'une Exponentielle$(2)$ et d'une Gamma$(2, 1/3)$. Nous pouvons
      proc√©der en suivant la proc√©dure explicit√©e √† la
      \autoref{sec:simulation:exercices} ou avec la fonction
      \code{rmixture} du paquetage \pkg{actuar}.
<<echo=TRUE, eval=FALSE>>=
n1 <- rbinom(1, n, 0.3)
c(rexp(n1, 2), rgamma(n - n1, 2, scale = 3))
@
<<echo=TRUE, eval=FALSE>>=
rmixture(n, c(0.3, 0.7),
         expression(rexp(2), rgamma(2, scale = 3)))
@
    \item Le m√©lange discret est, ici, celui d'une distribution d√©g√©n√©r√©e
      en $x = 0$ et d'une Poisson de param√®tre $\lambda = 2$. La
      simulation ¬´manuelle¬ª est en tous points similaire √† ce que nous
      avons fait en b). Cependant, √† moins de cr√©er une fonction de
      ¬´simulation¬ª d'une distribution d√©g√©n√©r√©e, nous ne pouvons
      utiliser directement la fonction \code{rmixture}.
<<echo=TRUE, eval=FALSE>>=
n1 <- rbinom(1, n, 0.3)
c(rep(n1, 0), rpois(n - n1, 2))
@
      Le paquetage \pkg{actuar} fournit n√©anmoins une fonction pour
      simuler des observations d'un tel m√©lange. En effet, celui-ci
      est simplement une distribution de Poisson avec une probabilit√©
      en $x = 0$ de $p_0 = 0,5647 + 0,4353 e^{-2} = 0,6236$, une
      distribution autrement connue sous le nom de Poisson
      z√©ro-modifi√©e. Les fonctions avec racine \meta{zmpois} de
      \pkg{actuar} permettent de travailler avec cette distribution.
<<echo=TRUE, eval=FALSE>>=
rzmpois(n, 2, p0 = 0.6236)
@
    \item Nous devons simuler des observations de la distribution
      Poisson-inverse gaussienne qui, par ailleurs, est d√©finie par le
      m√©lange continu pr√©sent√© dans la question. Nous pouvons proc√©der
      ¬´manuellement¬ª √† l'aide de la fonction \code{rinvgauss} du
      paquetage \pkg{actuar}, ou encore plus directement avec la
      fonction \code{rpoisinvgauss} du m√™me paquetage.
<<echo=TRUE, eval=FALSE>>=
rpois(n, rinvgauss(2, 1))
@
<<echo=TRUE, eval=FALSE>>=
rpoisinvgauss(n, 2, 1)
@
    \item Nous devons simuler des observations d'une distribution
      Poisson compos√©e. Le paquetage \pkg{actuar} fournit la fonction
      \code{rcomppois} pour ce faire.
<<echo=TRUE, eval=FALSE>>=
sapply(rpois(n, 2),
       function(n) sum(rlnorm(n, log(2000), 1)))
@
<<echo=TRUE, eval=FALSE>>=
rcomppois(n, 2, rlnorm(log(2000), 1))
@
    \item Il faut cette fois simuler des observations d'une distribution
      binomiale n√©gative compos√©e. Le paquetage \pkg{actuar} fournit
      la fonction \code{rcompound} pour ce cas g√©n√©ral.
<<echo=TRUE, eval=FALSE>>=
sapply(rnbinom(n, 2, 0.3),
       function(n) sum(rlnorm(n, log(2000), 1)))
@
<<echo=TRUE, eval=FALSE>>=
rcompound(n, rnbinom(2, 0.3), rlnorm(log(2000), 1))
@
    \end{enumerate}
  \end{sol}


\end{exercice}

\Closesolutionfile{reponses}
\Closesolutionfile{solutions}

\input{reponses-simulation}

%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "methodes-numeriques-en-actuariat_simulation"
%%% coding: utf-8
%%% End:
