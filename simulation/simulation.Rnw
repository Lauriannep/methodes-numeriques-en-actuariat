%%% Copyright (C) 2018 Vincent Goulet
%%%
%%% Ce fichier fait partie du projet
%%% «Méthodes numériques en actuariat avec R»
%%% http://github.com/vigou3/methodes-numeriques-en-actuariat
%%%
%%% Cette création est mise à disposition selon le contrat
%%% Attribution-Partage dans les mêmes conditions 4.0
%%% International de Creative Commons.
%%% http://creativecommons.org/licenses/by-sa/4.0/

\chapter{Simulation de nombres aléatoires non uniformes}
\label{chap:simulation}

<<echo=FALSE>>=
options(width = 52)
@

\begin{objectifs}
\item Développer un algorithme de simulation de
  nombres non uniformes à partir de la méthode de l'inverse.
\item Développer un algorithme de simulation de nombres non uniformes
  à partir de la méthode d'acceptation-rejet.
\item Calculer des nombres pseudo-aléatoires non uniformes en
  suivant un algorithme donné.
\item Utiliser les outils de Excel et R pour la simulation de
  nombres non uniformes.
\end{objectifs}

Habituellement, les applications de la simulation requièrent des
nombres aléatoires provenant non pas d'une distribution $U(0, 1)$,
mais plutôt d'une distribution avec fonction de répartition $F_X(x)$.
Ceci nécessite donc de transformer les nombres uniformes en des nombres
provenant de distributions non-uniformes.

Il existe de très nombreux algorithmes pour générer des nombres
aléatoires de différentes distributions; voir, par exemple,
\cite{Devroye:random:1986}. Nous n'en étudierons que deux en détail,
soit la méthode de l'inverse et la méthode d'acceptation-rejet. Par
ailleurs, la \autoref{sec:simulation:modeles-actuariels} présente des
procédures de simulation pour les modèles actuariels les plus
courants.

Plusieurs algorithmes de simulation de nombres non uniformes reposent
sur des résultats de transformations de variables aléatoires. Par
exemple:
\begin{itemize}
\item la somme de $\alpha$ exponentielles est une loi gamma avec
  paramètre de forme $\alpha$ entier;
\item la loi géométrique est la partie entière de l'exponentielle;
\item la loi $F$ est un ratio de deux khi-carré; etc.
\end{itemize}
Le lecteur qui ne serait pas à l'aise avec les notions de
transformation de variables aléatoires trouvera à
l'\autoref{chap:rappels_transformations} des rappels sur les
principales techniques étudiées dans les cours d'analyse probabiliste
et d'analyse statistique.

\begin{prob-enonce}
  Au chapitre précédent, nous nous sommes penchés sur le problème des
  quatre points de Sylvester lorsque la région $R$ est un carré.

  Cette fois, nous devons vérifier par simulation que lorsque la
  région $R$ est un disque, la probabilité que l'enveloppe convexe
  forme un triangle est $\frac{35}{12 \pi^2} \approx 0,29552$.
\end{prob-enonce}


\section{Méthode de l'inverse}
\label{sec:simulation:inverse}

La méthode de l'inverse repose sur une idée toute simple, soit que
nous pouvons transformer des nombres uniformes sur $(0, 1)$ en des
nombres provenant de la distribution avec fonction de répartition
$F_X(x)$ en utilisant le théorème suivant.

\begin{thm}
  \label{thm:simulation:inverse}
  Soit $X$ une variable aléatoire avec fonction de répartition
  $F_X(x)$. Alors
  \begin{displaymath}
    F_X(X) \sim U(0, 1).
  \end{displaymath}
\end{thm}
\begin{proof}
  Soit la transformation $U = F_X(X)$. Alors,
  \begin{align*}
    F_U(u)
    &= \Pr[U \leq u] \\
    &= \Pr[F_X(X) \leq u] \\
    &= \Pr[X \leq F_X^{-1}(u)] \\
    &= F_X(F_X^{-1}(u)) \\
    &= u,
  \end{align*}
  d'où $U \sim U(0, 1)$.
\end{proof}

Par conséquent, si $U \sim U(0, 1)$, alors
\begin{displaymath}
  F_X^{-1}(U) \sim X.
\end{displaymath}
La fonction de répartition inverse, $F_X^{-1}$, est aussi appelée
\emph{fonction de quantile}.

\begin{algorithme}[Méthode de l'inverse]
  Soit $X$ une variable aléatoire avec fonction de répartition
  $F_X(x)$. Les étapes ci-dessous permettent de générer une
  observation $x$ de cette variable aléatoire.
  \begin{enumerate}
  \item Obtenir un nombre $u$ d'une loi $U(0, 1)$.
  \item Poser $x = F_X^{-1}(u)$.
  \end{enumerate}
\end{algorithme}

La méthode de l'inverse consiste à choisir un nombre uniformément sur
l'axe des ordonnées d'une fonction de répartition (donc un nombre
entre $0$ et $1$) et à trouver la valeur correspondante sur l'axe des
abscisses telle que donnée par la fonction de quantile. Comme les
valeurs en $x$ seront plus concentrées là où la pente de la fonction
de répartition est la plus grande, et vice versa, la distribution en
abscisse ne sera pas uniforme. Voir la
\autoref{fig:simulation:inverse} pour une représentation
graphique de ce phénomène.

\begin{figure}
  \centering
<<echo=FALSE,fig=TRUE, width=8, height=4>>=
par(mfrow = c(1, 2), mar = c(5, 4, 2, 2))

## Graphique de fonction de répartition
plot(NA, xlim = c(0, 14), ylim = c(0, 1),
     xlab = expression(x), ylab = expression(F[X](x)),
     xaxs="i", yaxs="i")
u <- c(0.3, 0.4, 0.8, 0.9)
x <- qgamma(u, 5, 1)
polygon(c(0, x[1], x[1], x[2], x[2], 0),
        c(u[1], u[1], 0, 0, u[2], u[2]), col="lightblue")
polygon(c(0, x[3], x[3], x[4], x[4], 0),
        c(u[3], u[3], 0, 0, u[4], u[4]), col="lightblue")
curve(pgamma(x, 5, 1), add=TRUE)

## Graphique de la densité
plot(NA, xlim = c(0, 14), ylim = c(0, 0.2),
     xlab = expression(x), ylab = expression(f[X](x)),
     xaxs = "i", yaxs = "i")
xx <- seq(from = x[1], to = x[2], length = 100)
polygon(c(xx[1], xx, xx[100]), c(0, dgamma(xx, 5, 1), 0), col = "lightblue")
xx <- seq(from = x[3], to = x[4], length = 100)
polygon(c(xx[1], xx, xx[100]), c(0, dgamma(xx, 5, 1), 0), col = "lightblue")
curve(dgamma(x, 5, 1), xlim = c(0, 14), add=TRUE)
@
  \caption{Représentation graphique de la méthode de l'inverse. À
    des intervalles égaux en ordonnée correspondent des intervalles
    différents en abscisse selon la forme de la distribution. La
    fonction de répartition à gauche correspond à la densité de droite.}
  \label{fig:simulation:inverse}
\end{figure}

La méthode de l'inverse en est une bonne si la fonction de quantile
est facile à calculer. S'il n'existe pas de forme explicite pour
$F_X^{-1}(\cdot)$, résoudre numériquement
\begin{displaymath}
  F_X(x) - u = 0
\end{displaymath}
peut s'avérer aussi efficace que bien d'autres méthodes.

\tipbox{Dans Excel, il faut nécessairement utiliser la méthode de
  l'inverse. Plusieurs fonctions de quantiles sont disponibles; voir
  la \autoref{sec:simulation:excel_et_al}.}


\subsection{Distributions continues}
\label{sec:simulation:inverse:continues}

La méthode de l'inverse est, en principe du moins, simple à utiliser
avec les distributions continues: il suffit de connaître la fonction
de quantile et de l'appliquer à des nombres uniformes pour obtenir des
nombres de la distribution souhaitée.

Dans les faits, il y a peu de lois de probabilité continues dont la
fonction de répartition est simple à inverser (exponentielle, Pareto,
Weibull). Il faut parfois utiliser d'autres méthodes.

\begin{exemple}
  Nous voulons obtenir un échantillon aléatoire d'une distribution
  exponentielle de paramètre $\lambda$ avec fonction de densité de
  probabilité
  \begin{align*}
    f(x)
    &= \lambda e^{-\lambda x}, \quad x > 0 \\
    \intertext{et fonction de répartition}
    F(x)
    &= 1 - e^{-\lambda x}, \quad x > 0.
  \end{align*}
  Or,
  \begin{displaymath}
    F^{-1}(u) = - \frac{1}{\lambda} \ln (1 - u),
  \end{displaymath}
  donc
  \begin{displaymath}
    X = - \frac{1}{\lambda} \ln (1 - U) \sim \text{Exponentielle}(\lambda),
  \end{displaymath}
  où $U \sim U(0, 1)$. En fait, puisque $U \sim U(0, 1)
  \Leftrightarrow 1 - U \sim U(0, 1)$, nous pouvons nous contenter de la
  relation
  \begin{displaymath}
    X = - \frac{1}{\lambda} \ln U \sim \text{Exponentielle}(\lambda).
  \end{displaymath}
  Par conséquent, l'algorithme pour simuler des nombres provenant
  d'une exponentielle de paramètre $\lambda$ est:
  \begin{enumerate}
  \item Obtenir un nombre $u$ d'une $U(0, 1)$;
  \item Poser $x = - \lambda^{-1} \ln u$.
  \end{enumerate}

  \gotorbox{Exécuter le code informatique R de la
    \autoref{sec:simulation:code} correspondant à cet exemple pour une
    illustration de l'algorithme ci-dessus.}%
  \qed
\end{exemple}


\subsection{Distributions discrètes}
\label{sec:simulation:inverse:discretes}

Nous pouvons aussi utiliser la méthode de l'inverse avec les distributions
discrètes. Cependant, puisque la fonction de répartition comporte des
sauts, son inverse n'existe pas formellement. Par conséquent, il
n'existe pas de solution de $u = F_X(x)$ pour certaines valeurs de
$u$, ou alors une infinité de solutions.

Supposons une distribution avec un saut en $x_0$ et
\begin{align*}
  F_X(x_0^-) &= a \\
  F_X(x_0) &= b > a.
\end{align*}
Si $a < u < b$, nous posons $x = x_0$. Ainsi, nous simulerons $x_0$ dans une
proportion $b - a$ du temps, ce qui correspond à $\Pr[X = x_0]$. Voir
la \autoref{fig:simulation:discrete} pour une illustration.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
set.seed(1)
x <- rbinom(1000, 4, 0.6)
Fn <- ecdf(x)
plot(Fn, xlim = c(0, 5), ylab="F(x)", main = "")
polygon(c(par("usr")[1], 2, 2, par("usr")[1]),
        Fn(c(1, 1, 2, 2)), col = "lightblue", border = NA)
plot(Fn, add = TRUE)
u <- runif(1, Fn(1), Fn(2))
arrows(par("usr")[1], u, 2, u, length = 0.125)
segments(2, par("usr")[3], 2, Fn(2), lty = 2)
axis(2)
@
  \caption{Illustration de la méthode de l'inverse pour une distribution
   discrète. Tout nombre uniforme tiré dans la zone colorée est
   converti en la même valeur, ce qui correspond à la probabilité
   d'obtenir cette valeur (la hauteur du saut).}
  \label{fig:simulation:discrete}
\end{figure}

Que faire si $u = a$ ou $u = b$? Nous prenons la plus grande valeur de
l'intervalle où $F_X(x)$ est constante:
\begin{align*}
  u = a  &\Rightarrow x = x_0 \\
  u = b  &\Rightarrow x = x_1.
\end{align*}
Il faut procéder ainsi parce que plusieurs générateurs produisent des
nombres uniformes sur $[0, 1)$.

\begin{exemple}
  \label{exemple:simulation:mixte}
  Soit $X$ une variable aléatoire avec fonction de densité de probabilité
  \begin{displaymath}
    f(x) =
    \begin{cases}
      0,5, & 0 \leq x < 1 \\
      0,   & 1 \leq x < 2 \\
      0,5, & 2 \leq x < 3.
    \end{cases}
  \end{displaymath}
  Il s'agit d'une variable aléatoire \emph{mixte} (en partie continue
  et en partie discrète) dont la fonction de répartition est
  \begin{displaymath}
    F(x) =
    \begin{cases}
      0,5x, & 0 \leq x < 1 \\
      0,5,   & 1 \leq x < 2 \\
      0,5x - 0,5, & 2 \leq x < 3.
    \end{cases}
  \end{displaymath}
  Cette fonction est représentée à la \autoref{fig:simulation:mixte}.

  \begin{figure}
    \centering
<<echo=FALSE,fig=TRUE, width=8, height=4>>=
par(mfrow = c(1, 2), mar = c(5, 4, 2, 2))
plot(NA, xlim = c(0, 3), ylim = c(0, 1),
     xlab = expression(x), ylab = expression(f[X](x)),
     xaxt = "n", yaxt = "n")
points(c(0, 1), rep(0.5, 2), type = "o", pch = c(19, NA))
points(c(1, 2), rep(0, 2), type = "o", pch = c(19, NA))
points(c(2, 3), rep(0.5, 2), type = "o", pch = c(19, NA))
axis(1, at = 0:3)
axis(2, at = 0:2/2)
plot(0:3, c(0, 0.5, 0.5, 1), type = "o", pch = 19,
     xlab = expression(x), ylab = expression(F[X](x)),
     xaxt = "n", yaxt = "n")
axis(1, at = 0:3)
axis(2, at = 0:2/2)
@
    \caption{Fonction de densité de probabilité (gauche) et fonction de
      répartition (droite) de l'\autoref{exemple:simulation:mixte}}
    \label{fig:simulation:mixte}
  \end{figure}

  Un algorithme pour simuler des nombres aléatoires de cette
  distribution est donc:
  \begin{enumerate}
  \item Obtenir un nombre $u$ d'une loi $U(0, 1)$.
  \item Poser
    \begin{displaymath}
      x =
      \begin{cases}
        2u,    & \text{si } 0 \leq u < 0,5 \\
        2,     & \text{si } u = 0,5 \\
        2u + 1 & \text{si } 0,5 < u < 1.
      \end{cases}
    \end{displaymath}
  \end{enumerate}
  \qed
\end{exemple}

\begin{exemple}
  \label{ex:simulation:binomiale}
  Nous voulons simuler des observations d'une distribution binomiale de
  paramètres $n$ et $\theta$. Nous pourrions, pour chaque $x$ simulé,
  faire $n$ expériences de Bernoulli et compter le nombre de succès.
  Par la méthode de l'inverse pour la loi de Bernoulli, on a un succès
  si $u \leq \theta$. Par conséquent, un algorithme serait:
  \begin{enumerate}
  \item Simuler $n$ nombres uniformes indépendants $u_1, \dots, u_n$
    d'une loi $U(0, 1)$.
  \item Poser
    \begin{displaymath}
      x = \sum_{i = 1}^n I\{u_i \leq \theta\}.
    \end{displaymath}
  \end{enumerate}
  Cette technique requiert toutefois de simuler $n$ nombres uniformes
  pour chaque valeur de $x$.

  Il est plus efficace d'utiliser la méthode de l'inverse directement
  avec la distribution binomiale. Par exemple, si $n = 4$ et $\theta =
  0,5$, on a que
  \begin{align*}
    \Pr[X = x]
    &=
    \begin{cases}
      0,0625, & x = 0 \\
      0,25,   & x = 1 \\
      0,375,  & x = 2 \\
      0,25,   & x = 3 \\
      0,0625, & x = 4
    \end{cases} \\
    \intertext{et}
    \Pr[X \leq x]
    &=
    \begin{cases}
      0,      & x < 0 \\
      0,0625, & 0 \leq x < 1 \\
      0,3125, & 1 \leq x < 2 \\
      0,6875, & 2 \leq x < 3 \\
      0,9375, & 3 \leq x < 4 \\
      1,      & x \geq 4,
    \end{cases}
  \end{align*}
  d'où l'algorithme suivant:
  \begin{enumerate}
  \item Simuler un nombre $u$ d'une distribution $U(0, 1)$.
  \item Déterminer $x$ selon le tableau suivant:
    \begin{center}
      \begin{tabular}{lc}
        \toprule
        $u$ dans l'intervalle & Valeur de $x$ \\
        \midrule
        $[0, 0,0625)$         & $0$ \\
        $[0,0625, 0,3125)$    & $1$ \\
        $[0,3125, 0,6875)$    & $2$ \\
        $[0,6875, 0,9375)$    & $3$ \\
        $[0,9375, 1)$         & $4$ \\
        \bottomrule
      \end{tabular}
    \end{center}
  \end{enumerate}
  \qed
\end{exemple}

\tipbox{La méthode de l'inverse pour les distributions discrètes à support
  fini est facile à mettre en œuvre dans Excel à l'aide de la
  fonction \code{RECHERCHEV()}.}

\begin{prob-astuce}
  Au cœur du problème que nous tentons de résoudre, il y a la
  simulation de points dans un cercle. L'interprétation géométrique
  est claire.

  Il serait également possible d'en faire une interprétation
  probabiliste: nous voulons simuler des valeurs d'une distribution
  bidimensionnelle uniforme sur un disque --- sans perte de
  généralité, de rayon $1$ et centré à l'origine --- dont la fonction
  de densité conjointe est
  \begin{equation*}
    f_{XY}(x, y) = \frac{1}{\pi}, \quad -1 < x < 1,\, x^2 + y^2 < 1.
  \end{equation*}
  Cependant, les méthodes de simulation pour les distributions
  multidimensionnelles deviennent rapidement complexes, surtout
  lorsque les variables ne sont pas indépendantes, comme c'est le cas
  ici. Nous tâcherons donc de trouver une manière plus intuitive d'un
  point de vue géométrique pour simuler des points sur un disque.
\end{prob-astuce}


\section{Méthode acceptation-rejet}

Supposons qu'il est compliqué de simuler des réalisations de la
variable aléatoire $X$ avec fonction de densité de probabilité
$f_X(x)$. Si nous pouvons trouver une variable aléatoire $Y$ avec fonction
de densité de probabilité $g_Y(x)$ pour laquelle la simulation est
simple (uniforme, triangle, exponentielle, etc.) et qu'il est possible
d'«envelopper» la densité $f$ par un multiple de $g$, c'est-à-dire que
\begin{displaymath}
  c g_Y(x) \geq f_X(x)
\end{displaymath}
pour tout $x$, alors nous pouvons utiliser l'algorithme
d'acceptation-rejet ci-dessous pour générer des observations de la
variable aléatoire $X$.

\begin{algorithme}[Méthode d'acceptation-rejet]
  Soit $X$ une variable aléatoire avec fonction de densité de
  probabilité $f_X(x)$ et $Y$ une variable aléatoire avec fonction de
  densité de probabilité $g_Y(x)$. Les étapes ci-dessous permettent de
  générer un nombre $x$ issu de la distribution $f$.
  \begin{enumerate}
  \item Obtenir une réalisation $y$ de distribution avec densité
    $g_Y(\cdot)$.
  \item Obtenir un nombre $u$ d'une $U(0, 1)$.
  \item Si
    \begin{equation*}
      u \leq \frac{f_X(y)}{c g_Y(y)},
    \end{equation*}
    poser $x = y$. Sinon, retourner à l'étape 1.
  \end{enumerate}
\end{algorithme}

\tipbox{Puisque, par définition, l'aire sous les densités $f$ et $g$
  vaut $1$ dans les deux cas, la relation $c g_Y(x) \geq f_X(x)$ ne
  peut être vraie pour tout $x$ que si $c > 1$.}

L'idée de la %
\capsule{https://youtu.be/1ptmo6CPKuQ}{méthode d'acceptation-rejet} %
consiste à accepter la «bonne proportion» des réalisations de $Y$
comme provenant de $X$. La principale difficulté avec la méthode
d'acceptation-rejet consiste à trouver la fonction enveloppante
$c g_Y(x)$.

Dans l'illustration de la \autoref{fig:simulation:acceptation-rejet},
la densité $f$ à support fini est facilement enveloppée par un
rectangle. Un nombre $y$ simulé de cette distribution (à toutes fins
pratiques une uniforme, ici) est accepté comme provenant de $f$ dans
une proportion correspondant au ratio entre la valeur de $f_X(y)$ (les
segments pointillés dans la figure) et la valeur de $c g_Y(y)$ (les
segments pleins). Vous constaterez aisément que certaines valeurs $y$
seront acceptées plus souvent que d'autres en conformité avec la forme
de la densité.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
plot(NA, xlim = c(0, 1), ylim = c(0, 2), xlab = "y", ylab = "f(y)")
m <- dbeta(2/3, 3, 2)
x <- c(0.22, 0.58, 0.83)
eps <- eps <- 30E-4
segments(x - eps, 0, x - eps, dbeta(x, 3, 2), col = "lightblue", lwd = 3, lty = 2)
segments(x + eps, 0, x + eps, m, col = "lightblue", lwd = 3)
curve(dbeta(x, 3, 2), xlim = c(0, 1), add = TRUE, lwd = 2, col = "black")
segments(0, m, 1, m, col = "orange", lwd = 2)
text(0.1, m + 0.08, "c g(y)")
@
  \caption{Illustration de la méthode acceptation-rejet}
  \label{fig:simulation:acceptation-rejet}
\end{figure}

Une autre interprétation est possible. La méthode dit d'accepter la
valeur $y$ simulée de la densité $g$ comme provenant de la densité $f$
si
\begin{equation*}
  u c g_Y(y) \leq f_X(y),
\end{equation*}
où $u$ est un nombre issu d'une loi $U(0, 1)$. Graphiquement, cela
signifie que nous acceptons la valeur $y$ si le point $(y, u c g_Y(y))$
se trouve sous la courbe $f$ en $y$ et que nous le rejetons s'il se trouve
entre $f$ et l'enveloppe. La
\autoref{fig:simulation:acceptation-rejet2} illustre cette
interprétation.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
## plot(NA, xlim = c(0, 1), ylim = c(0, 2), xlab = "y", ylab = "f(y)")
## m <- dbeta(2/3, 3, 2)
## x <- rep(c(0.22, 0.58, 0.83), each = 8)
## y <- runif(24, 0, m)
## points(x, y, pch = ifelse(y <= dbeta(x, 3, 2), 21, 1), bg = "darkgray")
## curve(dbeta(x, 3, 2), xlim = c(0, 1), add = TRUE, lwd = 2, col = "blue3")
## segments(0, m, 1, m, col = "red3", lwd = 2)
m <- dbeta(2/3, 3, 2)
x <- runif(100)
mu <- m * runif(100)
plot(x, mu, xlim = c(0, 1), ylim = c(0, 2), xlab = "y", ylab = "f(y)",
     pch = ifelse(mu <= dbeta(x, 3, 2), 21, 1), bg = "lightblue")
curve(dbeta(x, 3, 2), xlim = c(0, 1), add = TRUE, lwd = 2, col = "black")
segments(0, m, 1, m, col = "orange", lwd = 2)
text(0.1, m + 0.08, "c g(y)")
@
  \caption{Interprétation alternative de la méthode acceptation-rejet.
    Chaque point représente un couple $(y, u c g_Y(y))$. On accepte les
    points pleins et on rejette les points vides.}
  \label{fig:simulation:acceptation-rejet2}
\end{figure}

\tipbox{Évidemment, plus l'enveloppe est près de $f_X(x)$, plus
  l'algorithme est performant puisque nous rejetons alors moins de
  valeurs.}

\begin{exemple}
  \label{exemple:simulation:beta:1}
  Soit $X \sim \text{Bêta}(3, 2)$. La densité de cette distribution est:
  \begin{align*}
    f_X(x)
    &= \frac{\Gamma(5)}{\Gamma(3) \Gamma(2)}\,
    x^{3 - 1} (1 - x)^{2 - 1} \\
    &= 12 x^2 (1 - x), \quad 0 < x < 1.
  \end{align*}
  Elle est représentée aux figures
  \ref{fig:simulation:acceptation-rejet} et
  \ref{fig:simulation:acceptation-rejet2}. Nous pouvons facilement
  inscrire celle-ci dans un rectangle. Le mode de la densité se trouvant en
  $x = 2/3$, la hauteur du rectangle est $f(2/3) = 48/27 = 16/9$. Par
  conséquent,
  \begin{displaymath}
    c g_Y(x) = \frac{16}{9}, \quad 0 < x < 1.
  \end{displaymath}
  Nous déduisons que $c = 16/9$ et que la densité $g$ est une uniforme sur
  $(0 , 1)$. Nous avons donc l'algorithme suivant:
  \begin{enumerate}
  \item Simuler deux nombres $u_1$ et $u_2$ d'une $U(0, 1)$.
  \item Poser $y = u_1$.
  \item Si
    \begin{displaymath}
      u_2 \leq \frac{f_X(y)}{16/9},
    \end{displaymath}
    alors poser $x = y$. Sinon, retourner à l'étape 1.
  \end{enumerate}

  L'aire du rectangle étant de $16/9$, nous pouvons nous attendre
  à rejeter une proportion de
  \begin{equation*}
    \frac{16/9 - 1}{16/9} = \frac{7}{16} \approx 44~\%
  \end{equation*}
  des nombres $u_1$ simulés à l'étape~1 de
  l'algorithme ci-dessus. Si l'enveloppe
  était plus «serrée» autour de la densité, l'efficacité de
  l'algorithme en serait augmentée.
  \qed
\end{exemple}

\begin{exemple}
  \label{exemple:simulation:beta:2}
  Reprenons l'\autoref{exemple:simulation:beta:1} en tentant
  d'améliorer l'efficacité de l'algorithme. Il s'avère que nous pouvons
  inscrire la densité de la loi Bêta$(3, 2)$ dans un triangle aux
  caractéristiques suivantes (voir la
  \autoref{fig:simulation:beta-triangle}):
  \begin{figure}
    \centering
<<echo=FALSE, fig=TRUE>>=
curve(dbeta(x, 3, 2), xlim = c(0, 1), ylim = c(0, 2.5), col = "black", lwd = 2)
lines(c(0, 0.8, 1), c(0, 2.4, 0), lwd = 2, col = "orange")
segments(0.5, 0, 0.5, 1.5, col = "lightblue", lwd = 2, lty = 3)
segments(0.8, 0, 0.8, 2.4, col = "lightblue", lwd = 2, lty = 3)
axis(side = 1, at = 0.5)
@
    \caption{Fonction de densité de probabilité d'une loi Bêta$(3, 2)$
      enveloppée d'un triangle}
    \label{fig:simulation:beta-triangle}
  \end{figure}
  \begin{enumerate}
  \item l'arête gauche passe par $(0, 0)$, donc est de la forme $y =
    mx$. Cette droite étant tangente à $f(x)$, la pente $m$ est telle
    que l'équation $mx = 12 x^2 (1 - x)$ a une seule racine autre que
    $0$, d'où $y = 3 x$;
  \item l'arête droite passe par $(1, 0)$, donc est de la forme $y =
    mx + b$ avec $m + b = 0$. Comme la pente de cette droite est égale
    à la pente de $f(x)$ en $x = 1$, nous trouvons que $y = 12 - 12 x$.
  \end{enumerate}
  Ainsi,
  \begin{displaymath}
    c g_Y(x) =
    \begin{cases}
      3x,       & 0 < x < 0,8 \\
      12 - 12 x, & 0,8 < x < 1.
    \end{cases}
  \end{displaymath}
  Or, l'aire du triangle est
  \begin{displaymath}
    \frac{(1) c g_Y(0,8)}{2} = \frac{(1)(2,4)}{2} = 1,2,
  \end{displaymath}
  d'où $c = 1,2$. Par conséquent,
  \begin{align*}
    g_Y(x)
    &=
    \begin{cases}
      2,5 x,     & 0 < x < 0,8 \\
      10 - 10 x, & 0,8 < x < 1.
    \end{cases}
  \end{align*}
  Pour simuler des observations de cette densité par la méthode de
  l'inverse, nous calculons la fonction de répartition correspondante,
  \begin{align*}
    G_Y(x)
    &=
    \begin{cases}
      1,25 x^2,          & 0 < x < 0,8 \\
      -5 x^2 + 10 x - 4, & 0,8 < x < 1,
    \end{cases} \\
    \intertext{d'où}
    G_Y^{-1}(y)
    &=
    \begin{cases}
      \sqrt{0,8 y},           & 0 < y < 0,8 \\
      1 - \sqrt{0,2 - 0,2 y}, & 0,8 < y < 1.
    \end{cases}
  \end{align*}
  Au final, nous obtenons l'algorithme suivant:
  \begin{enumerate}
  \item Simuler deux nombres $u_1$ et $u_2$ d'une $U(0, 1)$.
  \item Poser $y = G_Y^{-1}(u_1)$.
  \item Si
    \begin{displaymath}
      u_2 \leq \frac{f_X(y)}{1,2 g_Y(y)} \Leftrightarrow
      1,2 g_Y(y) u_2 \leq f_X(y),
    \end{displaymath}
    alors poser $x = y$. Sinon, retourner à l'étape 1.
  \end{enumerate}

  L'aire du triangle étant de $1,2$, nous pouvons maintenant
  s'attendre à ne rejeter que $0,2/1,2 \approx 17~\%$ des nombres
  simulés, une amélioration importante par rapport à
  l'\autoref{exemple:simulation:beta:1}. %

  \gotorbox{Une mise en œuvre en R de l'algorithme ci-dessus est
    présentée dans le code informatique de la
    \autoref{sec:simulation:code}.}%
  \qed
\end{exemple}

\begin{prob-astuce}
  \label{astuce:simulation:2}
  L'interprétation géométrique de la méthode d'acceptation-rejet
  illustrée à la \autoref{fig:simulation:acceptation-rejet2} nous met
  sur la piste d'une manière simple de générer des points distribués
  uniformément sur un disque.

  En effet, il suffit de simuler des points uniformément sur un carré
  comme nous l'avons déjà fait au \autoref{chap:generation}, puis de
  rejeter ceux qui ne se trouvent pas à l'intérieur du disque. La
  \autoref{fig:simulation:disque} illustre cette idée: nous
  accepterions les points pleins et nous rejetterions les points
  vides.
\end{prob-astuce}

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE>>=
x <- runif(200, -1, 1)
y <- runif(200, -1, 1)
w <- x^2 + y^2 < 1
t <- seq(0, 2 * pi, length = 1000)

plot(x, y, xlab="x", ylab="y", xlim=c(-1, 1), ylim=c(-1, 1),
     pch = ifelse(w, 21, 1), bg = "lightblue")
#points(x[!w], y[!w], xlab="x", ylab="y", pch = 1)
lines(x = cos(t), y = sin(t), col = "black", lwd = 2)
rect(-1, -1, 1, 1, border = "orange", lwd = 2)
@
  \caption{Simulation de points sur un disque par une méthode d'acceptation-rejet}
  \label{fig:simulation:disque}
\end{figure}


\section{Fonctions de simulation dans Excel et R}
\label{sec:simulation:excel_et_al}

Il est important de savoir simuler des valeurs d'une variable
aléatoire quelconque à partir de la méthode de l'inverse ou d'un autre
algorithme, surtout si la distribution de la variable aléatoire est
peu usitée. Néanmoins, les différents outils statistiques fournissent
en général la majorité des fonctions de simulation de variables
aléatoires dont nous pouvons avoir besoin pour un usage quotidien.

\subsection{Excel}

Tel que mentionné à la \autoref{sec:simulation:inverse}, il faut
généralement utiliser la méthode de l'inverse pour simuler des
observations de lois de probabilité dans Excel. Cette procédure est
facilitée par le fait qu'il existe des fonctions Excel pour calculer
la fonction de répartition inverse (ou fonction de quantile) des lois
les plus courantes.

Ainsi, Excel fournit la fonction de densité de probabilité (lois
continues) ou la fonction de masse de probabilité (lois discrètes) ou
la fonction de répartition et, dans certains cas seulement, la
fonction de quantile des lois de probabilité présentées au
\autoref{tab:excel}. Les noms anglais des fonctions ont été modifiés
pour être standardisés dans Excel~2010. Vous remarquerez que les
traducteurs français ont omis de faire de même.

\begin{table}[t]
  \centering
  \begingroup
    \renewcommand{\code}[1]{\texttt{\small #1}}
  \begin{tabularx}{\linewidth}{lp{17ex}X}
    \toprule
    Loi &
    Fonctions Excel \newline (nom anglais) &
    Fonctions Excel \newline (nom français) \\
    \midrule
    Bêta &
    \code{BETA.DIST} \newline \code{BETA.INV} &
    \code{LOI.BETA.N} \newline \code{BETA.INVERSE.N} \\
    Binomiale &
    \code{BINOM.DIST} \newline \code{BINOM.INV} &
    \code{LOI.BINOMALE.N} \newline \code{LOI.BINOMIALE.INVERSE} \\
    Binomiale nég. &
    \code{NEGBINOM.DIST} &
    \code{LOI.BINOMIALE.NEG.N} \\
    Exponentielle &
    \code{EXPON.DIST} &
    \code{LOI.EXPONENTIELLE.N} \\
    \emph{F} (Fisher) &
    \code{F.DIST} \newline \code{F.INV} &
    \code{LOI.F.N} \newline \code{INVERSE.LOI.F.N} \\
    Gamma &
    \code{GAMMA.DIST} \newline \code{GAMMA.INV} &
    \code{LOI.GAMMA.N} \newline \code{LOI.GAMMA.INVERSE.N} \\
    Hypergéom. &
    \code{HYPGEOM.DIST} &
    \code{LOI.HYPERGEOMETRIQUE.N} \\
    Khi carré &
    \code{CHISQ.DIST} \newline \code{CHISQ.INV} &
    \code{LOI.KHIDEUX} \newline \code{LOI.KHIDEUX.INVERSE} \\
    Log-normale &
    \code{LOGNORM.DIST} \newline \code{LOGNORM.INV} &
    \code{LOI.LOGNORMALE.N} \newline \code{LOI.LOGNORMALE.INVERSE.N} \\
    Normale &
    \code{NORM.DIST} \newline
    \code{NORM.INV} \newline
    \code{NORM.S.DIST} \newline
    \code{NORM.S.INV} &
    \code{LOI.NORMALE.N} \newline
    \code{LOI.NORMALE.INVERSE.N} \newline
    \code{LOI.NORMALE.STANDARD.N} \newline
    \code{LOI.NORMALE.STANDARD.INVERSE.N} \\
    Poisson &
    \code{POISSON.DIST} &
    \code{LOI.POISSON.N} \\
    \emph{t} (Student) &
    \code{T.DIST} \newline \code{T.INV} &
    \code{LOI.STUDENT.N} \newline \code{LOI.STUDENT.INVERSE.N} \\
    Weibull &
    \code{WEIBULL.DIST} &
    \code{LOI.WEIBULL.N} \\
    \bottomrule
  \end{tabularx}
  \endgroup
  \caption{Liste des fonctions relatives à des lois de probabilité
    depuis Excel 2010.}
  \label{tab:excel}
\end{table}

\subsection{R}

Un large éventail de fonctions donne directement accès aux
caractéristiques de plusieurs lois de probabilité dans R. Pour chaque
racine \code{\meta{loi}}, il existe quatre fonctions différentes :
\begin{enumerate}
\item \code{d\meta{loi}} calcule la fonction de densité de
  probabilité (loi continue) ou la fonction de masse de probabilité
  (loi discrète);
\item \code{p\meta{loi}} calcule la fonction de répartition;
\item \code{q\meta{loi}} calcule la fonction de quantile;
\item \code{r\meta{loi}} simule des observations de cette loi.
\end{enumerate}

Les différentes lois de probabilité disponibles dans le système R de
base, leur racine et le nom de leurs paramètres sont rassemblés au
\autoref{tab:rng:lois}. Des paquetages fournissent des fonctions
pour d'autres lois dont, entre autres, \pkg{actuar} \citep{actuar} et
\pkg{SuppDists} \citep{Rpackage:SuppDists}.

\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    Loi de probabilité & Racine dans R & Noms des paramètres \\
    \midrule
    Bêta & \code{beta} & \code{shape1}, \code{shape2} \\
    Binomiale & \code{binom} & \code{size}, \code{prob} \\
    Binomiale négative & \code{nbinom} & \code{size},
    \code{prob} ou \code{mu} \\
    Cauchy & \code{cauchy} & \code{location}, \code{scale} \\
    Exponentielle & \code{exp} & \code{rate} \\
    \emph{F} (Fisher) & \code{f} & \code{df1}, \code{df2} \\
    Gamma & \code{gamma} & \code{shape}, \code{rate} ou
    \code{scale} \\
    Géométrique & \code{geom} & \code{prob} \\
    Hypergéométrique & \code{hyper} & \code{m}, \code{n},
    \code{k} \\
    Khi carré & \code{chisq} & \code{df} \\
    Logistique & \code{logis} & \code{location}, \code{scale} \\
    Log-normale & \code{lnorm} & \code{meanlog}, \code{sdlog} \\
    Normale & \code{norm} & \code{mean}, \code{sd} \\
    Poisson & \code{pois} & \code{lambda} \\
    \emph{t} (Student) & \code{t} & \code{df} \\
    Uniforme & \code{unif} & \code{min}, \code{max} \\
    Weibull & \code{weibull} & \code{shape}, \code{scale} \\
    Wilcoxon & \code{wilcox} & \code{m}, \code{n} \\
    \bottomrule
  \end{tabular}
  \caption{Lois de probabilité pour lesquelles il existe des fonctions
    dans le système R de base}
  \label{tab:rng:lois}
\end{table}

Toutes les fonctions du \autoref{tab:rng:lois} sont vectorielles,
c'est-à-dire qu'elles acceptent en argument un vecteur de points où la
fonction (de densité, de répartition ou de quantile) doit être évaluée
et même un vecteur de paramètres. Par exemple, l'expression ci-dessous
retourne la probabilité que des lois de Poisson de paramètre $1$, $4$
et $10$ prennent les valeurs $3$, $0$ et $8$, dans l'ordre.
<<echo=TRUE>>=
dpois(c(3, 0, 8), lambda = c(1, 4, 10))
@

Le premier argument de toutes les fonctions de simulation est la
quantité de nombres aléatoires désirée. Ainsi, l'expression suivante
retourne trois nombres aléatoires issus de distributions de Poisson de
paramètre $1$, $4$ et $10$, respectivement.
<<echo=TRUE>>=
rpois(3, lambda = c(1, 4, 10))
@

Évidemment, spécifier un vecteur comme premier argument d'une fonction
de simulation n'a pas tellement de sens, mais, si c'est fait, R
retournera une quantité de nombres aléatoires égale à la
\emph{longueur} du vecteur (sans égard aux valeurs contenues dans le
vecteur).

Autre joueur important dans l'équipe de simulation de R, la fonction
\code{sample} permet de générer des nombres d'une distribution
discrète quelconque. Sa syntaxe est
\begin{Schunk}
\begin{Verbatim}
sample(x, size, replace = FALSE, prob = NULL)},
\end{Verbatim}
\end{Schunk}
\begin{itemize}
\item \code{x} est un vecteur des valeurs possibles de
  l'échantillon à simuler (le support de la distribution);
\item \code{size} est la quantité de nombres à simuler;
\item \code{prob} est un vecteur de probabilités associées à chaque
  valeur de \code{x} (par défaut \code{1/length(x)});
\item \code{replace} détermine si l'échantillonnage s'effectue avec ou
  sans remise.
\end{itemize}

\gotorbox{Le code informatique R de la \autoref{sec:simulation:code}
  contient des exemples plus détaillés d'utilisation des fonctions
  mentionnées ci-dessus.}


\section{Modèles actuariels}
\label{sec:simulation:modeles-actuariels}

Cette section traite de techniques de simulation pour des
distributions couramment utilisées dans la modélisation des risques en
sciences actuarielles.

\subsection{Mélanges discrets}
\label{sec:simulation:modeles-actuariels:melanges}

Les \emph{mélanges discrets} (ou finis; \emph{mixture} en anglais) sont
utiles pour décrire des phénomènes considérés comme provenant de la
combinaison de plusieurs phénomènes qui surviennent chacun avec une
probabilité connue d'avance. Par exemple, si nous estimons qu'il y a
90~\% de chances que la distribution du nombre de personnes dans une
file d'attente soit une Poisson de paramètre $2$ et 10~\% de chances
qu'il s'agisse plutôt d'une Poisson de paramètre $10$, alors la
fonction de masse de probabilité de la distribution du nombre de
personnes dans la file d'attente en tout temps est:
\begin{equation*}
  \Pr[X = x] =
  0,9 \times \frac{2^x e^{-2}}{x!} + 0,1 \times \frac{10^x e^{-10}}{x!}.
\end{equation*}
Cette fonction de masse de probabilité est un mélange discret de deux
lois de Poisson.

\begin{definition}
  \label{def:simulation:melange-discret}
  La variable aléatoire $Y$ est un mélange discret à $k$ points des
  variables aléatoires $X_1, X_2, \dots X_k$ si sa fonction de
  répartition est
  \begin{equation*}
    F_Y(x) = p_1 F_{X_1}(x) + p_2 F_{X_2}(x) + \dots + p_k F_{X_k}(x),
  \end{equation*}
  où $p_j \in [0, 1]$ pour tout $j = 1, \dots, k$ et
  $\sum_{i = 1}^k p_j = 1$.
\end{definition}

Dans la suite, nous ne considérerons que les mélanges de $k = 2$
distributions. Lorsque celles-ci sont continues avec fonctions de
densité de probabilité $f_1$ et $f_2$, la densité du mélange peut
s'écrire
\begin{equation*}
  f(x) = p f_1(x) + (1 - p) f_2(x).
\end{equation*}

\begin{exemple}
  Les mélanges discrets sont très souvent utilisés pour créer de
  nouvelles distributions aux caractéristiques particulières que l'on
  ne retrouve pas chez les distributions d'usage courant.

  La \autoref{fig:simulation:ln.ln} montre un mélange de deux
  distributions log-normales qui résulte en une fonction de densité de
  probabilité bimodale ayant une queue similaire à celle d'une
  log-normale. %
  \qed

  \begin{figure}
    \centering
<<echo=FALSE, fig=TRUE, width=4.5, height=7>>=
par(mfrow = c(3, 1), lwd = 1)
par1 <- c(3.6, 0.6)
par2 <- c(4.5, 0.3)
p <- 0.55
titre1 <- bquote(paste("Log-normale(", mu, " = ", .(par1[1]), ", ", sigma, " = ", .(par1[2]), ")"))
titre2 <- bquote(paste("Log-normale(", mu, " = ", .(par2[1]), ", ", sigma, " = ", .(par2[2]), ")"))
titre3 <- bquote(paste("Mélange (p = ", .(p), ")"))
curve(dlnorm(x, par1[1], par1[2]), xlim = c(0, 250),
      main = titre1, ylab = "f(x)", lwd = 2)
curve(dlnorm(x, par2[1], par2[2]), xlim = c(0, 250),
      main = titre2, ylab = "f(x)", lwd = 2)
curve(p * dlnorm(x, par1[1], par1[2]) + (1 - p) * dlnorm(x, par2[1], par2[2]),
      xlim = c(0, 250), main = titre3, ylab = "f(x)", lwd = 2)
@
    \caption{Fonctions de densité de probabilité de deux log-normales et
      de leur mélange discret}
    \label{fig:simulation:ln.ln}
  \end{figure}
\end{exemple}

La conception d'un algorithme de simulation d'un mélange discret à
deux distributions nécessite de bien comprendre que chaque observation
provient de l'une \emph{ou} de l'autre des distributions. Reste, à
chaque fois, à déterminer à partir de quelle distribution simuler.

\begin{algorithme}[Mélange discret de deux distributions]
  \label{algo:simulation:melange-discret}
  Soit $F(x) = p F_1(x) + (1 - p) F_2(x)$ la fonction de répartition
  d'un mélange discret des distributions avec fonctions de répartition
  $F_1$ et $F_2$. Les étapes ci-dessous permettent de générer un
  nombre aléatoire $x$ de la distribution $F$.
  \begin{enumerate}
  \item Obtenir un nombre $u$ d'une loi $U(0, 1)$.
  \item Si $u \leq p$, obtenir $x$ de la distribution $F_1$, sinon
    obtenir $x$ de $F_2$.
  \end{enumerate}
\end{algorithme}

Vous aurez reconnu dans le test $u \leq p$, où $u$ provient d'une loi
$U(0, 1)$, le cœur de la procédure de simulation d'une loi de
Bernoulli mentionnée à l'\autoref{ex:simulation:binomiale}.
L'\autoref*{algo:simulation:melange-discret} revient donc à simuler
un nombre de la distribution $F_1$ si $u \leq p$ et aucun nombre de la
distribution $F_2$ ou, au contraire, aucun nombre de $F_1$ si $u > p$ et
un nombre de $F_2$. Cette observation joue un rôle important pour
concevoir une procédure de simulation de $n > 1$ valeurs d'un mélange.

La fonction \code{rmixture} du paquetage \pkg{actuar} offre une
interface conviviale pour obtenir des nombres aléatoires de mélanges
discrets.

\gotorbox{Le code informatique de la \autoref{sec:simulation:code}
  propose une procédure de simulation d'un mélange discret ainsi que
  des exemples d'utilisation de la fonction \code{rmixture}.}

\subsection{Mélanges continus et modèles hiérarchiques}
\label{sec:simulation:modeles-actuariels:hierarchiques}

Nous pouvons étendre le concept de mélange discret à une quantité non
dénombrable de distributions. Il en résulte ce que l'on nomme un
\emph{mélange continu}. La fonction de densité de probabilité $u(\theta)$
dans la définition ci-dessous joue le rôle des probabilités discrètes
$p_j$ de la \autoref{def:simulation:melange-discret}.

\begin{definition}
  Soit $X$ une variable aléatoire dont la distribution dépend d'un
  paramètre $\theta$. On considère ce paramètre comme une réalisation
  d'une variable aléatoire $\Theta$ avec fonction de densité
  $u(\theta)$. La fonction de densité de la variable aléatoire
  $X|\Theta = \theta$ est $f(x|\theta)$. Par la loi des probabilités
  totales, la fonction de densité marginale (ou non conditionnelle) de
  la variable aléatoire $X$ est
  \begin{equation*}
    f_X(x) = \int_\Omega f(x|\theta) u(\theta)\, d\theta.
  \end{equation*}
  Cette distribution est un mélange continu des
  distributions de $X|\Theta$ et $\Theta$.
\end{definition}

Les mélanges continus sont beaucoup utilisés en analyse bayesienne et
dans certaines disciplines de l'actuariat. Plusieurs lois de
probabilité sont uniquement définies en tant que mélanges.

L'algorithme ci-dessous, très simple, permet d'obtenir des
observations aléatoires d'un mélange de distributions.

\begin{algorithme}[Mélange continu de deux distributions]
  \label{algo:simulation:melange-continu}
  Soit $f(x)$ la fonction de densité de probabilité du mélange continu
  des distributions avec densités $f(x|\theta)$ et $u(\theta)$. Les
  étapes ci-dessous permettent de générer un nombre aléatoire $x$ de
  la densité $f_X$.
  \begin{enumerate}
  \item Obtenir un nombre $\theta$ issu de la distribution avec
    densité $u(\theta)$.
  \item Obtenir $x$ de la distribution avec densité $f(x|\theta)$.
  \end{enumerate}
\end{algorithme}

Pour simuler plusieurs observations d'un mélange, il faut répéter les
deux étapes de l'\autoref*{algo:simulation:melange-continu} pour
\emph{chaque} observation. Le paramètre de mélange ($\theta$) change
donc à chaque fois. C'est essentiel, autrement il n'y aurait pas de
mélange et nous obtiendrions simplement un échantillon de la
distribution $f(x|\theta)$ pour une valeur de $\theta$ quelconque.

La simulation de mélanges continus s'avère très simple à réaliser en R
grâce à la nature vectorielle des fonctions de simulation.

\begin{exemple}
  Soit $X$ une variable aléatoire issue du mélange
  $X|\Theta = \theta \sim \text{Poisson}(\theta)$ et
  $\Theta \sim \text{Gamma}(5, 4)$. Les deux expressions R ci-dessous
  permettent de générer \nombre{1000} observations de $X$.
<<echo=TRUE, eval=FALSE>>=
theta <- rgamma(1000, 5, 4)
x <- rpois(1000, theta)
@

  Il est tout-à-fait raisonnable, ici, d'écrire le tout en une seule
  expression. Cela fait d'ailleurs bien ressortir le fait que le
  paramètre de la loi de Poisson est lui-même issu d'une simulation.
<<echo=TRUE, eval=FALSE>>=
x <- rpois(1000, rgamma(1000, 5, 4))
@
  \qed
\end{exemple}

Il est possible de répéter la procédure de mélange plus d'une fois, ce
qui donne lieu à ce que nous nommerons des \emph{modèles
  hiérarchiques}\footnote{%
  En sciences actuarielles, cette terminologie est surtout employée
  dans les contextes de structures de classification des risques sous
  forme d'arbre. Dans de tels cas, le nombre de valeurs à simuler
  pourrait différer à chaque niveau de l'arbre.}. %
La généralisation de l'\autoref{algo:simulation:melange-continu} pour
les modèles hiérarchiques devrait aller de soi.

\begin{exemple}
  Soit le modèle hiérarchique suivant où les variables aléatoires
  $\Theta$ et $\Lambda$ représentent les paramètres de mélange:
  \begin{align*}
    X|\Lambda, \Theta &\sim \text{Poisson}(\Lambda) \\
    \Lambda|\Theta    &\sim \text{Gamma}(3, \Theta) \\
    \Theta            &\sim \text{Gamma}(2, 2).
  \end{align*}
  L'expression R suivante permet de générer $n$ observations de la
  variable aléatoire $X$.
<<echo=TRUE, eval=FALSE>>=
rpois(n, rgamma(n, 3, rgamma(n, 2, 2)))
@
  \qed
\end{exemple}

\subsection{Distributions composées}
\label{sec:simulation:modeles-actuariels:composees}

Les distributions composées jouent un rôle central dans la
modélisation des risques en actuariat puisqu'elles permettent de
représenter séparément l'effet de la fréquence et de la sévérité des
sinistres dans un portefeuille d'assurance.

\begin{definition}
  Soit $N$ une variable aléatoire discrète avec fonction de
  répartition $P_N(x)$ et $X_1, X_2, \dots$ des variables aléatoires
  mutuellement indépendantes, identiquement distribuées avec fonction
  de répartition $F_X(x)$ et indépendantes de la variable aléatoire
  $N$. La variable aléatoire $S = X_1 + \dots + X_N$ --- avec, par
  convention, $S = 0$ lorsque $N = 0$ --- a une distribution composée.
  Sa fonction de répartition est
  \begin{equation*}
    F_S(x) = \sum_{n = 0}^\infty F_X^{* n}(x) p_N(n),
  \end{equation*}
  où $p_N$ est la fonction de masse de probabilité de $N$ et
  $F_X^{* n}$ est la fonction de répartition de la $n\ieme$
  convolution de $X$.
\end{definition}

Dans l'appellation de la distribution de la variable aléatoire $S$,
l'adjectif «composée» est accolé au nom de la distribution de la
variable aléatoire $N$. Par exemple, si la distribution de $N$ est une
loi de Poisson, alors la distribution de $S$ est une Poisson composée.
C'est d'ailleurs là le cas le plus fréquent --- et de loin --- en
modélisation des risques d'assurance.

L'algorithme suivant décrit la procédure de simulation d'observations
d'une distribution composée.

\begin{algorithme}[Distribution composée]
  \label{algo:simulation:compound}
  Soit $S = X_1 + \dots + X_N$ une distribution composée. Les étapes
  ci-dessous permettent de générer une observation $s$ de la variable
  aléatoire $S$.
  \begin{enumerate}
  \item Obtenir un nombre $n$ de la distribution avec fonction de
    répartition $P_N$.
  \item Obtenir des nombres $x_1, \dots, x_n$ de la distribution avec
    fonction de répartition $F_X$.
  \item Poser $s = x_1 + \dots + x_n$.
  \end{enumerate}
\end{algorithme}

Contrairement à la simulation des mélanges continus, il n'est pas
possible, en R, de vectoriser la procédure de simulation de
l'\autoref*{algo:simulation:compound}. Il faut avoir recours à une
boucle ou, mieux, à une fonction d'application.

La fonction \code{rcompound} du paquetage \pkg{actuar} permet de
générer facilement et intuitivement des nombres aléatoires d'une
distribution composée quelconque. La fonction \code{rcomppois} offre
une interface simplifiée pour le cas le plus courant de la Poisson
composée. Enfin, bien que le sujet se trouve hors de la portée du
présent document, mentionnons que la très générale fonction
\code{rcomphierarc} permet de générer des observations de modèles
hiérarchiques composés.

\gotorbox{Le code informatique de la \autoref{sec:simulation:code}
  fournit deux mises en œuvre de l'\autoref{algo:simulation:compound}
  ainsi que des exemples d'utilisation des fonctions
  \code{rcomppois} et \code{rcompound}.}

\tipbox{L'\autoref{chap:planification} propose quelques trucs pour
  bien planifier une étude de simulation en R.}

\begin{figure}
  \centering
  \begin{framed}
\begin{lstlisting}
sim.disque <- function(n)
{
    ## La fonction retourne les coordonnées des points
    ## dans une matrice de deux colonnes. Nous créons
    ## d'abord un contenant.
    X <- matrix(NA, n, 2)

    ## Remplissage de la matrice.
    i <- 1
    repeat
    {
        ## Simulation de coordonnées dans un carré
        ## 2 x 2 centré à l'origine.
        x <- runif(2, -1, 1)

        ## Si les coordonnées sont dans le disque...
        if (sum(x^2) < 1)
        {
            ## ... la paire est ajoutée à la matrice 'X'.
            X[i, ] <- x

            ## Il faut cesser après avoir accepté 'n' paires.
            if (n < (i <- i + 1))
                break
        }
    }
    X
}
\end{lstlisting}
  \end{framed}
  \caption{Code d'une fonction pour simuler des nombres uniformément
    sur un disque de rayon $1$ centré à l'origine par la méthode
    d'acceptation-rejet}
  \label{fig:simulation:sim.disque}
\end{figure}

\begin{prob-solution}
  Nous adoptons la stratégie expliquée à l'astuce de la
  \autopageref{astuce:simulation:2}. La fonction R de la
  \autoref{fig:simulation:sim.disque} genère des points distribués
  uniformément sur un disque par la méthode d'acceptation-rejet en
  simulant d'abord des points sur un carré.

  Avec en mains cette fonction, il devient simple de vérifier par
  simulation la probabilité $\frac{35}{12 \pi^2} \approx 0,29552$ en
  procédant comme au \autoref{chap:generation}:
<<echo=FALSE>>=
sim.disque <- function(n)
{
    X <- matrix(NA, n, 2)

    i <- 1
    repeat
    {
        x <- runif(2, -1, 1)

        if (sum(x^2) < 1)
        {
            X[i, ] <- x
            if (n < (i <- i + 1))
                break
        }
    }
    X
}
@
<<<echo=TRUE>>=
mean(replicate(1E5, 3 == length(chull(sim.disque(4)))))
@

  Une autre stratégie de simulation se révèle toutefois plus efficace. Elle
  repose sur la simulation des coordonnées polaires (rayon et angle)
  des points sur le disque, puis de leur transformation en coordonnées
  cartésiennes. Si un point du disque unité centré à l'origine se trouve à
  un rayon $r < 1$ du centre et à un angle $\theta$ de l'horizontale,
  alors ses coordonnées cartésiennes sont
  \begin{align*}
    x &= \sqrt{r} \cos \theta \\
    y &= \sqrt{r} \sin \theta.
  \end{align*}
  La \autoref{fig:simulation:sim.disque2} propose une
  mise en œuvre de cette méthode en R. La démonstration de la
  validité de cette méthode fait quant à elle l'objet de
  l'\autoref{ex:simulation:disque}.

  Parce qu'elle ne requiert aucune boucle, cette seconde méthode est
  \emph{beaucoup} plus rapide que la méthode d'acceptation-rejet:
<<echo=FALSE>>=
sim.disque2 <- function(n)
{
    r <- runif(n)
    angle <- runif(n, 0, 2 * pi)
    sqrt(r) * cbind(cos(angle), sin(angle))
}
@
<<echo=TRUE>>=
system.time(sim.disque(1E5))
system.time(sim.disque2(1E5))
@
\end{prob-solution}

\begin{figure}
  \centering
  \begin{framed}
\begin{lstlisting}
sim.disque2 <- function(n)
{
    ## Simulation des coordonnées polaires
    r <- runif(n)                       # rayon
    angle <- runif(n, 0, 2 * pi)        # angle

    ## Transformation en coordonnées cartésiennes
    sqrt(r) * cbind(cos(angle), sin(angle))
}
\end{lstlisting}
  \end{framed}
  \caption{Code d'une fonction pour simuler des nombres uniformément
    sur un disque de rayon $1$ centré à l'origine en passant par les
    coordonnées polaires}
  \label{fig:simulation:sim.disque2}
\end{figure}


\section{Code informatique}
\label{sec:simulation:code}

\def\scriptfilename{simulation.R}

\scriptfile{\scriptfilename}
\lstinputlisting[firstline=14]{\scriptfilename}


\section{Exercices}
\label{sec:simulation:exercices}

\Opensolutionfile{reponses}[reponses-simulation]
\Opensolutionfile{solutions}[solutions-simulation]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref*{chap:simulation}}
\addcontentsline{toc}{section}{Chapitre \ref*{chap:simulation}}

\end{Filesave}

\begin{exercice}
  La transformation de Box--Muller est populaire pour simuler des
  nombres normaux à partir de nombres uniformes. Soit $U_1 \sim U(0,
  1)$ et $U_2 \sim U(0, 1)$ deux variables aléatoires indépendantes et
  \begin{align*}
    X_1 &= (-2 \log U_1)^{1/2} \cos (2\pi U_2) \\
    X_2 &= (-2 \log U_1)^{1/2} \sin (2\pi U_2).
  \end{align*}
  \begin{enumerate}
  \item Vérifier de manière heuristique que la transformation
    ci-dessus est bijective de $\{(u_1, u_2); 0 < u_1 < 1, 0 < u_2 <
    1\}$ à $\{(x_1, x_2); -\infty < x_1 < \infty, -\infty < x_2 <
    \infty\}$, c'est-à-dire qu'elle associe à un point $(u_1, u_2)$ un
    et un seul point $(x_1, x_2)$.
  \item Démontrer que la transformation inverse est
    \begin{align*}
      U_1 &= e^{-(X_1^2 + X_2^2)/2} \\
      U_2 &= \frac{1}{2 \pi} \arctan \frac{X_2}{X_1}.
    \end{align*}
  \item Démontrer que $X_1$ et $X_2$ sont deux variables aléatoires
    indépendantes chacune distribuée selon une $N(0, 1)$.
  \item Vérifier empiriquement la validité de ces formules à l'aide de
    Excel ou de R. Dans R, il est possible de transformer les nombres
    uniformes obtenus avec la fonction \code{runif} en nombres normaux
    sans même utiliser de boucles grâce à la fonction \code{outer} et
    deux fonctions anonymes.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Tout d'abord, on voit que
      \begin{align*}
        \cos (2\pi U_{2}) &\in (-1, 1) \\
        \sin (2\pi U_{2}) &\in (-1, 1) \\
        \intertext{et}
        (-2 \log U_1)^{1/2} &\in (0, \infty).
      \end{align*}
      Par conséquent, $X_1 \in (-\infty, \infty)$ et $X_2 \in
      (-\infty, \infty)$. On vérifie la bijectivité de façon
      heuristique avec quelques valeurs de $u_1$ et $u_2$.
    \item On a
      \begin{align*}
        X_1^2 &= (-2 \log U_1) \cos^2 (2\pi U_2) \\
        X_2^2 &= (-2 \log U_1) \sin^2 (2\pi U_2).
      \end{align*}
      Or, puisque $\sin^{2}(x) + \cos^{2}(x) = 1$, $X_1^2 + x_2^2 = -2
      \log U_1$, d'où $U_1 = e^{-(X_1^2 + X_2^2)/2}$. D'autre part,
      $\sin(x)/\cos(x) = \tan(x)$, donc $\tan (2\pi U_2) = X_2/X_1$
      ou, de manière équivalente, $U_2 = (2 \pi)^{-1} \arctan
      X_2/X_1$.
    \item Soit les fonctions
      \begin{align*}
        x_1(u_1, u_2)
        &= (-2\log u_1)^{1/2} \cos (2\pi u_2)  &
        u_1(x_1,x_2)
        &= e^{-(x_1^2 + x_2^2)/2}  \\
        x_2(u_1, u_2)
        &= (-2\log u_1)^{1/2} \sin (2\pi u_2) &
        u_2(x_1, x_2)
        &= \frac{1}{2\pi} \arctan \frac{x_2}{x_1}.
      \end{align*}
      Les variables aléatoires $U_1$ et $U_2$ sont indépendantes, donc
      leur fonction de densité de probabilité conjointe est le produit
      des densités marginales:
      \begin{displaymath}
        f_{U_1, U_2}(u_1, u_2) = 1, \quad 0 < u_1 < 1, 0 < u_2 < 1.
      \end{displaymath}
      La densité conjointe de $X_1$ et $X_2$ est, par définition d'une
      transformation,
      \begin{displaymath}
        f_{X_1,X_2}(x_1, x_2) =
        f_{U_1, U_2}(x_1(u_1, u_2), x_2(u_1,u_2)) |\det(J)|,
      \end{displaymath}%
      où
      \begin{equation*}
        J =
        \begin{bmatrix}
          \dfrac{\partial u_1}{\partial x_1} &
          \dfrac{\partial u_1}{\partial x_2} \\[6pt]
          \dfrac{\partial u_2}{\partial x_1} &
          \dfrac{\partial u_2}{\partial x_2}
        \end{bmatrix} =
        \begin{bmatrix}
          -x_1 e^{- (x_1^2 + x_2^2)/2} &
          -x_2 e^{- (x_1^2 + x_2^2)/2} \\[6pt]
          -\dfrac{1}{2\pi} \dfrac{x_2}{x_1^2 + x_2^2} &
          \dfrac{1}{2\pi} \dfrac{x_1}{x_1^2 + x_2^2}
        \end{bmatrix}.
      \end{equation*}
      Or,
      \begin{align*}
        |\det(J)|
        &= \frac{1}{2\pi}\, e^{-(x_1^2 + x_2^2)/2} \\
        &= \frac{1}{\sqrt{2\pi}}\, e^{-x_1^2/2} \cdot
        \frac{1}{\sqrt{2\pi}} e^{-x_2^2/2},
      \end{align*}
      d'où
      \begin{displaymath}
        f_{X_1,X_2}(x_1, x_2) = \frac{1}{\sqrt{2\pi}} e^{-x_1^2/2} \cdot
        \frac{1}{\sqrt{2\pi}} e^{-x_2^2/2}.
      \end{displaymath}
      Par conséquent, $X_1$ et $X_2$ sont deux variables aléatoires $N(0, 1)$
      indépendantes.
    \item
<<echo=TRUE, eval=FALSE>>=
u1 <- runif(500)
u2 <- runif(500)
x1 <- outer(u1, u2, function(x, y)
            sqrt((-2 * log(x))) * cos(2 * pi * y))
x2 <- outer(u1, u2, function(x, y)
            sqrt((-2 * log(x))) * sin(2 * pi * y))
hist(x1, prob = TRUE)
curve(dnorm(x), add = TRUE)
@
    \end{enumerate}
    La \autoref{fig:simulation:boxmuller} illustre d'une autre façon que la
    transformation de Box--Muller fonctionne bel et bien. Le
    graphique de gauche contient plusieurs couples de points $(u_1, u_2)$,
    où chaque composante provient d'une distribution uniforme sur
    l'intervalle $(0, 1)$.

    Chacun de ces points a été transformé en un point $(x_1, x_2)$
    selon la transformation de Box--Muller, puis placé dans le
    graphique de droite. Nous avons superposé le nuage de points ainsi
    obtenu aux lignes de niveau d'une distribution normale bivariée
    (avec paramètre $\rho = 0$). Vous observerez que la répartition et la
    densité du nuage de points correspond effectivement aux lignes de
    niveau.
    \begin{figure}
      \centering
<<echo=FALSE, fig=TRUE, width=10, height=5>>=
u1 <- runif(1000)
u2 <- runif(1000)
x1 <- sqrt(-2 * log(u1)) * cos(2 * pi * u2)
x2 <- sqrt(-2 * log(u1)) * sin(2 * pi * u2)

col <- rgb(0.67, 0.85, 0.90, alpha = 0.4) # "lightblue" semi-transparent
par(mfrow = c(1, 2))

plot(u1, u2, pch = 19, col = col)

f <- function(x, y) dnorm(x) * dnorm(y)
x <- seq(-3, 3, length = 100)
z <- outer(x, x, f)
contour(x, x, z, xlim = c(-3, 3), ylim = c(-3, 3),
        nlevels = 15, method = "edge", xlab = "x1", ylab = "x2")
points(x1, x2, pch = 19, col = col)
@
      \caption{Démonstration graphique du fonctionnement de la
        transformation de Box--Muller}
      \label{fig:simulation:boxmuller}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  La distribution de Laplace, ou double exponentielle, est définie
  comme la différence entre deux distributions exponentielles
  identiques et indépendantes. Sa fonction de densité de probabilité
  est
  \begin{displaymath}
    f(x) = \frac{\lambda}{2}\, e^{-\lambda |x|}, \quad
    -\infty < x < \infty.
  \end{displaymath}
  Proposer une ou plusieurs façons de simuler des nombres issus de
  cette distribution.
  \begin{sol}
    Deux suggestions:
    \begin{enumerate}[1.]
    \item Simuler deux nombres indépendants $x_1$ et $x_2$ d'une loi
      exponentielle de paramètre $\lambda$ et poser $y = x_1 - x_2$.
    \item La fonction de répartition de la distribution de Laplace est
      \begin{align*}
        F_{Y}(y)
        &=
        \begin{cases}
          \frac{1}{2}e^{\lambda x}, & x < 0 \\
          1-\frac{1}{2}e^{-\lambda x}, & x \geq 0,
        \end{cases} \\
        \intertext{d'où}
        F_{Y}^{-1}(u)
        &=
        \begin{cases}
          \frac{1}{\lambda} \ln (2u) & u < 0,5 \\
          \frac{-1}{\lambda} \ln (2(1-u) ) & u \geq 0,5.
        \end{cases}
      \end{align*}
      Il est donc aussi possible d'utiliser la méthode de l'inverse.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  En ouverture de chapitre, nous mentionnons qu'une façon de simuler des
  nombres issus d'une loi géométrique consiste à extraire la partie
  entière de nombres aléatoires provenant d'une loi exponentielle.

  Démontrer que si la loi de la variable aléatoire $X$ est une
  exponentielle, alors la loi de $\lfloor X \rfloor$ est une
  géométrique. Déterminer le paramètre de la loi géométrique en
  fonction du paramètre de la loi exponentielle.
  \begin{rep}
    $p = 1 - e^{-\lambda}$
  \end{rep}
  \begin{sol}
    Soit $X \sim \text{Exponentielle}(\lambda)$ et
    $K = \lfloor X \rfloor$. Par conséquent:
    \begin{align*}
      \Pr[K = 0]
      &= \Pr[\lfloor X \rfloor = 0] \\
      &= \Pr[0 \leq X < 1] \\
      &= F_X(1) \\
      &= 1 - e^{-\lambda}, \displaybreak[0] \\
      \Pr[K = 1]
      &= \Pr[1 \leq X < 2]\\
      &= F_X(2) - F_X(1)\\
      &= (1 - e^{-2 \lambda}) - (1 - e^{-\lambda}) \\
      &= (e^{-\lambda} - e^{-2\lambda}) \\
      &= (1 - e^{-\lambda}) e^{-\lambda}, \displaybreak[0] \\
      \intertext{soit, de manière générale,}
      \Pr[K = k]
      &= \Pr[k \leq X < k+1]\\
      &= (1 - e^{-\lambda})(e^{-\lambda})^k,
        \quad k = 0, 1, 2, \dots
    \end{align*}
    La forme de cette fonction de masse de probabilité est celle
    d'une loi géométrique de paramètre $p = 1 - e^{-\lambda}$.
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:gamma}
  Réaliser une mise en œuvre informatique de l'algorithme de
  simulation suivant. Il s'agit d'un algorithme pour simuler des
  observations d'une loi Gamma$(\alpha, 1)$, où $\alpha > 1$.
  \begin{enumerate}[1.]
  \item Générer $u_1$ et $u_2$ indépendemment d'une loi $U(0, 1)$ et
    poser
    \begin{displaymath}
      v = \frac{(\alpha - \frac{1}{6\alpha}) u_1}{(\alpha - 1) u_2}.
    \end{displaymath}
  \item Si
    \begin{displaymath}
      \frac{2 (u_2 - 1)}{\alpha - 1} + v + \frac{1}{v} \leq 2,
    \end{displaymath}
    alors retourner le nombre $x = (\alpha - 1) v$. Sinon, si
    \begin{displaymath}
      \frac{2 \log u_2}{\alpha - 1} - \log v + v \leq 1,
    \end{displaymath}
    alors retourner le nombre $x = (\alpha - 1) v$.
  \item Répéter au besoin la procédure depuis l'étape 1.
  \end{enumerate}
  Faire les vérifications empiriques usuelles de la validité de
  l'algorithme.
  \begin{sol}
    Voir la fonction R de la \autoref{fig:simulation:gamma}. Les
    expressions suivantes permettent de tracer les graphiques pour vérifier
    la validité de l'algorithme.
<<echo=TRUE, eval=FALSE>>=
hist(rgamma2(1000, 5), prob = TRUE)
curve(dgamma(x, 5, 1), add = TRUE)
@
    \begin{figure}
      \centering
      \begin{framed}
\begin{lstlisting}
rgamma2 <- function(nsim, alpha)
{
    x <- numeric(nsim)
    i <- 0
    while (i < nsim)
    {
        u <- runif(2)
        v <- (alpha - 1/(6 * alpha)) * u[1] /
               ((alpha - 1) * u[2])

        if ((2 * (u[2] - 1)/(alpha - 1) +
               v + 1/v <= 2) |
            (2 * log(u[2])/(alpha - 1) -
               log(v) + v <= 1))
            x[i <- i + 1] <- (alpha - 1) * v
    }
    x
}
\end{lstlisting}
      \end{framed}
      \caption{Fonction de simulation d'une distribution
        Gamma$(\alpha, 1)$, $\alpha > 1$}
      \label{fig:simulation:gamma}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  En utilisant le résultat de l'exercice précédent, quelle procédure
  pourriez-vous suivre pour simuler des nombres d'une loi Gamma$(\alpha,
  \lambda)$ où $\alpha > 1$?
  \begin{sol}
    Deux suggestions.
    \begin{enumerate}[1.]
    \item Si $X \sim \text{Gamma}(\alpha, 1)$, alors $Y = X/\lambda
      \sim \text{Gamma}(\alpha, \lambda)$. Nous pouvons donc générer un
      nombre $x$ d'une loi Gamma$(\alpha, 1)$ avec l'algorithme de
      l'\autoref{ex:simulation:gamma}, puis poser $y =
      x/\lambda$.
    \item Si $\alpha$ est entier, nous pouvons également générer
      $\alpha$ nombres $x_1, \dots, x_\alpha$ d'une distribution
      Exponentielle$(\lambda)$ et poser $y = \sum_{i=1}^\alpha x_i$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item Démontrer que si $X|\Theta \sim \text{Exponentielle}(\Theta)$
    et $\Theta \sim \text{Gamma}(\alpha, \lambda)$, alors $X \sim
    \text{Pareto}(\alpha, \lambda)$. La fonction de densité de
    probabilité d'une loi de Pareto est
    \begin{displaymath}
      f(x) = \frac{\alpha \lambda^\alpha}{(x + \lambda)^{\alpha + 1}},
      \quad x > 0.
    \end{displaymath}
  \item Utiliser le résultat ci-dessus pour proposer un algorithme de
    simulation de nombres issus d'une loi de Pareto. Faire la mise en
    œuvre informatique de cet algorithme et les vérifications d'usage
    de sa validité.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Vous aurez reconnu dans cette combinaison de variables
      aléatoires un mélange d'une exponentielle et d'une gamma. Par la
      loi des probabilités totales,
      \begin{align*}
        f_{X}(x)
        &= \int_0^\infty f(x|\theta) u(\theta)\, d\theta  \\
        &= \frac{\lambda^\alpha}{\Gamma(\alpha)} \int_0^\infty
        \theta^{\alpha + 1 - 1} e^{-(\lambda + x) \theta}\, d\theta
          \displaybreak[0]\\
        &= \frac{\lambda^\alpha}{\Gamma(\alpha)}
        \frac{\Gamma(\alpha + 1)}{(\lambda + x)^{\alpha +1}} \\
        &= \frac{\alpha \lambda^\alpha}{(\lambda + x)^{\alpha +1}}.
      \end{align*}
    \item Pour générer un nombre d'une distribution de Pareto de
      paramètres $\alpha$ et $\lambda$ avec le résultat en a), il faut
      d'abord obtenir un nombre $\theta$ d'une distribution gamma de
      mêmes paramètres, puis obtenir un nombre $x$ d'une distribution
      exponentielle de paramètre $\theta$.
<<echo=TRUE, eval=FALSE>>=
rexp(1, rgamma(1, alpha, lambda))
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  La fonction de densité de probabilité de la loi de
  Pareto translatée est
  \begin{displaymath}
    f(x) = \frac{\alpha \lambda^\alpha}{x^{\alpha + 1}}, \quad x > \lambda.
  \end{displaymath}
  Simuler trois valeurs d'une telle distribution avec $\alpha = 2$ et
  $\lambda = \nombre{1000}$ à l'aide de la méthode de l'inverse et du
  générateur congruentiel linéaire suivant:
  \begin{displaymath}
    x_n = (65 x_{n-1} + 1) \bmod \nombre{2048}.
  \end{displaymath}
  Utiliser une amorce de $12$.
  \begin{rep}
    $\nombre{\Sexpr{round(1000/sqrt(1 - 0.3813))}}$,
    $\nombre{\Sexpr{round(1000/sqrt(1 - 0.7881))}}$,
    $\nombre{\Sexpr{round(1000/sqrt(1 - 0.2261))}}$
  \end{rep}
  \begin{sol}
    La fonction de répartition de la Pareto translatée$(\alpha,
    \lambda)$ est
    \begin{align*}
      F(x)
      &= \int_\lambda^x
      \frac{\alpha \lambda^\alpha}{y^{\alpha + 1}}\, dy \\
      &=
      \begin{cases}
        0, & x \leq \lambda \\
        1 - \left( \frac{\lambda}{x} \right)^\alpha, & x > \lambda
      \end{cases} \\
      \intertext{et son inverse est}
      F^{-1}(y)
      &=
      \begin{cases}
        \lambda, & y = 0 \\
        \frac{\lambda}{(1 - y)^{1/\alpha}}, & 0 < y < 1.
      \end{cases}
    \end{align*}
    Par conséquent, si $U \sim U(0, 1)$, alors
    \begin{displaymath}
      X = \frac{\lambda}{(1 - U)^{1/\alpha}} \sim
      \text{Pareto translatée}(\alpha, \lambda).
    \end{displaymath}
    Les trois premières valeurs retournées par le générateur
    \begin{displaymath}
      x_n = (65 x_{n-1} + 1) \bmod \nombre{2048}
    \end{displaymath}
    avec une amorce de $12$ sont $781$, $\nombre{1614}$ et $463$. La division
    de ces nombres par $\nombre{2048}$ fournit des nombres dans
    l'intervalle $(0, 1)$:
    \begin{displaymath}
      0,3813 \quad 0,7881 \quad 0,2261.
    \end{displaymath}
    Finalement, les observations de la $\text{Pareto}(2,
    \nombre{1000})$ sont les suivantes.
<<echo=TRUE>>=
1000/sqrt(1 - c(0.3813, 0.7881, 0.2261))
@
    \emph{Remarque:} puisque $1 - U \sim U(0, 1)$ si $U \sim U(0, 1)$,
    les nombres issus de la transformation $\lambda (1 -
    U)^{-1/\alpha}$ seraient tout aussi distribués selon une Pareto
    translatée. Les réponses seraient toutefois différentes.
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:disque}
  Soit $U_1$ et $U_2$ deux variables aléatoires indépendantes
  uniformément distribuées sur l'intervalle $(0, 1)$ et soit la
  transformation
  \begin{align*}
    X_1 &= \sqrt{U_1} \cos(2 \pi U_2) \\
    X_2 &= \sqrt{U_1} \sin(2 \pi U_2).
  \end{align*}
  Démontrer que la distribution conjointe de $X_1$ et $X_2$ est
  uniforme sur le disque de rayon de $1$ centré en $(x_1, x_2) = (0,
  0)$. À quoi ce résultat peut-il servir?
  \begin{sol}
    Nous avons la transformation
    \begin{displaymath}
      \begin{split}
        X_1 &= \sqrt{U_1} \cos(2 \pi U_2) \\
        X_2 &= \sqrt{U_1} \sin(2 \pi U_2)
      \end{split}
      \quad \Leftrightarrow \quad
      \begin{split}
        U_1 &= X_1^2 + X_2^2 \\
        U_2 &= \frac{1}{2\pi} \arctan \frac{X_2}{X_1}.
      \end{split}
    \end{displaymath}
    Cette transformation associe les points de l'espace $\{(u_1, u_2):
    0 < u_1 < 1, 0 < u_2 < 1\}$ à ceux de l'espace $\{(x_1, x_2):
    x_1^2 + x_2^2 < 1 \backslash (0, 0)\}$. Cela se vérifie aisément
    en examinant les limites de l'espace de départ:
    \begin{align*}
      u_1 &> 0 &\Rightarrow&& x_1^2 + x_2^2 &> 0 \\
      u_1 &< 1 &\Rightarrow&& x_1^2 + x_2^2 &< 1 \\
      u_2 &> 0 &\Rightarrow&& \frac{x_2}{x_1} &> 0 \\
      u_2 &< 1 &\Rightarrow&& \frac{x_2}{x_1} &< 0.
    \end{align*}
    Les troisième et quatrième inégalités définissent les quadrants I
    et III, puis II et IV de $\R^2$, respectivement. Vous remarquerez
    que le point $(0, 0)$, qui a probabilité zéro, ne se trouve pas
    dans l'espace image.

    Le Jacobien de la transformation est
    \begin{align*}
      J
      &=
      \begin{vmatrix}
        \dfrac{\partial u_1}{\partial x_1} &
        \dfrac{\partial u_1}{\partial x_2} \\[8pt]
        \dfrac{\partial u_2}{\partial x_1} &
        \dfrac{\partial u_2}{\partial x_2}
      \end{vmatrix} \\
      &=
      \begin{vmatrix}
        2 x_1 &
        2 x_2 \\[6pt]
        -\dfrac{1}{2\pi} \dfrac{x_2}{x_1^2 + x_2^2} &
        \dfrac{1}{2\pi} \dfrac{x_1}{x_1^2 + x_2^2},
      \end{vmatrix} \\
      &=
      \frac{1}{\pi}.
    \end{align*}
    La fonction de densité de probabilité conjointe de $X_1$ et $X_2$
    est donc
    \begin{align*}
      f_{X_1,X_2}(x_1, x_2)
      &= f_{U_1, U_2}(u_1, u_2) |J| \\
      &= \frac{1}{\pi}, \quad -1 < x_1 < 1, -\sqrt{1 - x_1^2} < x_2 <
      \sqrt{1 - x_1^2},
    \end{align*}
    soit une distribution uniforme sur le disque unité.

    Le résultat peut évidemment servir à simuler des points
    uniformément répartis sur un disque de rayon $1$ centré en $(0,
    0)$. La \autoref{fig:simulation:disque2} illustre d'ailleurs cette
    transformation. Les points $(u_1, u_2)$ dans le graphique de gauche
    sont tirés aléatoirement sur le carré $(0, 1) \times (0, 1)$. Le
    graphique de droite montre que suite à la transformation ci-dessus,
    les points $(x_1, x_2)$ sont distribués uniformément sur un
    disque de rayon $1$ centré en $(0, 0)$.
    \begin{figure}
      \centering
<<echo=FALSE, fig=TRUE, width=10, height=5>>=
col <- rgb(0.67, 0.85, 0.90, alpha = 0.4) # "lightblue" semi-transparent
par(mfrow = c(1, 2))
u1 <- runif(1000)
u2 <- runif(1000)
x1 <- sqrt(u1) * cos(2 * pi * u2)
x2 <- sqrt(u1) * sin(2 * pi * u2)
plot(u1, u2, pch = 19, col = col)
plot(x1, x2, pch = 19, col = col)
@
      \caption{Démonstration graphique du fonctionnement de la
        transformation de l'\autoref{ex:simulation:disque}}
      \label{fig:simulation:disque2}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:gamma-beta}
  \begin{enumerate}
  \item Soit $Y_1 \sim \text{Gamma}(\alpha, 1)$ et $Y_2 \sim
    \text{Gamma}(\beta, 1)$ deux variables aléatoires
    indépendantes. Démontrer que
    \begin{displaymath}
      X = \frac{Y_1}{Y_1 + Y_2} \sim \text{Bêta}(\alpha, \beta).
    \end{displaymath}
  \item Utiliser le résultat en a) pour proposer un algorithme de
    simulation d'observations d'une loi Bêta$(\alpha, \lambda)$.
  \item Faire la mise en œuvre informatique de l'algorithme en b)
    ainsi que les vérifications d'usage.
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Nous avons
      \begin{align*}
        f_{Y_1}(y_1)
        &= \frac{1}{\Gamma(\alpha)}\, y_1^{\alpha - 1} e^{-y_1}, \quad
        y_1 > 0, \\
        f_{Y_2}(y_2)
        &= \frac{1}{\Gamma(\beta)}\, y_2^{\beta - 1} e^{-y_2}, \quad
        y_2 > 0 \\
        \intertext{et}
        f_{Y_1, Y_2}(y_1, y_2)
        &= \frac{1}{\Gamma(\alpha) \Gamma(\beta)}\,
        y_1^{\alpha - 1} y_2^{\beta - 1} e^{-(y_1 + y_2)}, \quad
        y_1 > 0, y_2 > 0.
      \end{align*}
      Soit $X_1 = Y_1/(Y_1 + Y_2)$ et $X_2 = Y_1 + Y_2$ (le choix de
      $X_2$ étant justifié par l'exposant de la distribution conjointe
      de $Y_1$ et $Y_2$). Nous cherchons la distribution conjointe de
      $X_1$ et $X_2$, $f_{X_1, X_2}(x_1, x_2)$ à partir de la transformation
      \begin{displaymath}
        \begin{split}
          x_1 &= \frac{y_1}{y_1 + y_2} \\
          x_2 &= y_1 + y_2
        \end{split}
        \quad \Leftrightarrow \quad
        \begin{split}
          y_1 &= x_1 x_2 \\
          y_2 &= x_2 - x_1 x_2.
        \end{split}
      \end{displaymath}
      Cette transformation associe de manière évidente les points de
      l'espace $\{(y_1, y_2): y_1 > 0, y_2 > 0\}$ à ceux de l'espace
      $\{(x_1, x_2): 0 < x_1 < 1, x_2 > 0\}$.

      Le Jacobien de la transformation est
      \begin{align*}
        J
        &=
        \begin{vmatrix}
          \dfrac{\partial y_1}{\partial x_1} &
          \dfrac{\partial y_1}{\partial x_2} \\[8pt]
          \dfrac{\partial y_2}{\partial x_1} &
          \dfrac{\partial y_2}{\partial x_2}
        \end{vmatrix} \\
        &=
        \begin{vmatrix}
          x_2 & x_1 \\
          -x_2 & 1 - x_1
        \end{vmatrix} \\
        &=
        x_2.
      \end{align*}
      La fonction de densité de probabilité conjointe de $X_1$ et $X_2$
      est donc
      \begin{align*}
        f_{X_1,X_2}&(x_1, x_2)
         = f_{Y_1, Y_2}(y_1, y_2) |J| \\
        &= \frac{1}{\Gamma(\alpha) \Gamma(\beta)}\,
        x_1^{\alpha - 1} (1 - x_1)^{\beta - 1}
        x_2^{\alpha + \beta - 1} e^{-x_2} \\
        &=
        \left[
          \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}\,
          x_1^{\alpha - 1} (1 - x_1)^{\beta - 1}
        \right]
        \left[
          \frac{1}{\Gamma(\alpha + \beta)}\,
          x_2^{\alpha + \beta - 1} e^{-x_2}
        \right],
      \end{align*}
      pour $0 < x_1 < 1$, $x_2 > 0$, d'où $X_1$ et $X_2$ sont
      indépendantes, $X_1 \sim \text{Bêta}(\alpha, \beta)$ et $X_2
      \sim \text{Gamma}(\alpha + \beta)$ (un résultat connu).
    \item La conversion du résultat en un algorithme est très simple:
      \begin{quote}
        \begin{enumerate}[1.]
        \item Générer $y_1$ d'une distribution Gamma$(\alpha, 1)$.
        \item Générer $y_2$ d'une distribution Gamma$(\beta, 1)$.
        \item Poser $x = y_1/(y_1 + y_2)$.
        \end{enumerate}
      \end{quote}
      Cet algorithme suppose évidemment qu'une source de nombres
      provenant d'une loi gamma est disponible.

      La \autoref{fig:simulation:gamma-beta} illustre le fonctionnement de
      cette transformation. Dans le graphique de gauche, nous avons un nuage
      de points $(y_1, y_2)$ tirés de manière indépendante de deux
      distributions gamma de paramètre d'échelle égal à $1$. Nous avons
      superposé ce nuage de points aux courbes de niveaux de la
      distribution conjointe des deux lois gamma.

      Dans le graphique de droite, les points $x = y_1/(y_1 + y_2)$
      résultant de la transformation sont placés en abscisse. Vous
      constaterez que la répartition et la densité de ces points
      correspond à la densité de la loi bêta également représentée sur
      le graphique.
      \begin{figure}
        \centering
<<echo=FALSE, fig=TRUE, width=10, height=5>>=
col <- rgb(0.67, 0.85, 0.90, alpha = 0.4) # "lightblue" semi-transparent
par(mfrow = c(1, 2))
f <- function(x, y) dgamma(x, 2) * dgamma(y, 3)
y <- seq(0, 8, length = 100)
z <- outer(y, y, f)
contour(y, y, z, xlim = c(0, 8), ylim = c(0, 8),
        nlevels = 15, method = "edge",
        xlab = expression(y[1]), ylab = expression(y[2]))

y1 <- rgamma(100, 2)
y2 <- rgamma(100, 3)
points(y1, y2, pch = 19, col = col)

curve(dbeta(x, 2, 3), xlim = c(0, 1), lwd = 2)
points(y1/(y1 + y2), rep(0, length(y1)), pch = 19, col = col)
@
        \caption{Démonstration graphique du fonctionnement de la
          transformation de l'\autoref{ex:simulation:gamma-beta}}
        \label{fig:simulation:gamma-beta}
      \end{figure}
    \item En R:
<<echo=TRUE,eval=FALSE>>=
(y <- rgamma(1, alpha, 1))/(y + rgamma(1, beta, 1))
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item Dans la méthode d'acceptation-rejet, un nombre $y$ tiré d'une
    variable aléatoire $Y$ avec fonction de densité de probabilité
    $g_Y(\cdot)$ est accepté comme réalisation d'une variable
    aléatoire $X$ avec fonction de densité de probabilité $f_X(\cdot)$ si
    \begin{displaymath}
      U \leq \frac{f_X(y)}{c g_Y(y)},
    \end{displaymath}
    où $U \sim U(0, 1)$. Calculer la probabilité d'accepter une valeur
    lors de toute itération de la méthode d'acceptation-rejet,
    c'est-à-dire
    \begin{displaymath}
      \Pr \left[ U \leq \frac{f_X(Y)}{c g_Y(Y)} \right].
    \end{displaymath}
    \emph{Astuce}: utiliser la loi des probabilités totales en
    conditionnant sur $Y = y$.
  \item Déterminer la distribution du nombre d'essais avant d'accepter
    un nombre $y$ dans la méthode d'acceptation-rejet.
  \item Déterminer le nombre moyen d'essais avant d'accepter un nombre
    $y$ dans la méthode d'acceptation-rejet.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $1/c$
    \item Géométrique$(1/c)$ commençant à $1$
    \item $c$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Nous avons
      \begin{align*}
        \Pr \left[ U \leq \frac{f_X(y)}{c g_Y(y)} \right]
        &= \int_{-\infty}^\infty
        \Pr \left[ U \leq \frac{f_X(y)}{c g_Y(y)}\biggr\rvert Y = y \right] %|
        g_Y(y)\, dy \\
        &= \int_{-\infty}^\infty
        \frac{f_X(y)}{c g_Y(y)}\, g_Y(y)\, dy \displaybreak[0] \\
        &= \frac{1}{c} \int_{-\infty}^\infty f_X(y)\, dy \\
        &= \frac{1}{c}.
      \end{align*}
    \item Les essais étant indépendants, la distribution du nombre
      d'essais avant d'obtenir un succès (accepter un nombre $y$) est
      géométrique de paramètre $1/c$, c'est-à-dire que
      \begin{displaymath}
        \Pr[Z = z] =
        \left( \frac{1}{c} \right)
        \left( 1 - \frac{1}{c} \right)^{z - 1}, \quad z = 1, 2, \dots,
      \end{displaymath}
      où $Z$ représente le nombre d'essais avant d'accepter un nombre.
    \item Nous savons que $\esp{Z} = 1/(1/c) = c$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:mode}
  Soit $X$ une variable aléatoire continue définie sur l'intervalle
  $(a, b)$, où $a$ et $b$ sont des nombres réels. Pour simuler des
  observations de cette variable aléatoire par la méthode
  d'acceptation-rejet, il est toujours possible d'inscrire la fonction de
  densité de probabilité de $X$ dans un rectangle de hauteur $M$, ou
  $M$ est la valeur de la densité au mode de $X$.
  \begin{enumerate}
  \item Énoncer l'algorithme d'acceptation-rejet découlant d'une telle
    procédure.
  \item Calculer l'\emph{efficacité} de l'algorithme en a), soit la
    probabilité d'accepter une valeur lors d'une itération de
    l'algorithme.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
    \item $1/(M (b - a))$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Posons
      \begin{displaymath}
        c g_Y(x) = M, \quad a < x < b,
      \end{displaymath}
      soit $Y \sim U(a, b)$ et $c = M (b - a)$. L'algorithme
      d'acceptation-rejet est donc le suivant:
      \begin{enumerate}[1.]
      \item Simuler deux nombres indépendants $u_1$ et $u_2$ d'une
        loi $U(0, 1)$.
      \item Poser $y = a + (b - a) u_1$.
      \item Si $u_2 \leq f_X(y)/M$, poser $x = y$. Sinon, retourner
        à l'étape 1.
      \end{enumerate}
    \item L'efficacité est
      \begin{displaymath}
        \frac{1}{c} = \frac{1}{M (b - a)}.
      \end{displaymath}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:beta}
  Effectuer les calculs ci-dessous dans le contexte de la simulation
  d'observations d'une loi Bêta$(3, 2)$ à l'aide de la méthode
  d'acceptation-rejet.
  \begin{enumerate}
  \item Calculer l'efficacité de l'algorithme développé à
    l'\autoref{ex:simulation:mode} et à
    l'\autoref{exemple:simulation:beta:1} pour le cas présent.
  \item Calculer l'efficacité de l'algorithme développé dans
    l'\autoref{exemple:simulation:beta:2}, où nous avons déterminé que
    \begin{displaymath}
      f_{X}(x) \leq
      \begin{cases}
        3x, & 0 < x < 0,8 \\
        12 - 12 x, & 0,8 \leq x < 1.
      \end{cases}
    \end{displaymath}
  \item Faire la mise en œuvre informatique de l'algorithme le plus
    efficace entre celui de la partie a) et celui de la partie b).
    Vérifier la fonction en superposant l'histogramme d'un grand
    échantillon obtenu avec cette fonction et la vraie fonction de
    densité de la loi bêta.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $9/16$
    \item $4/5$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il est facile de démontrer que le mode $M$ d'une distribution
      bêta de paramètres $\alpha$ et $\beta$ se trouve en
      \begin{displaymath}
        x = \frac{\alpha - 1}{\alpha + \beta - 2}.
      \end{displaymath}
      Par conséquent, l'efficacité de l'algorithme d'acceptation-rejet
      décrit à l'\autoref{ex:simulation:mode} et consistant à
      borner la densité par un rectangle de hauteur $M$ est
      \begin{align*}
        \frac{1}{M}
        &= \frac{1}{f((\alpha - 1)/(\alpha + \beta - 2))} \\
        &= \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}
        \left(
          \frac{\alpha - 1}{\alpha + \beta - 2}
        \right)^{1 - \alpha}
        \left(
          \frac{\beta - 1}{\alpha + \beta - 2}
        \right)^{1 - \beta}.
      \end{align*}
      Avec $\alpha = 3$ et $\beta = 2$, l'efficacité est $9/16$.
    \item Nous avons obtenu $c = 1,2$ dans
      l'\autoref{exemple:simulation:beta:2}, soit une efficacité de
      $1/c = 5/6$. Cet algorithme est évidemment plus efficace puisque
      la surface de l'enveloppe de la densité bêta est nettement plus
      petite.
    \item Utilisons l'algorithme développé à
      l'\autoref{exemple:simulation:beta:2}. Une première mise en
      œuvre de l'algorithme en R est fournie dans le code
      informatique de la \autoref{sec:simulation:code}. La
      \autoref{fig:simulation:rbeta.ar2} en propose une autre.
      \begin{figure}[t]
        \centering
        \begin{framed}
\begin{lstlisting}
rbeta.ar2 <- function(n)
{
    g <- function(x)
        ifelse(x < 0.8, 2.5 * x, 10 - 10 * x)
    Ginv <- function(y)
        ifelse(y < 0.8, sqrt(0.8 * y),
               1 - sqrt(0.2 - 0.2 * y))
    x <- numeric(n)
    i <- 0
    while(i < n)
    {
        y <- Ginv(runif(1))
        if(1.2 * g(y) * runif(1) <=
           dbeta(y, shape1 = 3, shape2 = 2))
            x[i <- i + 1] <- y
    }
    x
}
\end{lstlisting}
        \end{framed}
        \caption{Code R de la fonction \code{rbeta.ar2}}
        \label{fig:simulation:rbeta.ar2}
      \end{figure}
      Vous pouvez vérifier l'exactitude la fonction \code{rbeta.ar2}
      avec les expressions suivantes.
<<echo=TRUE, eval=FALSE>>=
x <- rbeta.ar2(10000)
hist(x, prob = TRUE)
curve(dbeta(x, 3, 2), add = TRUE)
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:simulation:rejet}
  La fonction R de la \autoref{fig:simulation:fonction} permet de
  simuler des observations de la distribution Bêta$(\alpha, \beta)$.
  \begin{figure}[t]
    \begin{framed}
\begin{lstlisting}
simul <- function(n, alpha, beta)
{
    xmax <- (alpha - 1)/(alpha + beta - 2)
    M <- dbeta(xmax, alpha, beta)
    x <- numeric(n)
    i <- 0
    repeat
    {
        u <- runif(1)
        if (M * runif(1) <= dbeta(u, alpha, beta))
            x[i <- i + 1] <- u
        if (i == n)
            break
    }
    x
}
\end{lstlisting}
    \end{framed}
    \caption{Fonction de simulation d'une loi Bêta$(\alpha, \beta)$
      pour l'\autoref{ex:simulation:rejet}}
    \label{fig:simulation:fonction}
  \end{figure}
  \begin{enumerate}
  \item Identifier le type d'algorithme utilisé dans cette fonction.
  \item Vous disposez des valeurs suivantes, obtenues dans R.
<<echo=FALSE>>=
op <- options(width = 50, digits = 2)
simul <- function(n, alpha, beta)
{
    ymax <- dbeta((alpha-1)/(alpha + beta - 2), alpha, beta)
    x <- numeric(n)
    i <- 1
    repeat
    {
        u <- runif(1)
        if (ymax * runif(1) <= dbeta(u, alpha, beta))
        {
            x[i] <- u
            i <- i + 1
        }
        if (i > n)
            break
    }
    x
}
@
<<echo=TRUE>>=
set.seed(12345)
runif(10)
@
    Évaluer le résultat des expressions ci-dessous.
<<echo=TRUE, eval=FALSE>>=
set.seed(12345)
simul(2, alpha = 2, beta = 3)
@
  \end{enumerate}
  \begin{rep}
<<echo=FALSE>>=
set.seed(12345)
x <- round(simul(2, alpha = 2, beta = 3), 2)
x <- format(x, dec = ",")
options(op)
@
    \begin{enumerate}[a)]
      \stepcounter{enumii}
    \item $(\Sexpr{x[1]}$, $\Sexpr{x[2]})$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Vous reconnaitrez l'algorithme d'acceptation-rejet de
      l'\autoref{ex:simulation:mode}.
    \item Vous devez simuler deux observations d'une loi Bêta$(2, 3)$
      dont la fonction de densité de probabilité est
      \begin{displaymath}
        f(x) = 12 x (1 - x)^2, \quad 0 < x < 1.
      \end{displaymath}
      Le mode de cette densité se trouve en $x = 1/3$ (voir la
      solution de l'\autoref{ex:simulation:beta}) et la valeur de
      ce mode est $M = f(1/3) = 16/9$. Pour obtenir le résultat de
      l'appel de la fonction \code{simul}, il faut s'assurer
      d'utiliser les nombres uniformes dans le bon ordre. Quatre
      itérations de la boucle \code{repeat} seront nécessaires.
      Voici leurs résultas.
      \begin{enumerate}[1.]
      \item On a $u = 0,72$, puis $(16/9) (0,88) > f(0,72)$, donc $u$
        est rejeté.
      \item On a $u = 0,76$, puis $(16/9) (0,89) > f(0,76)$, donc $u$
        est rejeté.
      \item On a $u = 0,46$, puis $(16/9) (0,17) < f(0,46)$, donc $u$
        est accepté: $x_1 = 0,46$.
      \item On a $u = 0,33$, puis $(16/9) (0,51) < f(0,33)$, donc $u$
        est accepté: $x_2 = 0,33$.
      \end{enumerate}
      Le résultat est donc le vecteur $\mat{x} = (0,46, 0,33)$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item Démontrer que, si $0 < \alpha < 1$,
    \begin{displaymath}
      x^{\alpha - 1} e^{-x} \leq
      \begin{cases}
        x^{\alpha - 1}, & 0 \leq x \leq 1 \\
        e^{-x},        & x > 1.
      \end{cases}
    \end{displaymath}
  \item Développer un algorithme d'acceptation-rejet pour simuler des
    observations d'une loi Gamma$(\alpha, 1)$, $0 < \alpha < 1$ à
    partir du résultat en a).
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item Si $0 \leq x \leq 1$, $e^{-1} < e^{-x} < 1$, d'où $x^{\alpha
        - 1} e^{-x} \leq x^{\alpha - 1}$. De même, puisque $0 < \alpha
      < 1$, $x^{\alpha - 1} < 1$ pour $x > 1$, d'où  $x^{\alpha - 1}
      e^{-x} \leq e^{-x}$ pour $x > 1$.
    \item Il faut borner la densité $f_X(x) = x^{\alpha - 1}
      e^{-x}/\Gamma(\alpha)$, $x > 0$ et $0 < \alpha < 1$. Du résultat
      en a):
      \begin{displaymath}
        f_X(x) \leq
        \begin{cases}
          x^{\alpha - 1}/\Gamma(\alpha), & 0 \leq x \leq 1 \\
          e^{-x}/\Gamma(\alpha),        & x > 1.
        \end{cases}
      \end{displaymath}
      Posons
      \begin{displaymath}
        c g_Y(x) =
        \begin{cases}
          x^{\alpha - 1}/\Gamma(\alpha), & 0 \leq x \leq 1 \\
          e^{-x}/\Gamma(\alpha),        & x > 1.
        \end{cases}
      \end{displaymath}
      L'aire totale sous la fonction $c g_Y(x)$ est
      \begin{displaymath}
        \int_0^1 \frac{x^{\alpha - 1}}{\Gamma(\alpha)}\, dx +
        \int_1^\infty \frac{e^{-x}}{\Gamma(\alpha)}\, dx =
        \frac{1}{\Gamma(\alpha)}
        \left(
          \frac{1}{\alpha} + \frac{1}{e}
        \right),
      \end{displaymath}
      d'où
      \begin{align*}
        g_Y(x)
        &=
        \begin{cases}
          \dfrac{x^{\alpha - 1}}{(1/\alpha) + (1/e)}, & 0 \leq x \leq 1 \\
          \dfrac{e^{-x}}{(1/\alpha) + (1/e)}, & x > 1,
        \end{cases} \\
        G_Y(x)
        &=
        \begin{cases}
          \dfrac{e}{\alpha + e}\, x^\alpha, & 0 \leq x \leq 1 \\
          1 - \dfrac{e^{-x}}{(1/\alpha) + (1/e)}, & x > 1,
        \end{cases} \\
        \intertext{et}
        G_Y^{-1}(x)
        &=
        \begin{cases}
          \left( \dfrac{\alpha + e}{e}\, x \right)^{1/\alpha},
          & 0 \leq x \leq e/(\alpha + e) \\
          - \ln [((1/\alpha) + (1/e))(1 - x)],
          & e/(\alpha + e) < x \leq 1.
        \end{cases}
      \end{align*}
      Or,
      \begin{displaymath}
        \frac{f_X(x)}{c g_Y(x)} =
        \begin{cases}
          e^{-x},        & 0 \leq x \leq 1 \\
          x^{\alpha - 1}, & x > 1.
        \end{cases}
      \end{displaymath}
      Il découle de ce précède l'algorithme de simulation suivant.
      \begin{enumerate}[1.]
      \item Simuler deux nombres $u_1$ et $u_2$ d'une $U(0, 1)$.
      \item Poser $y = G_Y^{-1}(u_1)$.
      \item Si
        \begin{displaymath}
          u_2 \leq
          \begin{cases}
            e^{-y},        & 0 \leq y \leq 1 \\
            y^{\alpha - 1}, & y > 1,
          \end{cases}
        \end{displaymath}
        alors poser $x = y$. Sinon, retourner à l'étape 1.
      \end{enumerate}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit l'inégalité suivante, valide pour $\alpha \geq 1$:
  \begin{displaymath}
    x^{\alpha - 1} e^{-x}
    \leq \alpha^{\alpha - 1} e^{-x/\alpha + 1 -  \alpha}, \quad x > 0.
  \end{displaymath}
  Utiliser cette inégalité pour justifier l'algorithme
  d'acceptation-rejet ci-dessous servant à simuler des observations d'une loi
  Gamma$(\alpha, 1)$ avec $\alpha \geq 1$.
  \begin{enumerate}[1.]
  \item Simuler deux observations indépendantes $v_1$ et $v_2$ d'une
    loi exponentielle de moyenne $1$.
  \item Si $v_2 < (\alpha - 1)(v_1 - \ln v_1 - 1)$, poser $x =
    \alpha v_1$. Sinon, retourner à l'étape 1.
  \end{enumerate}
  \begin{sol}
    Vous devez simuler des observations de la fonction de densité de
    probabilité $f_X(x) = x^{\alpha - 1} e^{-x}/\Gamma(\alpha)$ avec
    $\alpha \geq 1$. Or, vous savez que
    \begin{displaymath}
      f_X(x) \leq \frac{\alpha^\alpha}{\Gamma(\alpha)}\,
      e^{1 - \alpha}\,
      \frac{1}{\alpha}\, e^{-x/\alpha}, \quad x > 0,
    \end{displaymath}
    d'où $f_X(x) \leq c g_Y(x)$ avec
    \begin{align*}
      c
      &= \frac{\alpha^\alpha}{\Gamma(\alpha)}\, e^{1 - \alpha} \\
      \intertext{et}
      g_Y(x)
      &= \frac{1}{\alpha}\, e^{-x/\alpha}.
    \end{align*}
    Ainsi, $Y \sim \text{Exponentielle}(1/\alpha)$. Soit $y$ une
    observation de la variable aléatoire $Y$ et $u$ une observation
    d'une loi $U(0, 1)$. Selon l'algorithme d'acceptation-rejet, on
    accepte la valeur $y$ comme observation d'une loi Gamma$(\alpha,
    1)$ avec $\alpha \geq 1$ si
    \begin{gather*}
      u \leq \frac{f_X(y)}{c g_Y(y)} =
      y^{\alpha - 1}\, \frac{e^{-y (1 - 1/\alpha)}}{\alpha^{\alpha -
          1} e^{-(\alpha - 1)}} \\
      \Updownarrow \\
      u^{1/(\alpha - 1)} \leq \left( \frac{y}{\alpha} \right)
      \frac{e^{-y/\alpha}}{e^{-1}} \\
      \Updownarrow \\
      \ln u \leq (\alpha - 1)
      \left[
        \ln \left( \frac{y}{\alpha} \right) - \frac{y}{\alpha} + 1
      \right] \\
      \Updownarrow \\
      - \ln u > (\alpha - 1)
      \left[
        \frac{y}{\alpha} - \ln \left( \frac{y}{\alpha} \right) - 1
      \right].
    \end{gather*}
    Or, tant la distribution de $-\ln U$ que celle de $Y/\alpha$ est
    une exponentielle de moyenne $1$, d'où l'algorithme donné dans
    l'énoncé.
  \end{sol}
\end{exercice}

\begin{exercice}
  Dans la mise en œuvre de
  l'\autoref{algo:simulation:melange-discret} de simulation d'un
  mélange discret présentée à la \autoref{sec:simulation:code}, nous
  générons un nombre d'une distribution binomiale pour déterminer la
  quantité de nombres à simuler de la première distribution du mélange
  (et, conséquemment, de celle de la seconde distribution).
  Généraliser le concept pour simuler des observations d'un mélange
  discret de $k > 2$ distributions. \emph{Astuce}: consulter le
  code source de la fonction \code{rmixture}.
  \begin{sol}
    Il faut déterminer le nombre d'observations provenant de chacune
    des $k$ distributions $F_1, \dots, F_k$ sachant que la probabilité
    qu'une observation provienne de la distribution $j$ est $p_j$. La
    distribution conjointe du nombre d'observations de chaque
    distribution est une multinomiale de paramètres $p_1, \dots, p_k$
    dont la fonction de masse de probabilité est:
    \begin{equation*}
      \Pr[X_1 = x_1, ... , X_k = x_k] =
      \frac{n!}{x_1! \cdots x_k!}
      \prod_{j = 1 , ..., k} p_j^{x_j},
    \end{equation*}
    où $n = x_1 + \dots + x_k$. La fonction \code{rmultinom} de R
    permet de simuler des valeurs $x_1, \dots, x_k$ pour $n$ et $p_1,
    \dots, p_k$ données. C'est la stratégie employée par la fonction
    \code{rmixture} de \pkg{actuar}.
  \end{sol}
\end{exercice}

\begin{exercice}
  Écrire des expressions R pour simuler des observations des modèles
  ci-dessous d'abord sans, puis avec, les fonctions dédiées du
  paquetage \pkg{actuar}, le cas échéant.
  \begin{enumerate}
  \item Variable aléatoire dont la fonction de densité est
    $f(x) = 0,3 f_1(x) + 0,7 f_2(x)$, où $f_1$ est la densité d'une
    distribution exponentielle de moyenne $1/2$ et $f_2$ est la
    densité d'une distribution gamma de paramètre $\alpha = 2$ et de
    moyenne $6$.
  \item Mélange $f(x) = 0,5647 I\{x = 0\} + 0,4353 (2^x e^{-2}/x!)$,
    où $I\{\mathcal{A}\}$ est une fonction indicatrice qui vaut $1$
    lorsque la condition $\mathcal{A}$ est vraie et $0$ sinon.
  \item Variable aléatoire $X$ définie ainsi:
    $X|\Theta = \theta \sim \text{Poisson}(\theta)$,
    $\Theta \sim \text{Inverse gaussienne}(2, 1)$. La fonction de
    densité de probabilité de la distribution inverse gaussienne de
    moyenne $\mu$ et de dispersion $\phi$ est
    \begin{equation*}
      f(x) = \left( \frac{1}{2 \pi \phi x^3} \right)^{1/2}
      \exp\left( -\frac{(x - \mu)^2}{2 \mu^2 \phi x} \right), \quad
      x \ge 0, \mu > 0, \phi > 0.
    \end{equation*}
    \emph{Astuce}: la fonction \code{rinvgauss} du paquetage
    \pkg{actuar} permet de simuler des observations de la distribution
    inverse gaussienne.
  \item Dsitribution composée $S = X_1 + \dots + X_N$ avec
    $N \sim \text{Poisson}(2)$ et $X \sim \text{Log-normale}(7, 1)$.
  \item Binomiale négative composée où les paramètres de la binomiale
    négative sont $r = 2$ et $p = 0,3$, et la distribution de la
    variable aléatoire $X$ est une log-normale de paramètres
    $(7, 1)$.
  \end{enumerate}
  \begin{sol}
    Dans chacun des cas ci-dessous, nous considérons que l'objet
    \code{n} contient le nombre de valeurs à simuler.
    \begin{enumerate}
    \item Il s'agit de simuler des observations d'un mélange discret
      d'une Exponentielle$(2)$ et d'une Gamma$(2, 1/3)$. Nous pouvons
      procéder en suivant la procédure explicitée à la
      \autoref{sec:simulation:exercices} ou avec la fonction
      \code{rmixture} du paquetage \pkg{actuar}.
<<echo=TRUE, eval=FALSE>>=
n1 <- rbinom(1, n, 0.3)
c(rexp(n1, 2), rgamma(n - n1, 2, scale = 3))
@
<<echo=TRUE, eval=FALSE>>=
rmixture(n, c(0.3, 0.7),
         expression(rexp(2), rgamma(2, scale = 3)))
@
    \item Le mélange discret est, ici, celui d'une distribution dégénérée
      en $x = 0$ et d'une Poisson de paramètre $\lambda = 2$. La
      simulation «manuelle» est en tous points similaire à ce que nous
      avons fait en a). Cependant, à moins de créer une fonction de
      «simulation» d'une distribution dégénérée, nous ne pouvons
      utiliser directement la fonction \code{rmixture}.
<<echo=TRUE, eval=FALSE>>=
n1 <- rbinom(1, n, 0.5647)
c(rep(0, n1), rpois(n - n1, 2))
@
      Le paquetage \pkg{actuar} fournit néanmoins une fonction pour
      simuler des observations d'un tel mélange. En effet, celui-ci
      est simplement une distribution de Poisson avec une probabilité
      en $x = 0$ de $p_0 = 0,5647 + 0,4353 e^{-2} = 0,6236$, une
      distribution autrement connue sous le nom de Poisson
      zéro modifiée. Les fonctions avec racine \meta{zmpois} de
      \pkg{actuar} permettent de travailler avec cette distribution.
<<echo=TRUE, eval=FALSE>>=
rzmpois(n, 2, p0 = 0.6236)
@
    \item Nous devons simuler des observations de la distribution
      Poisson-inverse gaussienne qui, par ailleurs, est définie par le
      mélange continu présenté dans la question. Nous pouvons procéder
      «manuellement» à l'aide de la fonction \code{rinvgauss} du
      paquetage \pkg{actuar}, ou encore plus directement avec la
      fonction \code{rpoisinvgauss} du même paquetage.
<<echo=TRUE, eval=FALSE>>=
rpois(n, rinvgauss(n, 2, 1))
@
<<echo=TRUE, eval=FALSE>>=
rpoisinvgauss(n, 2, 1)
@
    \item Nous devons simuler des observations d'une distribution
      Poisson composée. Le paquetage \pkg{actuar} fournit la fonction
      \code{rcomppois} pour ce faire.
<<echo=TRUE, eval=FALSE>>=
sapply(rpois(n, 2),
       function(n) sum(rlnorm(n, 7, 1)))
@
<<echo=TRUE, eval=FALSE>>=
rcomppois(n, 2, rlnorm(7, 1))
@
    \item Il faut cette fois simuler des observations d'une distribution
      binomiale négative composée. Le paquetage \pkg{actuar} fournit
      la fonction \code{rcompound} pour ce cas général.
<<echo=TRUE, eval=FALSE>>=
sapply(rnbinom(n, 2, 0.3),
       function(n) sum(rlnorm(n, 7, 1)))
@
<<echo=TRUE, eval=FALSE>>=
rcompound(n, rnbinom(2, 0.3), rlnorm(7, 1))
@
    \end{enumerate}
  \end{sol}


\end{exercice}

\Closesolutionfile{reponses}
\Closesolutionfile{solutions}

\input{reponses-simulation}

%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "methodes-numeriques-en-actuariat_simulation"
%%% coding: utf-8
%%% End:
