%%% Copyright (C) 2018 Vincent Goulet
%%%
%%% Ce fichier fait partie du projet
%%% «Méthodes numériques en actuariat avec R»
%%% http://github.com/vigou3/methodes-numeriques-en-actuariat
%%%
%%% Cette création est mise à disposition selon le contrat
%%% Attribution-Partage dans les mêmes conditions 4.0
%%% International de Creative Commons.
%%% http://creativecommons.org/licenses/by-sa/4.0/

\chapter{Intégration Monte Carlo}
\label{chap:montecarlo}

<<echo=FALSE>>=
options(width = 52)
@

\begin{objectifs}
\item Définir le principe du calcul de la valeur d'une intégrale
  définie à l'aide de nombres aléatoires.
\item Calculer la valeur d'une intégrale définie par la méthode Monte
  Carlo.
\end{objectifs}

On appelle simulation Monte Carlo (ou méthode de Monte Carlo) toute
méthode consistant à résoudre une expression mathématique à l'aide de
nombres aléatoires. Par extension, l'appellation est souvent utilisée
pour référer à toute utilisation de la simulation.

L'une des utilisations de la simulation Monte Carlo la plus répandue
est le calcul d'intégrales, principalement pour des dimensions
supérieures à un. Nous ne présentons ici que l'idée générale.

\begin{prob-enonce}
  L'intégrale de Gauss
  \begin{equation*}
    \int e^{-x^2}\, dx
  \end{equation*}
  intervient dans plusieurs disciplines des mathématiques. En
  probabilités, elle est notamment à la base de la définition de la
  loi normale, ou loi gaussienne. Sans primitive, elle sert souvent,
  en calcul intégral, comme exemple d'intégrale définie qu'il faut
  résoudre par la transformation du domaine en coordonnées polaires.

  Nous savons que
  \begin{equation*}
    G = \int_{-\infty}^{\infty} e^{-x^2}\, dx = \sqrt{\pi}.
  \end{equation*}
  Comment pourrions-nous vérifier ce résultat à l'aide de la
  simulation?
\end{prob-enonce}


\section{Contexte}
\label{sec:montecarlo:contexte}

Nous souhaitons calculer l'intégrale
\begin{equation}
  \label{eq:montecarlo:int:depart}
  \theta = \int_a^b h(x)\, dx,
\end{equation}
mais celle-ci est trop compliquée pour être évaluée explicitement.
Nous pourrions d'abord faire deux approximations de l'aire sous la
fonction $h$ à l'aide des sommes de Riemann
\begin{align*}
  R_n^- &= \sum_{k = 0}^{n - 1} h(a + k \Delta x) \Delta x \\
  \intertext{et}
  R_n^+ &= \sum_{k = 1}^n h(a + k \Delta x) \Delta x,
\end{align*}
où $\Delta x = (b - a)/n$ et $n$ est grand. Voir la
\autoref{fig:montecarlo:riemann} pour une illustration de ces deux
aires.

Par la suite, une estimation numérique de l'intégrale serait la
moyenne des deux sommes de Riemann:
\begin{equation*}
  \hat{\theta}_n = \frac{R_n^- + R_n^+}{2}.
\end{equation*}
Cette procédure devient toutefois rapidement compliquée pour les
intégrales multiples à plusieurs dimensions. Il y a éventuellement
beaucoup trop de points à évaluer.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE, width=8, height=4>>=
par(mfrow = c(1, 2), mar = c(5, 4, 2, 2))

curve(dgamma(x, 3, 2), xlim = c(0, 2), lwd = 2, ylab="h(x)")
xx <- seq(0, 2, by = 0.2)
for (i in seq.int(length(xx) - 1))
    polygon(rep(xx[c(i, i + 1)], each = 2),
            c(0, rep(dgamma(xx[i], 3, 2), 2), 0),
            col = "lightblue")
curve(dgamma(x, 3, 2), xlim = c(0, 2), lwd = 2, add = TRUE)

curve(dgamma(x, 3, 2), xlim = c(0, 2), lwd = 2, ylab="h(x)")
xx <- seq(0, 2, by = 0.2)
for (i in seq.int(length(xx) - 1))
    polygon(rep(xx[c(i, i + 1)], each = 2),
            c(0, rep(dgamma(xx[i + 1], 3, 2), 2), 0),
            col = "lightblue")
curve(dgamma(x, 3, 2), xlim = c(0, 2), lwd = 2, add = TRUE)
@
  \caption{Approximation de l'aire sous une fonction par les deux sommes
    de Riemann $R_n^-$ (gauche) et $R_n^+$ (droite)}
  \label{fig:montecarlo:riemann}
\end{figure}


\section{Méthode Monte Carlo}
\label{sec:montecarlo:methode}

L'idée de l'intégration Monte Carlo consiste à évaluer la fonction à
intégrer en des points choisis aléatoirement, puis à s'en remettre à
la Loi des grands nombres pour estimer l'intégrale.

Nous exprimons tout d'abord la fonction $h(x)$ sous la forme
\begin{equation*}
  h(x) = g(x) f(x),
\end{equation*}
où $f(x)$ est une densité sur $(a, b)$. Ainsi, par définition de
l'espérance:
\begin{align*}
  \theta
  &= \int_a^b g(x) f(x)\, dx \\
  &= \esp{g(X)},
\end{align*}
où $X$ est la variable aléatoire avec fonction de densité de
probabilité $f(x)$. Par la suite, si $x_1, \dots, x_n$ sont des
observations simulées de la densité $f(x)$, alors
\begin{equation}
  \label{eq:montecarlo:estimation}
  \hat{\theta}_n = \frac{1}{n} \sum_{i = 1}^n g(x_i)
\end{equation}
constitue une estimation de l'intégrale $\theta$.  Par la Loi des
grands nombres, $\hat{\theta}_n \stackrel{n \rightarrow
  \infty}{\longrightarrow} \theta$.

Par souci de simplicité et parce qu'il est facile d'obtenir un
échantillon aléatoire d'une loi uniforme, nous posons en général
$X \sim U(a, b)$, soit
\begin{equation*}
  f(x) = \frac{1}{b - a}, \quad a < x < b.
\end{equation*}
Dans ce cas, $g(x) = (b - a) h(x)$ et l'intégrale de départ
\eqref{eq:montecarlo:int:depart} se réécrit
\begin{align}
  \label{eq:montecarlo:int:uniforme}
  \theta
  &= (b - a) \int_a^b h(x)\, \frac{1}{b - a}\, dx \\
  \notag
  &= (b - a) \esp{h(X)}.
\end{align}
L'estimation \eqref{eq:montecarlo:estimation} de l'intégrale devient
alors
\begin{equation*}
  \hat{\theta}_n = \frac{b - a}{n} \sum_{i = 1}^n h(x_i),
\end{equation*}
où $x_1, \dots, x_n$ est un échantillon aléatoire d'une loi $U(a, b)$.

Vous remarquerez qu'il est équivalent d'effectuer le changement de
variable
\begin{equation*}
  \begin{split}
    u  &= \frac{x - a}{b - a} \\
    du &= \frac{dx}{b - a}
  \end{split}
  \quad \Leftrightarrow \quad
  \begin{split}
    x  &= a + (b - a) u \\
    dx &= (b - a)\, du
  \end{split}
\end{equation*}
dans l'intégrale~\eqref{eq:montecarlo:int:uniforme}. On obtient alors
\begin{align*}
  \theta
  &= (b - a) \int_0^1 h(a + (b - a)u) (1)\, du \\
  &= (b - a) \esp{h(a + (b - a)U)},
\end{align*}
où $U \sim U(0, 1)$. Une estimation de l'intégrale est donc
\begin{equation*}
  \hat{\theta}_n = \frac{b - a}{n} \sum_{i = 1}^n h(a + (b - a) u_i),
\end{equation*}
où $u_1, \dots, u_n$ est un échantillon aléatoire d'une loi $U(0, 1)$.
Nous devons utiliser la technique du changement de variable lorsque le
domaine est infini.


\begin{prob-astuce}
  La présentation ci-dessus repose sur un domaine d'intégration fini.
  Or, celui de l'intégrale à calculer est infini. Pour utiliser des
  nombres aléatoires uniformes dans la méthode Monte-Carlo, il faut
  ramener le domaine d'intégration infini à un domaine fini par le
  biais d'un changement de variable.

  Tout d'abord, par symétrie de l'intégrande,
  \begin{align*}
    G &= \int_{-\infty}^{\infty} e^{-x^2}\, dx \\
      &= 2 \int_{0}^{\infty} e^{-x^2}\, dx \\
      &= 2
        \left(
        \int_{0}^{1} e^{-x^2}\, dx +
        \int_{1}^{\infty} e^{-x^2}\, dx
        \right).
  \end{align*}

  Ensuite, plusieurs changements de variable sont possibles pour
  transformer le domaine de la seconde intégrale de droite. Nous
  choisissons le changement de variable $u = x^{-1}$. Le domaine
  d'intégration subit donc la transformation
  $(1, \infty) \mapsto (1, 0)$. Ainsi,
  \begin{align*}
    G &= 2
        \left(
        \int_{0}^{1} e^{-x^2}\, dx +
        \int_{1}^{0} e^{-u^{-2}} (-u^{-2})\, du
        \right) \\
      &= 2
        \left(
        \int_{0}^{1} e^{-x^2}\, dx +
        \int_{0}^{1} e^{-u^{-2}} u^{-2}\, du
        \right) \\
      &= 2 \int_{0}^{1} (e^{-x^2} + e^{-x^{-2}} x^{-2})\, dx.
  \end{align*}
  Exprimée ainsi, l'intégrale se prête à la méthode d'intégration
  Monte Carlo.
\end{prob-astuce}


\begin{exemple}
  \label{exemple:montecarlo:2d}
  Soit l'intégrale
  \begin{equation*}
    \theta = \int_2^5 x^{11/5} e^{-x/10}\, dx.
  \end{equation*}
  Parce que l'exposant de $x$ n'est pas un entier, nous ne pouvons
  évaluer cette intégrale par parties. Cependant, nous remarquons que
  la fonction à intégrer est, à une constante près, la densité d'une
  loi gamma:
  \begin{align*}
    \theta
    &= \int_2^5 x^{11/5} e^{-x/10}\, dx \\
    &= \frac{\Gamma\left(\frac{16}{5}\right)}{%
      \left(\frac{1}{10}\right)^{16/5}}
    \int_2^5 \frac{\left(\frac{1}{10}\right)^{16/5}}{%
      \Gamma\left(\frac{16}{5}\right)}\, x^{16/5 - 1} e^{-x/10}\, dx \\
    &= \frac{\Gamma\left(\frac{16}{5}\right)}{%
      \left(\frac{1}{10}\right)^{16/5}}
    \left[
      G \left(5; \frac{16}{5}, \frac{1}{10} \right) -
      G \left(2; \frac{16}{5}, \frac{1}{10} \right)
    \right],
  \end{align*}
  où $G(x; \alpha, \lambda)$ est la fonction de répartition de la loi
  Gamma$(\alpha, \lambda)$. R nous fournit la valeur exacte de
  l'intégrale.
<<echo=TRUE>>=
gamma(3.2) * diff(pgamma(c(2, 5), 3.2, 0.1)) / 0.1^3.2
@

  Pour utiliser la méthode Monte Carlo, nous posons
  \begin{equation*}
    \theta = 3 \int_2^5 x^{11/5} e^{-x/10}
    \left( \frac{1}{3} \right)\, dx.
  \end{equation*}
  Si $\{x_1, \dots, x_n\}$ est un échantillon aléatoire d'une loi
  $U(2, 5)$, alors
  \begin{equation*}
    \hat{\theta}_n = \frac{3}{n} \sum_{i = 1}^n
    x_i^{11/5} e^{-x_i/10}
  \end{equation*}
  est une estimation de $\theta$. Nous obtenons les résultats suivants
  avec R (voir aussi la \autoref{fig:montecarlo:2d}).
<<echo=TRUE>>=
f <- function(x) x^2.2 * exp(-x/10)
x <- runif(1e2, 2, 5)
3 * mean(f(x))
x <- runif(1e3, 2, 5)
3 * mean(f(x))
x <- runif(1e4, 2, 5)
3 * mean(f(x))
x <- runif(1e5, 2, 5)
3 * mean(f(x))
x <- runif(1e6, 2, 5)
3 * mean(f(x))
@
  \begin{figure}[t]
    \centering
<<echo=FALSE, fig=TRUE>>=
par(mfrow = c(2, 2))
f <- function(x) x^2.2 * exp(-x/10)
curve(f(x), xlim = c(2, 5), lwd = 3, main = "Vraie fonction")

x <- runif(1e2, 2, 5)
plot(x, f(x), main = "n = 100",
     pch = 21, bg = "lightblue")
x <- runif(1e3, 2, 5)
plot(x, f(x), main = "n = 1000",
     pch = 21, bg = "lightblue")
x <- runif(1e4, 2, 5)
plot(x, f(x), main = "n = 10 000",
     pch = 21, bg = "lightblue")
@
    \caption{Fonction $h(x)$ de l'\autoref{exemple:montecarlo:2d} et
      points choisis aléatoirement où la fonction est évaluée pour
      l'intégration Monte Carlo}
    \label{fig:montecarlo:2d}
  \end{figure}
  \gotorbox{Le code R pour effectuer les calculs et les graphiques
    ci-dessus est fourni à la \autoref{sec:montecarlo:code}.}%
  \qed
\end{exemple}

\begin{exemple}
  \label{exemple:montecarlo:3d}
  Soit l'intégrale
  \begin{equation*}
    \theta = \int_0^{5/4} \int_0^{5/4} \sqrt{4 - x^2 - y^2}\, dy\, dx.
  \end{equation*}
  La procédure à suivre avec les intégrales multiples est la même
  qu'avec les intégrales simples, sauf que nous tirons des points
  uniformément dans autant de dimensions que nécessaire. Ainsi, dans
  le cas présent, nous posons:
  \begin{align*}
    \theta
    &= \int_0^{5/4} \int_0^{5/4} \sqrt{4 - x^2 - y^2}\, dy\, dx \\
    &= \frac{25}{16} \int_0^{5/4} \int_0^{5/4} \sqrt{4 - x^2 - y^2}
    \frac{1}{\left(\frac{5}{4}\right) \left(\frac{5}{4}\right)}\, dy\, dx \\
    &= \frac{25}{16}\, \Esp{\sqrt{4 - X^2 - Y^2}},
  \end{align*}
  où $X$ et $Y$ sont des variables aléatoires indépendantes
  distribuées uniformément sur l'intervalle $(0, \frac{5}{4})$.
  Autrement dit, la densité conjointe de $X$ et $Y$ est uniforme sur
  $(0, \frac{5}{4}) \times (0, \frac{5}{4})$. Par conséquent, une
  estimation de $\theta$ calculée à partir d'un échantillon $\{(x_1,
  y_1), \dots, (x_n, y_n)\}$ tiré de cette loi conjointe est
  \begin{equation*}
    \hat{\theta}_n = \frac{25}{16 n} \sum_{i = 1}^n \sqrt{4 - x_i^2 - y_i^2}.
  \end{equation*}
  Voici quelques résultats obtenus avec R (voir aussi la
  \autoref{fig:montecarlo:3d}).
<<echo=TRUE>>=
u <- runif(1e2, 0, 1.25)
v <- runif(1e2, 0, 1.25)
mean(sqrt(4 - u^2 - v^2)) * 1.25^2
u <- runif(1e3, 0, 1.25)
v <- runif(1e3, 0, 1.25)
mean(sqrt(4 - u^2 - v^2)) * 1.25^2
u <- runif(1e4, 0, 1.25)
v <- runif(1e4, 0, 1.25)
mean(sqrt(4 - u^2 - v^2)) * 1.25^2
@
  \begin{figure}[t]
    \centering
<<echo=FALSE,fig=TRUE>>=
par(mfrow = c(2, 2), mar = c(1, 1, 2, 1))
f <- function(x, y) sqrt(4 - x^2 - y^2)
x <- seq(0, 1.25, length = 25)
y <- seq(0, 1.25, length = 25)
persp(x, y, outer(x, y, f), main = "Vraie fonction",
      zlim = c(0, 2), theta = 120, phi = 30, col = "lightblue",
      zlab = "z", ticktype = "detailed")
u <- runif(1e2, 0, 1.25)
v <- runif(1e2, 0, 1.25)
res <- persp(x, y, matrix(NA, length(x), length(y)), main = "n = 100",
             zlim = c(0, 2), theta = 120, phi = 30,
             zlab = "z", ticktype = "detailed")
points(trans3d(u, v, f(u, v), pm = res),
       pch = 21, bg = "lightblue")

u <- runif(1e3, 0, 1.25)
v <- runif(1e3, 0, 1.25)
res <- persp(x, y, matrix(NA, length(x), length(y)), main = "n = 1000",
             zlim = c(0, 2), theta = 120, phi = 30,
             zlab = "z", ticktype = "detailed")
points(trans3d(u, v, f(u, v), pm = res),
       pch = 21, bg = "lightblue")

u <- runif(1e4, 0, 1.25)
v <- runif(1e4, 0, 1.25)
res <- persp(x, y, matrix(NA, length(x), length(y)), main = "n = 10 000",
             zlim = c(0, 2), theta = 120, phi = 30,
             zlab = "z", ticktype = "detailed")
points(trans3d(u, v, f(u, v), pm = res),
       pch = 21, bg = "lightblue")
@
    \caption{Fonction $h(x)$ de l'\autoref{exemple:montecarlo:3d} et points choisis
      aléatoirement où la fonction est évaluée pour l'intégration
      Monte Carlo}
    \label{fig:montecarlo:3d}
  \end{figure}
  \gotorbox{Le code R pour effectuer les calculs et les graphiques
    ci-dessus est fourni à la \autoref{sec:montecarlo:code}.}%
  \enlargethispage{5mm}
  \qed
\end{exemple}

\begin{prob-solution}
  En posant
  \begin{align*}
    \theta
    &= 2 \int_{0}^{1} (e^{-x^2} + e^{-x^{-2}} x^{-2})\, dx \\
    &= 2 \esp{e^{-U^2} + e^{-U^{-2}} U^{-2}},
  \end{align*}
  où $U \sim U(0, 1)$, nous avons réécrit l'intégrale de Gauss comme
  l'espérance d'une transformation d'une variable aléatoire uniforme.
  Par la Loi des grands nombres, une bonne estimation de $\theta$ est
  \begin{equation*}
    \hat{\theta}_n =
    \frac{2}{n} \sum_{i=1}^{n} (e^{-u_i^2} + e^{-u_i^{-2}} u_i^{-2})
  \end{equation*}
  pour $n$ grand et où $u_1, \dots, u_n$ sont des nombres aléatoires
  distribués uniformément sur l'intervalle $(0, 1)$.

  Le calcul est ensuite très simple à faire avec R.
<<echo=FALSE>>=
options(digits = 7)
@
<<echo=TRUE>>=
u <- runif(1E5)
2 * (mean(exp(-u^2) + exp(-u^(-2)) * u^(-2)))
@
  Ce résultat est suffisamment près de $\sqrt{\pi} = 1,772454$ pour
  juger l'expérience concluante.
\end{prob-solution}


\section{Code informatique}
\label{sec:montecarlo:code}

\def\scriptfilename{montecarlo.R}

\scriptfile{\scriptfilename}
\lstinputlisting[firstline=14]{\scriptfilename}


\section{Exercices}
\label{sec:montecarlo:exercices}

\Opensolutionfile{reponses}[reponses-montecarlo]
\Opensolutionfile{solutions}[solutions-montecarlo]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref*{chap:montecarlo}}
\addcontentsline{toc}{section}{Chapitre \ref*{chap:montecarlo}}

\end{Filesave}

\begin{exercice}
  Évaluer l'intégrale
  \begin{displaymath}
    \int_0^1 \ln(5x + 4)\, dx
  \end{displaymath}
  exactement ainsi qu'à l'aide de l'intégration Monte Carlo. Comparer
  les réponses.
  \begin{rep}
    Valeur exacte: $1,845969$
  \end{rep}
  \begin{sol}
    On a
    \begin{align*}
      \theta
      &= \int_0^1 \ln(5u + 4)\, dx \\
      &= \esp{\ln(5U + 4)},
    \end{align*}
    où $U \sim U(0, 1)$, ou encore simplement
    \begin{displaymath}
      \theta = \esp{\ln(X)},
    \end{displaymath}
    où $X \sim U(4, 9)$. Une approximation de $\theta$ est
    \begin{displaymath}
      \hat{\theta} = \frac{1}{n} \sum_{i = 1}^n \ln(x_i),
    \end{displaymath}
    où $x_1, \dots, x_n$ est un échantillon aléatoire d'une
    distribution $U(4, 9)$. Voici le résultat d'une évaluation avec R.
<<echo=TRUE>>=
mean(log(runif(1e6, 4, 9)))
@
  \end{sol}
\end{exercice}

\begin{exercice}
  Évaluer l'intégrale
  \begin{displaymath}
    \int_0^1 \int_0^1 e^{2xy} \ln(3x + y^2)\, dx dy
  \end{displaymath}
  à l'aide de l'intégration Monte Carlo. Comparer la réponse obtenue
  avec la vraie valeur, $1,203758$, obtenue à l'aide de Maple.
  \begin{sol}
    Posons
    \begin{align*}
      \theta
      &= \int_0^1 \int_0^1 e^{2xy} \ln(3x + y^2)\, dx dy \\
      &= \esp{e^{2XY} \ln(3X + Y^2)},
    \end{align*}
    où $X \sim U(0, 1)$, $Y \sim U(0, 1)$ et $X$ et $Y$ sont
    indépendantes. Ainsi, leur densité conjointe est uniforme sur $(0,
    1) \times (0, 1)$. Une approximation de $\theta$ est
    \begin{displaymath}
      \hat{\theta} = \frac{1}{n} \sum_{i = 1}^n
      e^{2 x_i y_i} \ln(3x_i + y_i^2)
    \end{displaymath}
    où $x_1, \dots, x_n$ et $y_1, \dots, y_n$ sont deux échantillons
    aléatoires indépendants d'une distribution $U(0, 1)$. Voici le
    résultat d'une évaluation avec R.
<<echo=TRUE>>=
x <- runif(1e6)
y <- runif(1e6)
mean(exp(2 * x * y) * log(3 * x + y^2))
@
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit l'intégrale
  \begin{displaymath}
    \theta = \int_0^\infty x^2 \sin (\pi x) e^{-x/2}\, dx.
  \end{displaymath}
  \begin{enumerate}
  \item Évaluer cette intégrale par Monte Carlo en effectuant un
    changement de variable.
  \item Évaluer cette intégrale par Monte Carlo par échantillonnage
    direct d'une loi de probabilité appropriée.
  \end{enumerate}
  \begin{rep}
    Valeur exacte: $-0,055292$
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Soit le changement de variable $u = e^{-x/2} \Leftrightarrow
      x = - \ln u^2$, d'où $-2 du = e^{-x/2} dx$. On a donc
      \begin{align*}
        \theta
        &= \int_0^1 2 (- \ln u^2)^2 \sin (- \pi \ln u^2)\, du \\
        &= 2 \esp{(- \ln U^2)^2 \sin (- \pi \ln U^2)},
      \end{align*}
      où $U \sim U(0, 1)$. Une estimation de $\theta$ est
      \begin{displaymath}
        \hat{\theta} = \frac{2}{n} \sum_{i = 1}^n
        (- \ln u_i^2)^2 \sin (- \pi \ln u_i^2),
      \end{displaymath}
      où $u_1, \dots, u_n$ est un échantillon aléatoire d'une
      distribution $U(0, 1)$. Voici le résultat d'une évaluation avec R.
<<echo=TRUE>>=
u <- runif(1e6)
2 * mean((-log(u^2))^2 * sin(pi * (-log(u^2))))
@
    \item On remarque que la fonction à intégrer contient, à une
      constante près, la fonction de densité de probabilité d'une loi
      Gamma$(3, 1/2)$. Ainsi,
      \begin{align*}
        \theta
        &= 16 \int_0^\infty \sin (\pi x) \frac{(1/2)^3}{\Gamma(3)}\,
        x^2 e^{-x/2}\, dx \\
        &= 16 \esp{\sin (\pi X)},
      \end{align*}
      où $X \sim \text{Gamma}(3, 1/2)$. Une estimation de $\theta$ est
      donc
      \begin{displaymath}
        \hat{\theta} = \frac{16}{n} \sum_{i = 1}^n \sin(\pi x_i)
      \end{displaymath}
      où $x_1, \dots, x_n$ est un échantillon aléatoire d'une
      Gamma$(3, 1/2)$. Voici le résultat d'une évaluation avec R.
<<echo=TRUE>>=
16 * mean(sin(pi * rgamma(1e6, 3, 0.5)))
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_{25}$ un échantillon aléatoire de taille $25$
  d'une distribution $N(0, 1)$ et soit $X_{(1)} \leq \dots \leq
  X_{(25)}$ les statistiques d'ordre de cet échantillon, c'est-à-dire
  les données de l'échantillon triées en ordre croissant. Estimer
  $\esp{X_{(5)}}$ par intégration Monte Carlo. Comparer la réponse
  obtenue avec la vraie espérance, $-0,90501$.
  \begin{sol}
    Vous pouvez effectuer l'estimation avec R simplement avec les
    expressions suivantes.
<<echo=TRUE>>=
x <- replicate(10000, rnorm(25))
mean(apply(x, 2, function(x) sort(x)[5]))
@
  \end{sol}
\end{exercice}

\Closesolutionfile{reponses}
\Closesolutionfile{solutions}

\input{reponses-montecarlo}

%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "methodes-numeriques-en-actuariat_simulation"
%%% coding: utf-8
%%% End:
