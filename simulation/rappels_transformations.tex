\chapter{Transformations de variables aléatoires}
\label{chap:rappels_transformations}

Cette annexe porte sur les transformations, ou fonctions, de variables
aléatoires. En termes mathématiques, étant donné la distribution
conjointe des variables aléatoires $X_1, \dots, X_n$, on cherche à
déterminer la fonction de probabilité ou de densité de la variable
aléatoire $Y = u(X_1, \dots, X_n)$.

Voici quelques exemples de transformations fréquemment rencontrées:
\begin{align*}
  Y &= X^2 \\
  Y &= \frac{X - \esp{X}}{\sqrt{\var{X}}} \\
  Y &= \frac{X_1 + \dots + X_n}{n} \\
  Y &= F_X(X).
\end{align*}

Il existe trois techniques principales pour déterminer la distribution
de la transformation $Y$:
\begin{enumerate}
\item la technique de la fonction de répartition;
\item la technique du changement de variable;
\item la technique de la fonction génératrice des moments.
\end{enumerate}


\section{Technique de la fonction de répartition}

C'est la technique la plus simple, mais pas toujours la plus facile
d'emploi. En effet, la fonction de répartition de certaines lois de
probabilité est compliquée, voire n'existe pas sous forme explicite
(penser ici aux lois normale et gamma, par exemple).

L'idée consiste simplement à calculer la fonction de répartition de la
transformation avec
\begin{align*}
  F_Y(y)
  &= \prob{Y \leq y} \\
  &= \prob{u(X_1, \dots, X_n) \leq y},
\end{align*}
puis à calculer la densité (ou la fonction de probabilité) par
différenciation:
\begin{displaymath}
  f_Y(y) = F_Y^\prime(y).
\end{displaymath}

\begin{rem}
  Il importe de noter que le domaine de définition de la
  transformation n'est pas nécessairement le même que celui des
  variables aléatoires de départ.
\end{rem}

\begin{exemple}
  Soit
  \begin{displaymath}
    f_X(x) =
    \begin{cases}
      6x (1 - x), & 0 < x < 1 \\
      0, & \text{ailleurs},
    \end{cases}
  \end{displaymath}
  c'est-à-dire $X \sim \text{Bêta}(2, 2)$. On détermine la densité de
  $Y = X^3$. On a
  \begin{align*}
    F_Y(y)
    &= \prob{X^3 \leq y} \\
    &= \prob{X \leq y^{1/3}} \\
    &= F_X(y^{1/3}) \\
    &= \int_0^{y^{1/3}} 6x(1 - x)\, dx \\
    &= 3y^{2/3} - 2y, \qquad 0 < y < 1, \\
    \intertext{et donc}
    f_Y(y)
    &= F_Y^\prime(y) \\
    &=
    \begin{cases}
      2y^{-1/3} - 2, & 0 < y < 1 \\
      0, & \text{ailleurs}.
    \end{cases}
  \end{align*}
  \qed
\end{exemple}

\begin{exemple}
  Soit $X$ une variable aléatoire continue quelconque avec fonction de
  répartition $F_X(x)$ et $Y = aX + b$, où $a$ et $b$ sont des
  constantes réelles. On a
  \begin{align*}
    F_Y(y)
    &= \prob{a X + b \leq y} \\
    &= F_X\left( \frac{y - b}{a} \right) \\
    \intertext{et, par conséquent,}
    f_Y(y)
    &= \frac{1}{a}\, f_X\left( \frac{y - b}{a} \right).
  \end{align*}
  La transformation $Y = X + b$ représente une \emph{translation} de
  $X$, vers la droite si $b > 0$ et vers la gauche si $b < 0$. En
  assurance, $b$ pourra être interprété comme une franchise.

  La transformation $Y = aX$ n'est quand à elle qu'un \emph{changement
    d'échelle} --- par exemple un changement d'unité monétaire. Si $a
  > 1$ on a une dilatation, alors que le cas où $0 < a < 1$ est une
  contraction. %
  \qed
\end{exemple}

\begin{exemple}
  \label{ex:transformations:val_abs}
  Soit $X$ une variable aléatoire continue quelconque avec densité
  $f_X(x)$. On cherche la densité de $Y = \abs{X}$. Premièrement, il
  convient de remarquer que $Y$ est définie au plus sur les réels
  positifs, même si $X$ est définie sur tout l'axe des réels. Par la
  technique de la fonction de répartition,
  \begin{align*}
    F_Y(y)
    &= \prob{\abs{X} \leq y} \\
    &= \prob{-y < X < y} \\
    &= F_X(y) - F_X(-y) \\
    \intertext{et}
    f_Y(y)
    &=
    \begin{cases}
      f_X(y) + f_X(-y), & y > 0 \\
      0, & \text{ailleurs}.
    \end{cases}
  \end{align*}
  Par exemple, soit $X \sim N(0, 1)$, c'est-à-dire
  \begin{align*}
    f_X(x)
    &= \phi(x) \\
    &= \frac{1}{\sqrt{2\pi}} e^{- \frac{1}{2} x^2}.
  \end{align*}
  La densité de $Y = \abs{X}$ est alors
  \begin{align*}
    f_Y(y)
    &= \phi(y) + \phi(-y) \\
    &= 2 \phi(y) \\
    &= \frac{2}{\sqrt{2\pi}}\, e^{- \frac{1}{2} y^2}, \quad y > 0.
  \end{align*}
  \qed
\end{exemple}

\begin{exemple}
  \label{ex:transformations:convolution}
  Soit $Y = X_1 + X_2$, où $X_1$ et $X_2$ sont deux variables
  aléatoires indépendantes chacune distribuée uniformément sur
  l'intervalle $(0, 1)$. On a donc
  \begin{displaymath}
    f_{X_1 X_2}(x_1, x_2) = f_{X_1}(x_1) = f_{X_2}(x_2) = 1, \quad
    0 < x_1 < 1, 0 < x_2 < 1.
  \end{displaymath}
  Le domaine de définition de $Y$ sera l'intervalle $(0, 2)$. Le
  domaine d'intégration étant un carré, il faut distinguer quatre cas.
  \begin{enumerate}
  \item Si $y \leq 0$, on a clairement $F_Y(y) = 0$.
  \item Si $0 < y < 1$, on a
    \begin{align*}
      F_Y(y)
      &= \int_0^y \int_0^{y - x_2} dx_1\, dx_2 \\
      &= \frac{1}{2}\, y^2.
    \end{align*}
  \item Si $1 < y < 2$, alors
    \begin{align*}
      F_Y(y)
      &= (y - 1)(1) + \int_{y-1}^1 \int_0^{y - x_2} dx_1\, dx_2 \\
      &= 1 - \frac{1}{2} (2 - y)^2.
    \end{align*}
  \item Enfin, si $y \geq 2$, clairement $F_Y(y) = 1$.
  \end{enumerate}
  Par conséquent, on a
  \begin{align*}
    F_Y(y)
    &=
    \begin{cases}
      0, & y \leq 0 \\
      \frac{1}{2}\, y^2, & 0 < y \leq 1 \\
      1 - \frac{1}{2} (2 - y)^2, & 1 < y < 2 \\
      1, & y > 2
    \end{cases}
    \intertext{et}
    f_Y(y)
    &=
    \begin{cases}
      0, & y \leq 0 \\
      y, & 0 < y \leq 1 \\
      2 - y, & 1 < y < 2 \\
      0, & y > 2.
    \end{cases}
  \end{align*}
  \qed
\end{exemple}


\section{Technique du changement de variable univariée}

Cette technique est étroitement liée au changement de variable en
intégration. Il convient toutefois de faire une distinction entre les
cas discret et continu.

\subsection{Cas discret}

On considère la transformation $Y = u(X)$. Dans le cas discret, il
suffit généralement de faire la substitution:
\begin{align*}
  \prob{Y = y}
  &= \prob{u(X) = y} \\
  &= \prob{X = u^{-1}(y)}.
\end{align*}
Les probabilités ne changent donc pas, elles ne sont qu'affectées à
d'autres valeurs.

\begin{exemple}
  Soit la variable aléatoire $X$ avec fonction de probabilité
  \begin{displaymath}
    \prob{X = x} =
    \begin{cases}
      1/16, & x = 0 \\
      4/16, & x = 1 \\
      6/16, & x = 2 \\
      4/16, & x = 3 \\
      1/16, & x = 4.
    \end{cases}
  \end{displaymath}
  On cherche la fonction de probabilité de $Y = (X - 2)^2$. Les
  valeurs possibles de $Y$ sont $y = 0, 1$ et $4$. On a
  \begin{align*}
    \prob{Y = y}
    &= \prob{(X - 2)^2 = y} \\
    &= \prob{X = \pm \sqrt{y} + 2} \\
    \intertext{et donc}
    \prob{Y = 0}  &= \prob{X = 2} = \frac{6}{16} \\
    \prob{Y = 1}  &= \prob{X = 1} + \prob{X = 3}  = \frac{10}{16} \\
    \prob{Y = 4}  &= \prob{X = 0} + \prob{X = 4}  = \frac{2}{16}.
  \end{align*}
  \qed
\end{exemple}

\subsection{Cas continu}

Le cas continu est beaucoup plus délicat. On considère toujours la
transformation $Y = u(X)$ avec les hypothèses suivantes:
\begin{itemize}
\item la fonction $u(\cdot)$ est différentiable;
\item la fonction $u(\cdot)$ est soit croissante, soit décroissante
  sur tout le domaine de $f_X(x)$.
\end{itemize}
Ainsi, l'inverse $u^{-1}(\cdot) = w(\cdot)$ de la fonction $u$ existe
et est différentiable.

\begin{thm}
  \label{thm:transformations:trans_simple}
  Soit $f_X(x)$ la fonction de densité de probabilité en $x$ d'une
  variable aléatoire $X$ et $y = u(x)$ une fonction satisfaisant les
  hypothèses ci-dessus. Alors la densité de la variable aléatoire $Y =
  u(X)$ est
  \begin{align*}
    f_Y(y)
    &= f_X(u^{-1}(y))\, \abs{(u^{-1}(y))^\prime} \\
    &= f_X(w(y))\, \abs{w^\prime(y)},
  \end{align*}
  où $w(y) = u^{-1}(y)$ et en supposant $u^\prime(x) \neq 0$.
\end{thm}
\begin{proof}
  On considère le cas où $u$ est une fonction croissante. Alors
  \begin{align*}
    \prob{a < Y < b}
    &= \prob{u^{-1}(a) < X < u^{-1}(b)} \\
    &= \int_{w(a)}^{w(b)} f_X(x)\, dx \\
    \intertext{puis, avec le changement de variable $y = u(x)
      \Leftrightarrow x = w(y)$ et donc $dx = w^\prime(y)\, dy$}
    \prob{a < Y < b}
    &= \int_a^b f_X(w(y))\, w^\prime(y)\, dy,
  \end{align*}
  d'où $f_Y(y) = f_X(w(y))\, w^\prime(y)$. Si $u$ est décroissante,
  alors
  \begin{align*}
    f_Y(y)
    &= - f_X(w(y))\, w^\prime(y) \\
    &= f_X(w(y))\, |w^\prime(y)|
  \end{align*}
  car $w^\prime(y) < 0$.
\end{proof}

\begin{exemple}
  Soit $Y = -2 \ln(X)$ où $X \sim U(0, 1)$. En premier
  lieu, il importe de spécifier qu'au domaine $(0, 1)$ de la variable
  aléatoire $X$ correspond le domaine $(0, \infty)$ pour la
  transformation $Y$. On a $u(x) = -2 \ln(x)$, d'où $w(y) = u^{-1}(y)
  = e^{-y/2}$ et $w^\prime(y) = -\frac{1}{2} e^{-y/2}$. Par le
  \autoref{thm:transformations:trans_simple}, on obtient
  \begin{align*}
    f_Y(y)
    &= f_X(e^{-y/2})\,
    \left|
      - \frac{1}{2}\, e^{-y/2}
    \right| \\
    &= \frac{1}{2}\, e^{-y/2} \\
    &= \frac{1/2}{\Gamma(1)}\, y^{1 - 1} e^{-y/2}, \quad y > 0,
  \end{align*}
  soit $Y \sim \text{Gamma}(1, 1/2) \equiv \chi^2(2)$.
  \qed
\end{exemple}

\begin{exemple}
  On cherche la distribution de $Y = Z^2$, où $Z \sim
  N(0, 1)$. La variable aléatoire $Z$ étant définie sur
  $\R$, la transformation $y = z^2$ n'est pas bijective. Le truc
  consiste ici à d'abord définir la transformation $X = \abs{Z}$, de
  sorte que $X$ soit définie sur les réels positifs seulement. De
  l'\autoref{ex:transformations:val_abs}, on sait que
  \begin{displaymath}
    f_X(x) = \frac{2}{\sqrt{2\pi}}\, e^{-y^2/2}.
  \end{displaymath}
  Par la suite, on définit $Y = X^2 = \abs{Z}^2 = Z^2$, une
  transformation bijective de $X$ dont le domaine de définition est
  $\R^+$. On a alors $u(x) = x^2$ pour $x > 0$, soit $w(y) = \sqrt{y}$
  et $w^\prime(y) = \frac{1}{2} y^{-1/2}$, d'où
  \begin{align*}
    f_Y(y)
    &= f_X(\sqrt{y})\,
    \left|
      \frac{y^{-1/2}}{2}
    \right| \\
    &= \frac{y^{-1/2}}{\sqrt{2\pi}}\, e^{-y/2} \\
    &= \frac{(\frac{1}{2})^{1/2}}{\Gamma(\frac{1}{2})}\, y^{-1/2}\,
    e^{-y/2}, \quad y > 0,
  \end{align*}
  soit $Y \sim \chi^2(1)$.
  \qed
\end{exemple}


\section{Technique du changement de variable multivariée}

Il s'agit simplement ici de généraliser les concepts étudiés à la
section précédente à des transformations impliquant plusieurs
variables aléatoires.

L'idée reste la même sinon qu'il faut s'assurer que la transformation
compte autant de nouvelles variables que d'anciennes. Ainsi, si
l'on part de la distribution conjointe de deux variables aléatoires
$X_1$ et $X_2$, il faudra trouver la distribution conjointe de deux
nouvelles variables aléatoires $Y_1 = u_1(X_1, X_2)$ et $Y_2 =
u_2(X_1, X_2)$. Plus souvent qu'autrement, seule la distribution de la
variable $Y_1$ est d'intérêt. Il suffit alors de définir $Y_2$ comme
une variable muette, par exemple $Y_2 = X_2$ (les textes anglais
utilisent généralement l'expression \emph{dummy variable}).

\subsection{Cas discret}

Si la transformation $Y_1 = u_1(X_1, X_2)$ et $Y_2 = u_2(X_1, X_2)$
est bijective, alors simplement
\begin{align*}
  \prob{Y_1 = y_1, Y_2 = y_2}
  &= \prob{u_1(X_1, X_2) = y_1, u_2(X_1, X_2) = y_2} \\
  &= \prob{X_1 = w_1(y_1, y_2), X_2 = w_2(y_1, y_2)},
\end{align*}
où
\begin{align*}
  w_1(y_1, y_2) &= u_1^{-1}(x_1, x_2) \\
  w_2(y_1, y_2) &= u_2^{-1}(x_1, x_2).
\end{align*}
Les fonctions de probabilité marginales sont alors obtenues en
sommant:
\begin{align*}
  \prob{Y_1 = y_1} &=
  \sum_{y_2 = -\infty}^\infty \prob{Y_1 = y_1, Y_2 = y_2} \\
  \prob{Y_2 = y_2} &=
  \sum_{y_1 = -\infty}^\infty \prob{Y_1 = y_1, Y_2 = y_2}.
\end{align*}

\begin{exemple}
  \label{ex:transformations:poisson}
  Soit $X_1 \sim \text{Poisson}(\lambda_1)$, $X_2 \sim
  \text{Poisson}(\lambda_2)$ et $X_1$ et $X_2$ sont indépendantes. On
  pose $Y = X_1 + X_2$.

  Le calcul de la distribution de $Y$ requiert une variable muette.
  Définissons $Y_2 = X_2$. On a donc
  \begin{displaymath}
  \begin{aligned}
    Y_1 &= X_1 + X_2 \\
    Y_2 &= X_2
  \end{aligned}
  \qquad \Leftrightarrow \qquad
  \begin{aligned}
    X_1 &= Y_1 - Y_2 \\
    X_2 &= Y_2.
  \end{aligned}
\end{displaymath}
Le domaine de $Y_1$ est donc $0, 1, 2, \dots$, alors que celui de
  $Y_2$ est $0, 1, \dots, Y_1$. Ainsi,
  \begin{align*}
    \prob{Y_1 = y_1, Y_2 = y_2}
    &= \prob{X_1 + X_2 = y_1, X_2 = y_2} \\
    &= \prob{X_1 = y_1 - y_2, X_2 = y_2} \\
    &= \prob{X_1 = y_1 - y_2} \prob{X_2 = y_2} \\
    &= \frac{\lambda_1^{y_1 - y_2} e^{-\lambda_1}}{(y_1 - y_2)!}
    \frac{\lambda_2^{y_2} e^{-\lambda_2}}{y_2!}
    \intertext{et donc}
    \prob{Y_1 = y_1}
    &= \sum_{y_2 = 0}^{y_1} \frac{\lambda_1^{y_1 - y_2} \lambda_2^{y_2}
      e^{-(\lambda_1 + \lambda_2)}}{(y_1 - y_2)!\, y_2!} \\
    &= \frac{e^{-(\lambda_1 + \lambda_2)}}{y_1!}
    \sum_{y_2 = 0}^{y_1} \frac{y_1!}{(y_1 - y_2)!\, y_2!}\,
    \lambda_1^{y_1 - y_2} \lambda_2^{y_2} \\
    &= \frac{e^{-(\lambda_1 + \lambda_2)}}{y_1!}
    \sum_{y_2 = 0}^{y_1} \binom{y_1}{y_2}
    \lambda_1^{y_1 - y_2} \lambda_2^{y_2} \\
    &= \frac{e^{-(\lambda_1 + \lambda_2)}}{y_1!} (\lambda_1 +
    \lambda_2)^{y_1},
  \end{align*}
  soit $Y \sim \text{Poisson}(\lambda_1 + \lambda_2)$.
  \qed
\end{exemple}

\subsection{Cas continu}

On généralise le \autoref{thm:transformations:trans_simple} afin
de trouver la distribution conjointe (et éventuellement les
distributions marginales) de $Y_1 = u_1(X_1, X_2)$ et $Y_2 = u_2(X_1,
X_2)$. On suppose que
\begin{itemize}
\item toutes les premières dérivées partielles de $u_1$ et $u_2$
  existent sur le domaine de $X_1$ et $X_2$;
\item la transformation est bijective.
\end{itemize}
Ces hypothèses garantissent que les fonctions inverses $w_1 =
u_1^{-1}$ et $w_2 = u_2^{-1}$ existent.

\begin{thm}
  \label{thm:transformations:trans_multiple}
  Soit $f_{X_1 X_2}(x_1, x_2)$ la fonction de densité de probabilité
  conjointe en $(x_1, x_2)$ des variables aléatoires $X_1$ et $X_2$ et
  $y_1 = u_1(x_1, x_2)$, $y_2 = u_2(x_1, x_2)$ des fonctions
  satisfaisant les hypothèses ci-dessus. Alors la densité conjointe
  des variables aléatoires $Y_1 = u_1(X_1, X_2)$ et $Y_2 = u_2(X_1,
  X_2)$ est
  \begin{align*}
    f_{Y_1 Y_2}(y_1, y_2)
    &= f_{X_1 X_2}(w_1(y_1, y_2), w_2(y_1, y_2))\, |J|,
  \end{align*}
  où
  \begin{displaymath}
    J =
    \begin{vmatrix}
      \dfrac{\partial x_1}{\partial y_1} &
      \dfrac{\partial x_1}{\partial y_2} \\[12pt]
      \dfrac{\partial x_2}{\partial y_1} &
      \dfrac{\partial x_2}{\partial y_2}
    \end{vmatrix}
  \end{displaymath}
  est appelé le \emph{Jacobien} de la transformation.
\end{thm}

\begin{exemple}
  \label{ex:transformations:gamma}
  Soit les variables aléatoires stochastiquement indépendantes $X_1 \sim
  \text{Gamma}(\alpha, 1)$ et $X_2 \sim \text{Gamma}(\eta, 1)$. On va
  démontrer que les variables aléatoires $Y_1 = X_1 + X_2$ et $Y_2 =
  X_1/(X_1 + X_2)$ sont indépendantes et trouver leur densité
  marginale respective.

  Tout d'abord, on a
  \begin{displaymath}
    f_{X_1 X_2}(x_1, x_2) = \frac{1}{\Gamma(\alpha) \Gamma(\eta)}
    x_1^{\alpha - 1} x_2^{\eta - 1} e^{-x_1 - x_2}, \quad
    x_1 > 0, x_2 > 0.
  \end{displaymath}
  De plus,
  \begin{displaymath}
    \begin{aligned}
      Y_1 &= X_1 + X_2 \\
      Y_2 &= \frac{X_1}{X_1 + X_2}
    \end{aligned}
    \qquad \Leftrightarrow \qquad
    \begin{aligned}
      X_1 &= Y_1 Y_2 \\
      X_2 &= Y_1(1 - Y_2)
    \end{aligned}
  \end{displaymath}
  et donc
  \begin{align*}
    \frac{\partial x_1}{\partial y_1} &= y_2 &
    \frac{\partial x_1}{\partial y_2} &= y_1 \\
    \frac{\partial x_2}{\partial y_1} &= 1 - y_2 &
    \frac{\partial x_2}{\partial y_2} &= - y_1,
  \end{align*}
  d'où le Jacobien de la transformation est
  \begin{displaymath}
    J =
    \begin{vmatrix}
      y_2 & y_1 \\
      1 - y_2 & -y_1
    \end{vmatrix}
    = - y_1.
  \end{displaymath}

  Le domaine de $Y_1$ est $\R^+$ alors que celui de $Y_2$ est limité à
  l'intervalle $(0, 1)$. Par le \autoref{thm:transformations:trans_multiple},
  \begin{align*}
    f_{Y_1 Y_2}(y_1, y_2)
    &= f_{X_1 X_2}(y_1 y_2, y_1(1-y_2))\, \abs{-y_1} \\
    &= \frac{1}{\Gamma(\alpha) \Gamma(\eta)}
    y_1 (y_1 y_2)^{\alpha - 1} (y_1 (1 - y_2))^{\eta - 1} e^{-y_1 y_2
      - y_1(1 - y_2)} \\
    &=  \frac{1}{\Gamma(\alpha) \Gamma(\eta)}
    y_1^{\alpha + \eta - 1} y_2^{\alpha - 1} (1 - y_2)^{\eta - 1}
    e^{-y_1} \\
    &= g(y_1)\, h(y_2)
  \end{align*}
  où $g(\cdot)$ et $h(\cdot)$ sont des fonctions quelconques. Ceci
  démontre que $Y_1$ et $Y_2$ sont indépendantes. De plus,
  \begin{align*}
    f_{Y_1}(y_1)
    &= \int_0^1 f_{Y_1 Y_2}(y_1, y_2)\, dy_2 \\
    &= y_1^{\alpha + \eta - 1} e^{-y_1}
    \int_0^1 \frac{1}{\Gamma(\alpha) \Gamma(\eta)}
    y_2^{\alpha - 1} (1 - y_2)^{\eta - 1}\, dy_2 \\
    &= \frac{1}{\Gamma(\alpha + \eta)}\, y_1^{\alpha + \eta - 1}
    e^{-y_1}, \quad y_1 > 0 \\
    \intertext{et}
    f_{Y_2}(y_1, y_2)
    &= \frac{f_{Y_1 Y_2}(y_1, y_2)}{f_{Y_1}(y_1, y_2)} \\
    &= \frac{\Gamma(\alpha + \eta)}{\Gamma(\alpha) \Gamma(\eta)}\,
    y_2^{\alpha - 1} (1 - y_2)^{\eta - 1}, \quad
    0 < y_2 < 1.
  \end{align*}
  On a donc $Y_1 = X_1 + X_2 \sim \text{Gamma}(\alpha + \eta, 1)$ et
  $Y_2 = X_1/(X_1 + X_2) \sim \text{Bêta}(\alpha, \eta)$. %
  \qed
\end{exemple}


\section{Technique de la fonction génératrice des moments}

Cette technique s'avère tout spécialement puissante pour déterminer la
distribution (marginale) d'une combinaison linéaire de variables
aléatoires indépendantes. La technique repose sur le théorème
suivant.

\begin{thm}
  Soit $X_1, \dots, X_n$ des variables aléatoires indépendantes et $Y
  = X_1 + \dots + X_n$. Alors
  \begin{displaymath}
    M_Y(t) = \prod_{i=1}^n M_{X_i}(t).
  \end{displaymath}
\end{thm}

Il est laissé en exercice de refaire les exemples
\ref{ex:transformations:poisson} et \ref{ex:transformations:gamma}
(distribution de $Y_1$ seulement) à l'aide de la technique de la
fonction génératrice des moments.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "methodes_numeriques-partie_2"
%%% End:
