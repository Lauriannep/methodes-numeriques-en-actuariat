\chapter{Valeurs propres, vecteurs propres et diagonalisation}
\label{chap:valeurspropres}

\begin{objectifs}
\item Connaître les définitions de valeur et vecteur propre d'une
  matrice.
\item Savoir calculer les valeurs propres d'une matrice et les
  vecteurs propres correspondants.
\item Pouvoir déterminer si une matrice est diagonalisable ou non.
\item Savoir diagonaliser une matrice carrée.
\end{objectifs}


Les valeurs et vecteurs propres sont des caractéristiques liées à une
matrice carrée. Ces notions admettent une grande variété
d'interprétations physiques différentes, chacune ancrée dans son
domaine d'application: géométrie, résolution d'équations
différentielles, mécanique quantique, finance, etc.

Dans le cadre de ce cours, l'étude des valeurs et vecteurs propres
demeurera plutôt collée aux définitions de base. Nous ne verrons
qu'une seule application, celle que vous êtes la plus susceptible de
rencontre dans vos études en actuariat, soit la diagonalisation d'une
matrice.


\section{Définitions}
\label{sec:valeurspropres:definitions}

En algèbre vectorielle, le produit par une matrice $\mat{A}$ peut être
vu comme un opérateur permettant d'appliquer une transformation à un
vecteur $\mat{x}$. Par exemple:
\begin{itemize}
\item la multiplication par un scalaire
  \begin{equation*}
    \begin{bmatrix} 3 & 0 \\ 8 & -1 \end{bmatrix}
    \begin{bmatrix} 1 \\ 2 \end{bmatrix} =
    \begin{bmatrix} 3 \\ 6 \end{bmatrix} =
    3 \begin{bmatrix} 1 \\ 2 \end{bmatrix};
  \end{equation*}
\item la rotation d'un angle $\theta$
  \begin{equation*}
    \begin{bmatrix}
      \cos \theta & - \sin \theta \\ \sin \theta & \cos \theta
    \end{bmatrix}
    \begin{bmatrix} x \\ y \end{bmatrix} =
    \begin{bmatrix}
      x \cos \theta - y \sin \theta \\ x \sin \theta + y \cos
      \theta
    \end{bmatrix} =
    \begin{bmatrix} w \\ z \end{bmatrix};
  \end{equation*}
\item la projection sur un autre vecteur
  \begin{equation*}
    \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
    \begin{bmatrix} 2 \\ 1 \end{bmatrix} =
    \begin{bmatrix} 2 \\ 0 \end{bmatrix}.
  \end{equation*}
\end{itemize}
Voir la figure~\ref{fig:valeurspropres:transformations} pour des
illustrations des transformations ci-dessus.

\begin{figure}
  \begin{minipage}{0.33\linewidth}
    \centering
<<echo=FALSE, fig=TRUE>>=
par(mar = rep(1, 4))
x <- c(2, 1.75); y <- 1.75 * x
plot(NA, xlim = c(0, 4), ylim = c(0, 4),
     xlab = "", ylab = "", axes = FALSE)
arrows(0, 0, y[1], y[2], lwd = 4, col = "blue")
text((x[1] + y[1])/2 - 0.1, (x[2] + y[2])/2 + 0.1, labels = "Ax",
     pos = 3, font = 2, cex = 3, col = "blue")
arrows(0, 0, x[1], x[2], lwd = 4, col = "black")
text(x[1]/2, x[2]/2 + 0.1, labels = "x",
     pos = 3, font = 2, cex = 3, col = "black")
@
    \subcaption{produit par un scalaire}
  \end{minipage}
  \begin{minipage}{0.33\linewidth}
    \centering
<<echo=FALSE, fig=TRUE>>=
par(mar = rep(1, 4))
plot(NA, xlim = c(0, 4), ylim = c(0, 4),
     xlab = "", ylab = "", axes = FALSE)
R <- matrix(c(cos(pi/4), sin(pi/4), -sin(pi/4), cos(pi/4)), nrow = 2)
x <- c(3.25, 1)
w <- R %*% x
arrows(0, 0, x[1], x[2], lwd = 4)
arrows(0, 0, w[1], w[2], lwd = 4, col = "blue")
f <- function(x, r) sqrt(r^2 - x^2)
xx <- c(1.5, 1.5/x[1])
ww <- R %*% xx
r <- sqrt(sum(xx^2))
curve(f(x, r), xlim = c(ww[1], xx[1]), add = TRUE, lwd = 2)
text(1.3, 1.2, expression(theta), cex = 3)
text(x[1]/2 + 0.2, x[2]/2, labels = "x",
     pos = 1, font = 2, cex = 3)
text(x[1], x[2], labels = "(x, y)", pos = 4, cex = 3)
text(w[1]/2, w[2]/2 + 0.1, labels = "Ax",
     pos = 2, font = 2, cex = 3, col = "blue")
text(w[1], w[2], labels = "(w, z)", pos = 4, cex = 3, col = "blue")
@
    \subcaption{rotation}
  \end{minipage}
  \begin{minipage}{0.33\linewidth}
    \centering
<<echo=FALSE, fig=TRUE>>=
par(mar = rep(1, 4))
o <- c(0, 0.5)
x <- c(3.5, 3.25); y <- c(x[1], o[2])
plot(NA, xlim = c(0, 4), ylim = c(0, 4),
     xlab = "", ylab = "", axes = FALSE)
arrows(o[1], o[2] , x[1], x[2], lwd = 4, col = "black")
text(x[1]/2, (x[2] + o[2])/2 + 0.1, labels = "x",
     pos = 3, font = 2, cex = 3, col = "black")
arrows(o[1], o[2], y[1], y[2], lwd = 4, col = "blue")
text(y[1]/2, (y[2] + o[2])/2 - 0.1, labels = "Ax",
     pos = 1, font = 2, cex = 3, col = "blue")
len <- 0.25
segments(y[1], y[2], x[1], x[2], lty = 2)
segments(y[1] - len, y[2], y[1] - len, y[2] + len)
segments(y[1] - len, y[2] + len, y[1], y[2] + len)
@
    \subcaption{projection}
  \end{minipage}
  \caption{Illustration de trois transformations d'un vecteur
    $\mat{x}$ que l'on peut représenter par un produit matriciel
    $\mat{A x}$.}
  \label{fig:valeurspropres:transformations}
\end{figure}

Les définitions de valeur propre et de vecteur propre procèdent de
cette interprétation du produit matriciel.

\begin{definition}
  \label{sec:valeurspropres:definition}
  Soit $\mat{A}$ une matrice $n \times n$. Alors le vecteur non nul
  $\mat{x} \in \mathbb{R}^n$ est un \emph{vecteur propre} de $\mat{A}$
  si
  \begin{displaymath}
    \mat{A} \mat{x} = \lambda \mat{x}
  \end{displaymath}
  pour une valeur de $\lambda$ appelée \emph{valeur propre} de
  $\mat{A}$. On dit que $\mat{x}$ est le vecteur propre
  \emph{correspondant} à $\lambda$.
\end{definition}

Ainsi, par définition, lorsqu'un vecteur propre $\mat{x}$ est
multiplié par la matrice $\mat{A}$, il est transformé en un multiple
de lui-même; voir la figure
\begin{figure}
  \begin{minipage}{0.22\linewidth}
<<echo=FALSE, fig=TRUE>>=
par(mar = rep(0, 4))
x <- c(1, 1)
plot(0, 0, xlim = c(0, 1), ylim = c(0, 1), pch = 19, cex = 3,
     xlab = "", ylab = "", axes = FALSE)
arrows(0, 0, x[1], x[2], lwd = 8)
arrows(0, 0, x[1]/2, x[2]/2, lwd = 8, col = "blue")
@
    \subcaption{$0 \leq \lambda \leq 1$}
  \end{minipage}
  \hfill
  \begin{minipage}{0.22\linewidth}
<<echo=FALSE, fig=TRUE>>=
par(mar = rep(0, 4))
x <- c(1, 1)/2
plot(0, 0, xlim = c(0, 1), ylim = c(0, 1), pch = 19, cex = 3,
     xlab = "", ylab = "", axes = FALSE)
arrows(0, 0, 2 * x[1], 2 * x[2], lwd = 8, col = "blue")
arrows(0, 0, x[1], x[2], lwd = 8)
@
    \subcaption{$\lambda \geq 1$}
  \end{minipage}
  \hfill
  \begin{minipage}{0.22\linewidth}
<<echo=FALSE, fig=TRUE>>=
par(mar = rep(0, 4))
x <- c(1, 1)
plot(0.25, 0.25, xlim = c(0, 1), ylim = c(0, 1), pch = 19, cex = 3,
     xlab = "", ylab = "", axes = FALSE)
arrows(0.25, 0.25, x[1], x[2], lwd = 8)
arrows(0.25, 0.25, 0, 0, lwd = 8, col = "blue")
@
    \subcaption{$-1 \leq \lambda \leq 0$}
  \end{minipage}
  \hfill
  \begin{minipage}{0.22\linewidth}
<<echo=FALSE, fig=TRUE>>=
par(mar = rep(0, 4))
x <- c(1, 1)
plot(0.75, 0.75, xlim = c(0, 1), ylim = c(0, 1), pch = 19, cex = 3,
     xlab = "", ylab = "", axes = FALSE)
arrows(0.75, 0.75, x[1], x[2], lwd = 8)
arrows(0.75, 0.75, 0, 0, lwd = 8, col = "blue")
@
    \subcaption{$\lambda \leq -1$}
  \end{minipage}
  \caption{Un vecteur propre $\mat{x}$ d'une matrice $\mat{A}$ (en
    noir) est transformé en un multiple $\lambda$ de lui-même lorsque
    multiplié par $\mat{A}$ (en bleu).}
  \label{fig:valeurspropres:illustration}
\end{figure}

\begin{exemple}
  Le vecteur $\mat{x} = (1, 2)$ est un vecteur propre de
  \begin{displaymath}
    \mat{A} = \begin{bmatrix} 3 & 0 \\ 8 & -1 \end{bmatrix}
  \end{displaymath}
  correspondant à la valeur propre $\lambda = 3$ car $\mat{Ax} = 3
  \mat{x}$. En effet,
  \begin{displaymath}
    \begin{bmatrix} 3 & 0 \\ 8 & -1 \end{bmatrix}
    \begin{bmatrix} 1 \\ 2 \end{bmatrix} =
    3 \begin{bmatrix} 1 \\ 2 \end{bmatrix}.
  \end{displaymath}
  \qed
\end{exemple}

\begin{rem}
  Le qualificatif «propre» doit être pris dans le sens de «qui
  caractérise». En anglais, les termes les plus souvent utilisés pour
  «valeur propre» et «vecteur propre» sont, dans l'ordre,
  \emph{eigenvalue} et \emph{eigenvector}. Ils sont formés à partir du
  préfixe allemand \emph{eigen} plutôt qu'à partir de son équivalent
  anglais \emph{own}, probablement pour des raisons générales
  d'agrément phonétique.
\end{rem}



\section{Valeurs propres}
\label{sec:valeurspropres:valeurs}

Pour trouver les valeurs propres d'une matrice $\mat{A}$, on doit
résoudre
\begin{displaymath}
  \mat{A} \mat{x} = \lambda \mat{x}
\end{displaymath}
ou, de manière équivalente,
\begin{displaymath}
  (\lambda \mat{I} - \mat{A}) \mat{x} = \mat{0}.
\end{displaymath}
pour $\mat{x} \neq \mat{0}$. Par conséquent, $\lambda$ est une valeur
propre de $\mat{A}$ si, et seulement si, il existe une solution autre
que la solution triviale à ce système d'équations homogène.

Or, par le théorème \ref{thm:revision:unification}, il existe une
solution de $(\lambda \mat{I} - \mat{A}) \mat{x} = \mat{0}$ autre que
la solution triviale si, et seulement si, $\det(\lambda \mat{I} -
\mat{A}) = 0$. Par conséquent:
\begin{itemize}
\item les valeurs propres de $\mat{A}$ sont les solutions de
  \begin{displaymath}
    \det(\lambda \mat{I} - \mat{A}) = 0;
  \end{displaymath}
\item cette équation est l'\emph{équation caractéristique} de $\mat{A}$;
\item $\det(\lambda \mat{I} - \mat{A})$ est le \emph{polynôme
    caractéristique} (de degré $n$) de $\mat{A}$.
\end{itemize}

\begin{exemple}
  Soit
  \begin{displaymath}
    \mat{A} =
    \begin{bmatrix}
      0 &   1 & 0 \\
      0 &   0 & 1 \\
      4 & -17 & 8
    \end{bmatrix}.
  \end{displaymath}
  Le polynôme caractéristique de $\mat{A}$ est
  \begin{align*}
    \det(\lambda \mat{I} - \mat{A})
    &=
    \begin{vmatrix}
      \lambda &  -1 & 0 \\
      0 & \lambda & -1 \\
      -4 & 17 & \lambda - 8
    \end{vmatrix} \\
    &= \lambda [\lambda (\lambda - 8) + 17] - 4 \\
    &= \lambda^3 - 8 \lambda^2 + 17 \lambda - 4.
  \end{align*}
  Les valeurs propres sont les racines de ce polynôme, c'est-à-dire
  les solutions de
  \begin{displaymath}
    \lambda^3 - 8 \lambda^2 + 17 \lambda - 4 = 0.
  \end{displaymath}
  Or,
  \begin{displaymath}
    \lambda^3 - 8 \lambda^2 + 17 \lambda - 4 =
    (\lambda - 4)(\lambda^2 - 4 \lambda + 1),
  \end{displaymath}
  d'où les valeurs propres sont
  \begin{align*}
    \lambda_1 &= 4 \\
    \lambda_2 &= 2 + \sqrt{3} \\
    \lambda_3 &= 2 - \sqrt{3}.
  \end{align*}
  \qed
\end{exemple}


\begin{exemple}
  Soit la matrice triangulaire
  \begin{displaymath}
    \mat{A} =
    \begin{bmatrix}
      a_{11} & a_{12} & a_{13} \\
          0  & a_{22} & a_{23} \\
          0  &      0 & a_{33}
    \end{bmatrix}.
  \end{displaymath}
  On a
  \begin{align*}
    \det(\lambda \mat{I} - \mat{A})
    &=
    \begin{vmatrix}
      \lambda - a_{11} & -a_{12} & -a_{13} \\
          0  & \lambda - a_{22} & -a_{23} \\
          0  &      0 & \lambda - a_{33}
    \end{vmatrix} \\
    &= (\lambda - a_{11})(\lambda - a_{22})(\lambda - a_{33}),
  \end{align*}
  d'où $\lambda_1 = a_{11}$, $\lambda_2 = a_{22}$ et $\lambda_3 =
  a_{33}$. %
  \qed
\end{exemple}

Le résultat de l'exemple précédent justifie un théorème.

\begin{thm}
  \label{thm:valeurspropres:triangulaire}
  Si $\mat{A}_{n \times n}$ est une matrice triangulaire (supérieure
  ou inférieure) ou diagonale, alors ses valeurs propres sont les
  éléments de la diagonale.
\end{thm}

De ce théorème, on peut établir les faits suivants pour une matrice
triangulaire:
\begin{enumerate}
\item la trace est égale à la somme des valeurs propres;
\item le déterminant est égal au produit des valeurs propres.
\end{enumerate}
Le théorème suivant confirme que ces résultats tiennent pour toute
matrice carrée.

\begin{thm}
  \label{thm:valeurspropres:trace_et_determinant}
  Soit $\mat{A}_{n \times n}$ une matrice dont les valeurs propres
  sont $\lambda_1, \dots, \lambda_n$ (égalités et valeurs complexes
  comprises). Alors
  \begin{align*}
    \tr(\mat{A}) &= \sum_{i = 1}^n \lambda_i \\
    \det(\mat{A}) &= \prod_{i = 1}^n \lambda_i.
  \end{align*}
\end{thm}
\begin{proof}
  On peut démontrer que le polynôme caractéristique d'une matrice
  carrée $\mat{A}$ est toujours de la forme
  \begin{align*}
    \det(\lambda \mat{I} - \mat{A})
    &= \lambda^n - c_1 \lambda^{n - 1} + \dots + c_n \\
    &= (\lambda - \lambda_1) \cdots (\lambda - \lambda_n),
  \end{align*}
  où
  \begin{align*}
    c_1 &= \lambda_1 + \dots + \lambda_n \\
    c_n &= (-1)^n \lambda_1 \cdots \lambda_n.
  \end{align*}
  (Pensez au cas $n = 2$, soit un polynôme de second degré; vous savez
  que le coefficient du terme de degré un est la somme des racines et
  que le terme constant est le produit des racines.) Il faut
  maintenant établir que $c_1 = \tr(\mat{A})$ et que $c_n =
  \det(\mat{A})$ pour compléter la démonstration. Or, d'une part, on
  vérifie facilement que le coefficient de $\lambda^{n - 1}$ dans le
  polynôme caractéristique provient du produit
  \begin{displaymath}
    (\lambda - a_{11}) \cdots (\lambda - a_{nn}) =
    \lambda^n - (a_{11} + \dots + a_{nn}) \lambda^{n - 1} + \dots
  \end{displaymath}
  dans le calcul du déterminant de $\lambda \mat{I} - \mat{A}$. Par
  conséquent,
  \begin{align*}
    \tr(\mat{A})
    &= \sum_{i = 1}^n a_{ii} \\
    &= c_1 \\
    &= \sum_{i = 1}^n \lambda_i.
  \end{align*}

  D'autre part, en posant $\lambda = 0$ dans le polynôme
  caractéristique, on obtient
  \begin{align*}
    \det(-\mat{A})
    &= (-1)^n \det(\mat{A}) \\
    &= c_n \\
    &= (-1)^n \lambda_1 \cdots \lambda_n, \\
    \intertext{d'où}
    \det(\mat{A})
    &= \prod_{i = 1}^n \lambda_i.
  \end{align*}
\end{proof}

Du théorème précédent, il découle que si une valeur propre d'une
matrice est nulle, alors son déterminant est nul et la matrice est
singulière. Cette observation nous permet d'ajouter un énoncé au
théorème~\ref{thm:revision:unification}.

\begin{thm}
  \label{thm:valeurspropres:unification}
  Soit $\mat{A}$ une matrice $n \times n$. Les énoncés suivants
  sont équivalents.
  \begin{enumerate}[widest=10]
  \item $\mat{A}$ est inversible.
  \item $\det(\mat{A}) \neq 0$
  \item $\mathrm{rang}(\mat{A}) = n$
  \item La seule solution de $\mat{Ax} = \mat{0}$ est la solution
    triviale.
  \item Le système d'équations $\mat{Ax} = \mat{b}$ a une solution
    pour tout vecteur $\mat{b}$ et celle-ci est unique.
  \item $\mat{A}$ peut être exprimée comme un produit de matrices
    élémentaires.
  \item Les vecteurs lignes de $\mat{A}$ sont linéairement
    indépendants.
  \item Les vecteurs colonnes de $\mat{A}$ sont linéairement
    indépendants.
  \item Les vecteurs lignes de $\mat{A}$ forment une base de $\mathbb{R}^n$.
  \item Les vecteurs colonnes de $\mat{A}$ forment une base de $\mathbb{R}^n$.
  \item $\lambda = 0$ n'est pas une valeur propre de $\mat{A}$.
  \end{enumerate}
\end{thm}

Un dernier résultat: on observe que si $\lambda$ est une valeur propre
de $\mat{A}$ et $\mat{x}$ est un vecteur propre correspondant, alors
\begin{displaymath}
  \mat{A}^2 \mat{x} =
  \mat{A}(\mat{A} \mat{x}) =
  \mat{A}(\lambda \mat{x}) =
  \lambda (\mat{A} \mat{x}) =
  \lambda (\lambda \mat{x}) =
  \lambda^2 \mat{x},
\end{displaymath}
d'où $\lambda^2$ est une valeur propre de $\mat{A}^2$ et $\mat{x}$ est
un vecteur propre correspondant.

\begin{thm}
  Si $\lambda$ est une valeur propre de la matrice $\mat{A}$ et
  $\mat{x}$ est un vecteur propre correspondant, alors
  \begin{itemize}
  \item $\lambda^k$ est une valeur propre de $\mat{A}^k$, $k \in
    \mathbb{N}$;
  \item $\mat{x}$ est un vecteur propre correspondant.
  \end{itemize}
\end{thm}
\begin{proof}
  Laissée en exercice.
\end{proof}

\begin{rem}
  L'équation caractéristique $\det(\lambda \mat{I} - \mat{A})$ peut
  aussi fournir cette interprétation de la valeur propre: c'est le
  nombre de fois qu'il faut «déplacer» la matrice $\mat{A}$ de
  $\mat{I}$ jusqu'à ce qu'elle devienne singulière.
\end{rem}



\section{Vecteurs propres}
\label{sec:valeurspropres:vecteurs}

Les vecteurs propres de la matrice $\mat{A}$ correspondant à la valeur
propre $\lambda$ sont les vecteurs non nuls satisfaisant
\begin{displaymath}
  \mat{A} \mat{x} = \lambda \mat{x}
\end{displaymath}
ou, de manière équivalente
\begin{displaymath}
  (\lambda \mat{I} - \mat{A}) \mat{x} = \mat{0}.
\end{displaymath}
Or, puisque $\det(\lambda \mat{I} - \mat{A}) = 0$,
\begin{itemize}
\item le rang de $\lambda \mat{I} - \mat{A}$ est inférieur à $n$;
\item il existe une solution $(\lambda \mat{I} - \mat{A}) \mat{x} =
  \mat{0}$ autre que la solution triviale;
\item le système d'équations homogène comporte une infinité de
  solutions;
\item pour chaque valeur propre $\lambda$, on doit trouver une base de
  $\mathbb{R}^k$, où
  \begin{displaymath}
    k = n - \mathrm{rang}(\lambda \mat{I} - \mat{A}).
  \end{displaymath}
\end{itemize}

\begin{exemple}
  \label{ex:valeurspropres:vecteurs}
  Soit
  \begin{displaymath}
    \mat{A} =
    \begin{bmatrix}
      0 & 0 & -2 \\
      1 & 2 &  1 \\
      1 & 0 &  3
    \end{bmatrix}.
  \end{displaymath}
  L'équation caractéristique de $\mat{A}$ est
  \begin{displaymath}
    (\lambda - 2)^2 (\lambda - 1) = 0,
  \end{displaymath}
  d'où les valeurs propres sont $\lambda_1 = 1$ et $\lambda_2 =
  \lambda_3 = 2$.

  \begin{enumerate}
  \item Cas $\lambda_1 = 1$. On doit trouver $\mat{x} = (x_1, x_2,
    x_3)$ tel que
    \begin{gather*}
      (\mat{I} - \mat{A}) \mat{x} = \mat{0} \\
      \intertext{soit}
      \begin{bmatrix}
         1 &  0 &  2 \\
        -1 & -1 & -1 \\
        -1 &  0 & -2
      \end{bmatrix}
      \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
      \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \\
      \intertext{ou, de manière équivalente,}
      \begin{bmatrix}
        1 &  0 &  2 \\
        0 &  1 & -1 \\
        0 &  0 &  0
      \end{bmatrix}
      \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
      \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}.
    \end{gather*}
    Par conséquent:
    \begin{itemize}
    \item la solution est $x_1 = -2 x_3$, $x_2 = x_3$ et $x_3$ est une
      variable libre;
    \item $(-2s, s, s)$ est un vecteur propre correspondant à $\lambda
      = 1$;
    \item en fait, tout vecteur de la forme $(-2, 1, 1)$ est un
      vecteur propre correspondant à $\lambda = 1$;
    \item $(-2, 1, 1)$ est une base de vecteurs propres correspondant
      à $\lambda = 1$.
    \end{itemize}
  \item Cas $\lambda_2 = \lambda_3 = 2$. On a
    \begin{gather*}
      (2 \mat{I} - \mat{A}) \mat{x} = \mat{0} \\
      \intertext{soit}
      \begin{bmatrix}
         2 &  0 &  2 \\
        -1 &  0 & -1 \\
        -1 &  0 & -1
      \end{bmatrix}
      \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
      \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \\
      \intertext{ou, de manière équivalente,}
      \begin{bmatrix}
        1 &  0 &  1 \\
        0 &  0 &  0 \\
        0 &  0 &  0
      \end{bmatrix}
      \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
      \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}.
    \end{gather*}
    Par conséquent:
    \begin{itemize}
    \item la solution est $x_1 = -x_3$ et $x_2$ et $x_3$ sont des
      variables libres;
    \item $(s, t, -s)$ est un vecteur propre correspondant à $\lambda
      = 2$;
    \item on a
      \begin{displaymath}
        \begin{bmatrix} s \\ t \\ -s  \end{bmatrix} =
        s \begin{bmatrix} 1 \\ 0 \\ -1  \end{bmatrix} +
        t \begin{bmatrix} 0 \\ 1 \\ 0  \end{bmatrix};
      \end{displaymath}
    \item $(1, 0, -1)$ et $(0, 1, 0)$ forment une base de vecteurs
      propres correspondant à $\lambda = 2$.
    \end{itemize}
  \end{enumerate}
  \qed
\end{exemple}

\begin{rem}
  Le nombre de vecteurs propres dans la base est \emph{au plus} la
  multiplicité de $\lambda$. Par conséquent, si toutes les valeurs
  propres sont différentes, alors on a $n$ vecteurs propres
  différents.
\end{rem}



\section{Diagonalisation}
\label{sec:valeurspropres:diagonalisation}

La diagonalisation d'une matrice est la principale application des
valeurs et vecteurs propres qu'un étudiant en actuariat est
susceptible de rencontrer dans ses études. Elle joue un rôle dans la
résolution de systèmes d'équations différentielles ordinaires, ainsi
que dans le calcul de l'exponentielle d'une matrice, tel qu'expliqué
dans l'exemple suivant.

\begin{exemple}
  L'exponentielle d'un nombre réel $x$ est définie comme
  \begin{align*}
    e^x &= 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots \\
    &= \sum_{n = 0}^\infty \frac{x^n}{n!}.
  \end{align*}
  Par analogie, on définit l'exponentielle d'une matrice $\mat{A}$ ainsi:
  \begin{align*}
    e^{\mat{A}} &= I + \mat{A} + \frac{\mat{A}^2}{2!} +
    \frac{\mat{A}^3}{3!} + \dots \\
    &= \sum_{n = 0}^\infty \frac{\mat{A}^n}{n!},
  \end{align*}
  avec $\mat{A}^0 = \mat{I}$ par convention.

  L'évaluation numérique de l'exponentielle d'une matrice de manière
  efficace, fiable et précise est chose difficile. Dans un article
  célèbre, \citet{Moler:dubious:1978} passent en revue pas moins de 19
  manières différentes de le faire, pour conclure{\dots} qu'aucune méthode
  n'est uniformément meilleure que les autres!

  Cela dit, le calcul est simple si $\mat{A}$ est diagonale.
  Supposons, sans perte de généralité, que $\mat{A}$ est une matrice
  $2 \times 2$:
  \begin{equation*}
    \mat{A} =
    \begin{bmatrix}
      a_1 & 0 \\ 0 & a_2
    \end{bmatrix}.
  \end{equation*}
  Alors
  \begin{equation*}
    \mat{A}^n =
    \begin{bmatrix}
      a_1^n & 0 \\ 0 & a_2^n
    \end{bmatrix}
  \end{equation*}
  et donc
  \begin{align*}
    e^{\mat{A}} &= \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
    + \begin{bmatrix} a_1 & 0 \\ 0 & a_2 \end{bmatrix}
    + \frac{1}{2!} \begin{bmatrix} a_1^2 & 0 \\ 0 & a_2^2 \end{bmatrix}
    + \frac{1}{3!} \begin{bmatrix} a_1^3 & 0 \\ 0 & a_2^3 \end{bmatrix}
    + \dots \\
    &=
    \begin{bmatrix}
      1 + a_1 + \frac{a_1^2}{2!} + \frac{a_1^3}{3!} a_1^n + \dots & 0 \\
      0 & 1 + a_1 + \frac{a_2^2}{2!} + \frac{a_2^3}{3!} a_2^n + \dots
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      e^{a_1} & 0 \\
      0 & e^{a_2}
    \end{bmatrix}.
  \end{align*}

  Supposons maintenant que $\mat{B}$ est une matrice que l'on peut
  écrire sous la forme
  \begin{equation*}
    \mat{B} = \mat{P^{-1}} \mat{A} \mat{P},
  \end{equation*}
  où $\mat{A}$ est diagonale. Puisque
  \begin{align*}
    \mat{B}^2
    &= \mat{P^{-1}} \mat{A} \mat{P} \mat{P^{-1}} \mat{A} \mat{P} \\
    &= \mat{P^{-1}} \mat{A}^2 \mat{P}, \\
    \mat{B}^3
    &= \mat{P^{-1}} \mat{A} \mat{P} \mat{P^{-1}} \mat{A}^2 \mat{P} \\
    &= \mat{P^{-1}} \mat{A}^3 \mat{P} \\
    &\vdots \\
    \mat{B}^n &= \mat{P^{-1}} \mat{A}^n \mat{P},
  \end{align*}
  alors
  \begin{align*}
    e^{\mat{B}}
    &= \sum_{n = 0}^\infty \frac{\mat{P^{-1}} \mat{A}^n \mat{P}}{n!} \\
    &= \mat{P^{-1}}
    \left(
      \sum_{n = 0}^\infty \frac{\mat{A}^n}{n!}
    \right)
    \mat{P} \\
    &= \mat{P^{-1}} e^{\mat{A}} \mat{P}.
  \end{align*}
  Le calcul de l'exponentielle de la matrice $\mat{B}$ est donc simple
  dans ce cas.%
  \qed
\end{exemple}

\begin{definition}
  Une matrice carrée $\mat{A}$ est \emph{diagonalisable} s'il existe
  une matrice inversible $\mat{P}$ tel que
  \begin{displaymath}
    \mat{P}^{-1} \mat{A} \mat{P}
  \end{displaymath}
  est une matrice diagonale.
\end{definition}

Nous établirons qu'une matrice est diagonalisable si elle possède $n$
vecteurs propres linéairement indépendants.

\begin{exemple}
  À l'exemple \ref{ex:valeurspropres:vecteurs}, on a obtenu que les
  vecteurs propres de la matrice $\mat{A}$ sont
  \begin{align*}
    \mat{p}_1 &= \begin{bmatrix} -2 \\ 1 \\  1 \end{bmatrix} &
    \mat{p}_2 &= \begin{bmatrix}  1 \\ 0 \\ -1 \end{bmatrix} &
    \mat{p}_3 &= \begin{bmatrix}  0 \\ 1 \\  0 \end{bmatrix}.
  \end{align*}
  Ces vecteurs propres sont linéairement indépendants. Vérification
  rapide:
  \begin{displaymath}
    \begin{vmatrix}
      -2 &  1 & 0 \\
       1 &  0 & 1 \\
       1 & -1 & 0
    \end{vmatrix} = 1 \neq 0.
  \end{displaymath}
  \qed
\end{exemple}

Supposons qu'une matrice $\mat{A}_{3 \times 3}$ possède trois vecteurs
propres linéairement indépendants $\mat{p}_1, \dots, \mat{p}_3$ et
soit
\begin{align*}
  \mat{P}
  &= [\mat{p}_1\, \mat{p}_2\, \mat{p}_3] \\
  &=
  \begin{bmatrix}
    p_{11} & p_{12} & \dots & p_{1n} \\
    p_{21} & p_{22} & \dots & p_{2n} \\
    \vdots & \vdots &      & \vdots \\
    p_{n1} & p_{n2} & \dots & p_{nn}
  \end{bmatrix}.
\end{align*}
Alors,
\begin{align*}
  \mat{A} \mat{P}
  &= [\mat{A} \mat{p}_1\, \mat{A} \mat{p}_2\, \mat{A} \mat{p}_3] \\
  &= [\lambda_1 \mat{p}_1\, \lambda_2 \mat{p}_2\, \lambda_3 \mat{p}_3] \\
  &= [\mat{p}_1\, \mat{p}_2\, \mat{p}_3]
  \begin{bmatrix}
    \lambda_1 & 0 & \dots & 0 \\
    0 & \lambda_2 & \dots & 0 \\
    \vdots & \vdots &     & \vdots \\
    0 & 0 & \dots & \lambda_n
  \end{bmatrix} \\
  &= \mat{P} \mat{D},
\end{align*}
où $\mat{D}$ est une matrice diagonale. Or, puisque les colonnes de
$\mat{P}$ sont linéairement indépendantes, $\mat{P}^{-1}$ existe et,
par conséquent,
\begin{displaymath}
  \mat{P}^{-1} \mat{A} \mat{P} = \mat{D} = \diag(\lambda_1, \dots, \lambda_n),
\end{displaymath}
d'où $\mat{A}$ est diagonalisable.

On pourrait faire le cheminement inverse, ce qui justifie le théorème
suivant.

\begin{thm}
  \label{thm:valeurspropres:diagonalisable}
  Soit $\mat{A}$ une matrice $n \times n$. Les énoncés suivants sont
  équivalents.
  \begin{enumerate}
  \item $\mat{A}$ est diagonalisable.
  \item $\mat{A}$ possède $n$ vecteurs propres linéairement
    indépendants.
  \end{enumerate}
  De plus, si $\mat{P}$ est la matrice qui diagonalise $\mat{A}$, alors
  \begin{displaymath}
    \mat{P}^{-1} \mat{A} \mat{P} =
    \begin{bmatrix}
      \lambda_1 & 0 & \dots & 0 \\
      0 & \lambda_2 & \dots & 0 \\
      \vdots & \vdots &     & \vdots \\
      0 & 0 & \dots & \lambda_n
    \end{bmatrix}.
  \end{displaymath}
\end{thm}

La remarque qui suit l'exemple \ref{ex:valeurspropres:vecteurs}
établissait que si toutes les valeurs propres sont différentes
(multiplicité 1), alors on a un total de $n$ vecteurs propres.

Or, on peut démontrer que si les valeurs propres sont distinctes,
alors les vecteurs propres sont linéairement indépendants.

\begin{thm}
  \label{thm:valeurspropres:diagonalisable2}
  Si la matrice $\mat{A}_{n \times n}$ possède $n$ valeurs propres
  distinctes, alors elle est diagonalisable.
\end{thm}

\begin{rem}
  Si les valeurs propres ne sont pas distinctes, la matrice peut
  néanmoins être diagonalisable.
\end{rem}

\begin{exemple}
  Les valeurs propres de la matrice
  \begin{displaymath}
    \mat{A} =
    \begin{bmatrix}
      0 & 0 & -2 \\
      1 & 2 &  1 \\
      1 & 0 &  3
    \end{bmatrix}
  \end{displaymath}
  de l'exemple \ref{ex:valeurspropres:vecteurs} sont $\lambda_1 = 1$
  et $\lambda_2 = \lambda_3 = 2$. Néanmoins, les trois vecteurs
  propres forment une base de $\mathbb{R}^3$. Par conséquent,
  \begin{align*}
    \mat{P}^{-1} \mat{A} \mat{P}
    &=
    \begin{bmatrix}
      -1 & 0 & -1 \\
      -1 & 0 & -2 \\
       1 & 1 &  1
    \end{bmatrix}
    \begin{bmatrix}
      0 & 0 & -2 \\
      1 & 2 &  1 \\
      1 & 0 &  3
    \end{bmatrix}
    \begin{bmatrix}
      -2 &  1 & 0 \\
       1 &  0 & 1 \\
       1 & -1 & 0
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 2 & 0 \\
      0 & 0 & 2
    \end{bmatrix}.
  \end{align*}
  \qed
\end{exemple}

\begin{exemple}
  Soit
  \begin{displaymath}
    \mat{A} =
    \begin{bmatrix}
       1 & 0 & 0 \\
       1 & 2 & 0 \\
      -3 & 5 & 2
    \end{bmatrix}.
  \end{displaymath}
  Le polynôme caractéristique de $\mat{A}$ est
  \begin{displaymath}
    \det(\lambda \mat{I} - \mat{A}) = (\lambda - 1)(\lambda - 2)^2,
  \end{displaymath}
  d'où les valeurs propres sont $\lambda_1 = 1$ et $\lambda_2 =
  \lambda_3 = 2$. Il est laissé en exercice de vérifier que
  \begin{itemize}
  \item le vecteur propre correspondant à $\lambda = 1$ est $\mat{p}_1
    = (\frac{1}{8}, -\frac{1}{8}, 1)$;
  \item le vecteur propre correspondant à $\lambda = 2$ est $\mat{p}_2
    = (0, 0, 1)$.
  \end{itemize}
  Puisque l'on a seulement deux vecteurs propres linéairement
  indépendants, la matrice $\mat{A}$ n'est pas diagonalisable.
  \qed
\end{exemple}

\begin{exemple}
  Soit
  \begin{displaymath}
    \mat{A} =
    \begin{bmatrix}
      -1 & 2 & 4 &  0 \\
       0 & 3 & 1 &  7 \\
       0 & 0 & 5 &  8 \\
       0 & 0 & 0 & -2
    \end{bmatrix}
  \end{displaymath}
  On a:
  \begin{itemize}
  \item la matrice $\mat{A}$ est triangulaire;
  \item ses valeurs propres sont $\lambda_1 = -1$, $\lambda_2 = 3$,
    $\lambda_4 = 5$ et $\lambda_4 = -2$;
  \item les valeurs propres sont distinctes;
  \item la matrice est diagonalisable.
  \end{itemize}
  \qed
\end{exemple}

\begin{gotoR}
  On calcule les valeurs et vecteurs propres dans R avec la fonction
  \code{eigen}. Voir le code informatique de la
  section~\ref{sec:revision:code} pour les détails.
\end{gotoR}


\section{Code informatique}
\label{sec:revision:code}

\lstinputlisting[firstline=3]{valeurs_propres.R}

\vfill

\input{exercices-valeurs_propres}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "methodes_numeriques-partie_4"
%%% End:
